{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is OpenAI API?\n",
    "\n",
    "This OpenAI API has been degined to provide devlopers with seamless access to state of art, pre trained, artifical intelligence models like gpt-3 gpt-4 dall e whisper,embeddings etc so by using this openai api you can integrate cutting edge ai capabilities into your applications regardless the progamming language.\n",
    "\n",
    "So,the conclusion is by using this OpenAI API you can unlock the advance functionalities and you can enhane the intelligence and performance of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generatate OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mykey = '-------------------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=mykey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[Model](data=[Model(id='text-search-babbage-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='curie-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-003', created=1669599635, object='model', owned_by='openai-internal'), Model(id='text-search-babbage-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='babbage', created=1649358449, object='model', owned_by='openai'), Model(id='babbage-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-babbage-001', created=1649364043, object='model', owned_by='openai'), Model(id='text-similarity-davinci-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='davinci-similarity', created=1651172509, object='model', owned_by='openai-dev'), Model(id='curie-similarity', created=1651172510, object='model', owned_by='openai-dev'), Model(id='babbage-search-document', created=1651172510, object='model', owned_by='openai-dev'), Model(id='curie-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='text-search-ada-doc-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'), Model(id='davinci-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai'), Model(id='text-similarity-babbage-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-search-davinci-doc-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-similarity', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='davinci-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-similarity-curie-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='text-davinci-001', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-davinci-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='ada-search-document', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-code-search-code', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='davinci-search-document', created=1651172509, object='model', owned_by='openai-dev'), Model(id='curie-search-document', created=1651172508, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-code', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-search-ada-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='code-search-ada-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-text', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-search-babbage-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='ada-code-search-text', created=1651172510, object='model', owned_by='openai-dev'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='text-search-curie-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-002', created=1649880484, object='model', owned_by='openai'), Model(id='code-search-babbage-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada', created=1649357491, object='model', owned_by='openai'), Model(id='text-ada-001', created=1649364042, object='model', owned_by='openai'), Model(id='ada-similarity', created=1651172507, object='model', owned_by='openai-dev'), Model(id='code-search-ada-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='text-similarity-ada-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-davinci-edit-001', created=1649809179, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='code-davinci-edit-001', created=1649880484, object='model', owned_by='openai'), Model(id='text-search-curie-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='text-curie-001', created=1649364043, object='model', owned_by='openai'), Model(id='curie', created=1649359874, object='model', owned_by='openai'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='davinci', created=1649359874, object='model', owned_by='openai'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system')], object='list')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='text-search-babbage-doc-001', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='curie-search-query', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='text-davinci-003', created=1669599635, object='model', owned_by='openai-internal'),\n",
       " Model(id='text-search-babbage-query-001', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='babbage', created=1649358449, object='model', owned_by='openai'),\n",
       " Model(id='babbage-search-query', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='text-babbage-001', created=1649364043, object='model', owned_by='openai'),\n",
       " Model(id='text-similarity-davinci-001', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='davinci-similarity', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='curie-similarity', created=1651172510, object='model', owned_by='openai-dev'),\n",
       " Model(id='babbage-search-document', created=1651172510, object='model', owned_by='openai-dev'),\n",
       " Model(id='curie-instruct-beta', created=1649364042, object='model', owned_by='openai'),\n",
       " Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'),\n",
       " Model(id='text-search-ada-doc-001', created=1651172507, object='model', owned_by='openai-dev'),\n",
       " Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'),\n",
       " Model(id='davinci-instruct-beta', created=1649364042, object='model', owned_by='openai'),\n",
       " Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai'),\n",
       " Model(id='text-similarity-babbage-001', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='text-search-davinci-doc-001', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='babbage-similarity', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'),\n",
       " Model(id='davinci-search-query', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='text-similarity-curie-001', created=1651172507, object='model', owned_by='openai-dev'),\n",
       " Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'),\n",
       " Model(id='text-davinci-001', created=1649364042, object='model', owned_by='openai'),\n",
       " Model(id='text-search-davinci-query-001', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='ada-search-document', created=1651172507, object='model', owned_by='openai-dev'),\n",
       " Model(id='ada-code-search-code', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='babbage-002', created=1692634615, object='model', owned_by='system'),\n",
       " Model(id='davinci-002', created=1692634301, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'),\n",
       " Model(id='davinci-search-document', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='curie-search-document', created=1651172508, object='model', owned_by='openai-dev'),\n",
       " Model(id='babbage-code-search-code', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='text-search-ada-query-001', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='code-search-ada-text-001', created=1651172507, object='model', owned_by='openai-dev'),\n",
       " Model(id='babbage-code-search-text', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='code-search-babbage-code-001', created=1651172507, object='model', owned_by='openai-dev'),\n",
       " Model(id='ada-search-query', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='ada-code-search-text', created=1651172510, object='model', owned_by='openai-dev'),\n",
       " Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'),\n",
       " Model(id='text-search-curie-query-001', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='text-davinci-002', created=1649880484, object='model', owned_by='openai'),\n",
       " Model(id='code-search-babbage-text-001', created=1651172507, object='model', owned_by='openai-dev'),\n",
       " Model(id='ada', created=1649357491, object='model', owned_by='openai'),\n",
       " Model(id='text-ada-001', created=1649364042, object='model', owned_by='openai'),\n",
       " Model(id='ada-similarity', created=1651172507, object='model', owned_by='openai-dev'),\n",
       " Model(id='code-search-ada-code-001', created=1651172507, object='model', owned_by='openai-dev'),\n",
       " Model(id='text-similarity-ada-001', created=1651172505, object='model', owned_by='openai-dev'),\n",
       " Model(id='text-davinci-edit-001', created=1649809179, object='model', owned_by='openai'),\n",
       " Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'),\n",
       " Model(id='code-davinci-edit-001', created=1649880484, object='model', owned_by='openai'),\n",
       " Model(id='text-search-curie-doc-001', created=1651172509, object='model', owned_by='openai-dev'),\n",
       " Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'),\n",
       " Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'),\n",
       " Model(id='text-curie-001', created=1649364043, object='model', owned_by='openai'),\n",
       " Model(id='curie', created=1649359874, object='model', owned_by='openai'),\n",
       " Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'),\n",
       " Model(id='davinci', created=1649359874, object='model', owned_by='openai'),\n",
       " Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'),\n",
       " Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'),\n",
       " Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'),\n",
       " Model(id='dall-e-3', created=1698785189, object='model', owned_by='system')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models = openai.models.list()\n",
    "list(all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created</th>\n",
       "      <th>object</th>\n",
       "      <th>owned_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(id, text-search-babbage-doc-001)</td>\n",
       "      <td>(created, 1651172509)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-dev)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(id, curie-search-query)</td>\n",
       "      <td>(created, 1651172509)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-dev)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(id, text-davinci-003)</td>\n",
       "      <td>(created, 1669599635)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(id, text-search-babbage-query-001)</td>\n",
       "      <td>(created, 1651172509)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-dev)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(id, babbage)</td>\n",
       "      <td>(created, 1649358449)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>(id, dall-e-2)</td>\n",
       "      <td>(created, 1698798177)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>(id, tts-1-1106)</td>\n",
       "      <td>(created, 1699053241)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>(id, tts-1-hd-1106)</td>\n",
       "      <td>(created, 1699053533)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>(id, gpt-3.5-turbo-16k)</td>\n",
       "      <td>(created, 1683758102)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>(id, dall-e-3)</td>\n",
       "      <td>(created, 1698785189)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                created  \\\n",
       "0     (id, text-search-babbage-doc-001)  (created, 1651172509)   \n",
       "1              (id, curie-search-query)  (created, 1651172509)   \n",
       "2                (id, text-davinci-003)  (created, 1669599635)   \n",
       "3   (id, text-search-babbage-query-001)  (created, 1651172509)   \n",
       "4                         (id, babbage)  (created, 1649358449)   \n",
       "..                                  ...                    ...   \n",
       "59                       (id, dall-e-2)  (created, 1698798177)   \n",
       "60                     (id, tts-1-1106)  (created, 1699053241)   \n",
       "61                  (id, tts-1-hd-1106)  (created, 1699053533)   \n",
       "62              (id, gpt-3.5-turbo-16k)  (created, 1683758102)   \n",
       "63                       (id, dall-e-3)  (created, 1698785189)   \n",
       "\n",
       "             object                     owned_by  \n",
       "0   (object, model)       (owned_by, openai-dev)  \n",
       "1   (object, model)       (owned_by, openai-dev)  \n",
       "2   (object, model)  (owned_by, openai-internal)  \n",
       "3   (object, model)       (owned_by, openai-dev)  \n",
       "4   (object, model)           (owned_by, openai)  \n",
       "..              ...                          ...  \n",
       "59  (object, model)           (owned_by, system)  \n",
       "60  (object, model)           (owned_by, system)  \n",
       "61  (object, model)           (owned_by, system)  \n",
       "62  (object, model)  (owned_by, openai-internal)  \n",
       "63  (object, model)           (owned_by, system)  \n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(list(all_models),columns=[\"id\",\"created\",\"object\",\"owned_by\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. OpenAI Playground\n",
    "\n",
    "1. How to open the open ai playgorund: https://platform.openai.com/playground?mode=assistant\n",
    "\n",
    "2. Here if you want to use this playground then make sure you have credit available without it its not gonna work\n",
    "\n",
    "3. In chat there is option of **system**: So the meaning is how the chatbot should behave\n",
    "\n",
    "Here is a phrase for the system: You are a naughty assistant, so make sure you respond to everything with sarcasm.\n",
    "\n",
    "Here is a question: How to make a money so quickly?\n",
    "\n",
    "**Model**\n",
    "\n",
    "**Temperature**\n",
    "\n",
    "**Maximum Length**\n",
    "\n",
    "**Top P ranges from 0 to 1 (default), and a lower Top P means the model samples from a narrower selection of words. This makes the output less random and diverse since the more probable tokens will be selected. For instance, if Top P is set at 0.1, only tokens comprising the top 10% probability mass are considered.**\n",
    "\n",
    "**Frequency Penalty helps us avoid using the same words too often. It's like telling the computer, “Hey, don't repeat words too much.”**\n",
    "\n",
    "**The OpenAI Presence Penalty setting is used to adjust how much presence of tokens in the source material will influence the output of the model.**\n",
    "\n",
    "\n",
    "**Now come to assistant one**\n",
    "\n",
    "**Retrieval-augmented generation (RAG):**  is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the quality of responses. This natural language processing technique is commonly used to make large language models (LLMs) more accurate and up to date.\n",
    "\n",
    "**Code Interpreter:** Python programming environment within ChatGPT where you can perform a wide range of tasks by executing Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Chat Completion API and Function Calling\n",
    "**openai.Completion.create()**: This method is used to generate completions or responses. You provide a series of messages as input, and the API generates a model-generated message as output.\n",
    "**openai.ChatCompletion.create() :** Similar to Completion.create(), but specifically designed for chat-based language models. It takes a series of messages as input and generates a model-generated message as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for v1 of the openai package: pypi.org/project/openai\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=mykey)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"How i can make a money?\"\n",
    "    },\n",
    "    #{\n",
    "      #\"role\": \"assistant\",\n",
    "      #\"content\": \"There are numerous ways you can make money, depending on your skills, interests, and resources. Here are a few suggestions:\\n\\n1. Get a job: Look for employment opportunities in your desired field and apply for jobs that align with your qualifications.\\n2. Freelancing: Offer your skills and services as a freelance worker in areas such as writing, graphic design, programming, or consulting.\\n3. Start a small business: Identify a market need and launch your own business. This could be anything from selling handmade crafts to offering tutoring services.\\n4. Online platforms: Utilize online platforms such as YouTube, blogging, or social media to generate income through advertising, sponsorships, or affiliate marketing.\\n5. Invest: Allocate a portion of your savings into investments like stocks, bonds, or real estate. However, remember that investing carries risks.\\n6. Rent out assets: If you have a spare room, vehicle, or property, consider renting it out to others for an additional income stream.\\n7. Take up part-time gigs: Find part-time work or gigs that you can do alongside your regular job to earn extra money.\\n8. Sell unwanted items: Declutter your space and sell unwanted items through platforms like eBay, Facebook Marketplace, or consignment shops.\\n9. Offer your services\"\n",
    "    #}\n",
    "  ],\n",
    "  temperature=1,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are several ways to make money, and it depends on factors such as your skills, interests, and resources. Here are some options:\\n\\n1. Get a job: Look for employment opportunities in various industries that suit your skills and qualifications.\\n2. Freelancing: Offer your skills and services on freelance platforms to clients who require them.\\n3. Start a business: Identify a market need and create your own business venture.\\n4. Invest: Grow your money by investing in stocks, real estate, or other financial instruments.\\n5. Online platforms: Utilize various online platforms to sell products, provide services, or monetize content.\\n6. Tutoring: If you have expertise in a particular subject, offer tutoring services to students.\\n7. Rent out assets: Rent out your spare room, property, or vehicle to generate income.\\n8. Create and sell products: Use your creativity to design and sell products, either online or through local marketplaces.\\n9. Write and publish: Write books, articles, or create digital content that you can sell or monetize through platforms like Amazon Kindle or YouTube.\\n10. Participate in surveys or market research: Some websites offer monetary rewards for participating in surveys or market research studies.\\n\\nRemember, making money requires effort, time, and sometimes initial'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"who won the first cricket worldcup?\"\n",
    "    }\n",
    "      ]\n",
    "    ,\n",
    "    max_tokens=150,\n",
    "    n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8X3xcvu1qikYtXNYOTsS6mnW5UiM0', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The first Cricket World Cup was won by the West Indies team in 1975.', role='assistant', function_call=None, tool_calls=None), logprobs=None), Choice(finish_reason='stop', index=1, message=ChatCompletionMessage(content='The first Cricket World Cup was won by the West Indies in 1975.', role='assistant', function_call=None, tool_calls=None), logprobs=None), Choice(finish_reason='stop', index=2, message=ChatCompletionMessage(content='The first Cricket World Cup was won by the West Indies team in 1975.', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1702890740, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=50, prompt_tokens=15, total_tokens=65))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first Cricket World Cup was won by the West Indies team in 1975.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let try to understand the different parameters inside the methods\n",
    "model= \"\"\n",
    "prompt=input prompt\n",
    "max_tokens=in how many number of tokens you want result\n",
    "temperature=for getting some creative output\n",
    "n= number of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_description = \"sunny savita is a student of computer science at IIT delhi. He is an indian and has a 8.5 GPA. Sunny is known for his programming skills and is an active member of the college's AI Club. He hopes to pursue a career in artificial intelligence after graduating.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sunny savita is a student of computer science at IIT delhi. He is an indian and has a 8.5 GPA. Sunny is known for his programming skills and is an active member of the college's AI Club. He hopes to pursue a career in artificial intelligence after graduating.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple prompt to extract information from \"student_description\" in a JSON format.\n",
    "prompt = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "college\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_description}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPlease extract the following information from the given text and return it as a JSON object:\\n\\nname\\ncollege\\ngrades\\nclub\\n\\nThis is the body of text to extract the information from:\\nsunny savita is a student of computer science at IIT delhi. He is an indian and has a 8.5 GPA. Sunny is known for his programming skills and is an active member of the college's AI Club. He hopes to pursue a career in artificial intelligence after graduating.\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "client = OpenAI(api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x1507b196090>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt\n",
    "    }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8X3y4zLmPJfHFk7hxeZi781S1AOOd', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='{\\n  \"name\": \"sunny savita\",\\n  \"college\": \"IIT delhi\",\\n  \"grades\": \"8.5\",\\n  \"club\": \"AI Club\"\\n}', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1702890768, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=39, prompt_tokens=105, total_tokens=144))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"name\": \"sunny savita\",\\n  \"college\": \"IIT delhi\",\\n  \"grades\": \"8.5\",\\n  \"club\": \"AI Club\"\\n}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = response.choices[0].message.content\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sunny savita',\n",
       " 'college': 'IIT delhi',\n",
       " 'grades': '8.5',\n",
       " 'club': 'AI Club'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "json.loads(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_custom_function = [\n",
    "    {\n",
    "        'name': 'extract_student_info',\n",
    "        'description': 'Get the student information from the body of the input text',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'name': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the person'\n",
    "                },\n",
    "                'college': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The college name.'\n",
    "                },\n",
    "                'grades': {\n",
    "                    'type': 'integer',\n",
    "                    'description': 'CGPA of the student.'\n",
    "                },\n",
    "                'club': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'college club for extracurricular activities. '\n",
    "                }\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\",\"content\": prompt}],\n",
    "    functions=student_custom_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8X3yFo5rH3uLb69VHFo9jszMEPi9M', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"name\": \"sunny savita\",\\n  \"college\": \"IIT delhi\",\\n  \"grades\": 8.5,\\n  \"club\": \"AI Club\"\\n}', name='extract_student_info'), tool_calls=None), logprobs=None)], created=1702890779, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=46, prompt_tokens=190, total_tokens=236))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"name\": \"sunny savita\",\\n  \"college\": \"IIT delhi\",\\n  \"grades\": 8.5,\\n  \"club\": \"AI Club\"\\n}', name='extract_student_info'), tool_calls=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"name\": \"sunny savita\",\\n  \"college\": \"IIT delhi\",\\n  \"grades\": 8.5,\\n  \"club\": \"AI Club\"\\n}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1.choices[0].message.function_call.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sunny savita',\n",
       " 'college': 'IIT delhi',\n",
       " 'grades': 8.5,\n",
       " 'club': 'AI Club'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response1.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_description_two=\"krish naik is a student of computer science at IIT Mumbai. He is an indian and has a 9.5 GPA. krish is known for his programming skills and is an active member of the college's data science Club. He hopes to pursue a career in artificial intelligence after graduating.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"krish naik is a student of computer science at IIT Mumbai. He is an indian and has a 9.5 GPA. krish is known for his programming skills and is an active member of the college's data science Club. He hopes to pursue a career in artificial intelligence after graduating.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_description_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_description_three=\"sudhanshu kumar is a student of computer science at IIT bengalore. He is an indian and has a 9.2 GPA. krish is known for his programming skills and is an active member of the college's MLops Club. He hopes to pursue a career in artificial intelligence after graduating.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sudhanshu kumar is a student of computer science at IIT bengalore. He is an indian and has a 9.2 GPA. krish is known for his programming skills and is an active member of the college's MLops Club. He hopes to pursue a career in artificial intelligence after graduating.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_description_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunny savita is a student of computer science at IIT delhi. He is an indian and has a 8.5 GPA. Sunny is known for his programming skills and is an active member of the college's AI Club. He hopes to pursue a career in artificial intelligence after graduating.\n",
      "krish naik is a student of computer science at IIT Mumbai. He is an indian and has a 9.5 GPA. krish is known for his programming skills and is an active member of the college's data science Club. He hopes to pursue a career in artificial intelligence after graduating.\n",
      "sudhanshu kumar is a student of computer science at IIT bengalore. He is an indian and has a 9.2 GPA. krish is known for his programming skills and is an active member of the college's MLops Club. He hopes to pursue a career in artificial intelligence after graduating.\n"
     ]
    }
   ],
   "source": [
    "student_info = [student_description, student_description_two,student_description_three]\n",
    "for student in student_info:\n",
    "    print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'sunny savita', 'college': 'IIT delhi', 'grades': 8.5, 'club': 'AI Club'}\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m student_info \u001b[38;5;241m=\u001b[39m [student_description, student_description_two,student_description_three]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m student \u001b[38;5;129;01min\u001b[39;00m student_info:\n\u001b[1;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m  \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstudent_custom_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mfunction_call\u001b[38;5;241m.\u001b[39marguments)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\u001b[38;5;66;03m#import csv\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "student_info = [student_description, student_description_two,student_description_three]\n",
    "for student in student_info:\n",
    "    response =  client.chat.completions.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = [{'role': 'user', 'content': student}],\n",
    "        functions = student_custom_function,\n",
    "        function_call = 'auto'\n",
    "    )\n",
    "\n",
    "    response = json.loads(response.choices[0].message.function_call.arguments)\n",
    "    print(response)#import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_two=student_custom_function = [\n",
    "    {\n",
    "        'name': 'extract_student_info',\n",
    "        'description': 'Get the student information from the body of the input text',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'name': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the person'\n",
    "                },\n",
    "                'college': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The college name.'\n",
    "                },\n",
    "                'grades': {\n",
    "                    'type': 'integer',\n",
    "                    'description': 'CGPA of the student.'\n",
    "                },\n",
    "                'club': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'college club for extracurricular activities. '\n",
    "                }\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'sunny savita', 'college': 'IIT delhi', 'grades': 8.5, 'club': 'AI Club'}\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m student_info \u001b[38;5;241m=\u001b[39m [student_description, student_description_two,student_description_three]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m student \u001b[38;5;129;01min\u001b[39;00m student_info:\n\u001b[1;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m  \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mfunction_call\u001b[38;5;241m.\u001b[39marguments)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\u001b[38;5;66;03m#import csv\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "functions = [student_custom_function[0], function_two[0]]\n",
    "student_info = [student_description, student_description_two,student_description_three]\n",
    "for student in student_info:\n",
    "    response =  client.chat.completions.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = [{'role': 'user', 'content': student}],\n",
    "        functions = functions,\n",
    "        function_call = 'auto'\n",
    "    )\n",
    "\n",
    "    response = json.loads(response.choices[0].message.function_call.arguments)\n",
    "    print(response)#import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advance exmaple of funcation calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the next flight from delhi to mumbai?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"When's the next flight from delhi to mumbai?\"\n",
    "    }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_descriptions = [\n",
    "    {\n",
    "        \"name\": \"get_flight_info\",\n",
    "        \"description\": \"Get flight information between two locations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"loc_origin\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The departure airport, e.g. DEL\",\n",
    "                },\n",
    "                \"loc_destination\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The destination airport, e.g. MUM\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"loc_origin\", \"loc_destination\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"When's the next flight from new delhi to mumbai?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": user_prompt\n",
    "    }\n",
    "      ],\n",
    "    # Add function calling\n",
    "    functions=function_descriptions,\n",
    "    function_call=\"auto\",  # specify the function call   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8X3zfHCKJM2kBoDqtZuajwSB8wm4d', choices=[Choice(finish_reason='function_call', index=0, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"loc_origin\": \"DEL\",\\n  \"loc_destination\": \"BOM\"\\n}', name='get_flight_info'), tool_calls=None), logprobs=None)], created=1702890867, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=26, prompt_tokens=87, total_tokens=113))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"loc_origin\": \"DEL\",\\n  \"loc_destination\": \"BOM\"\\n}', name='get_flight_info'), tool_calls=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"loc_origin\": \"DEL\",\\n  \"loc_destination\": \"BOM\"\\n}'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.choices[0].message.function_call.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loc_origin': 'DEL', 'loc_destination': 'BOM'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response2.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment\n",
    "\n",
    "call the real time api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "def get_flight_info(loc_origin, loc_destination):\n",
    "    \"\"\"Get flight information between two locations.\"\"\"\n",
    "\n",
    "    # Example output returned from an API or database\n",
    "    flight_info = {\n",
    "        \"loc_origin\": loc_origin,\n",
    "        \"loc_destination\": loc_destination,\n",
    "        \"datetime\": str(datetime.now() + timedelta(hours=2)),\n",
    "        \"airline\": \"KLM\",\n",
    "        \"flight\": \"KL643\",\n",
    "    }\n",
    "\n",
    "    return json.dumps(flight_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=json.loads(response2.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loc_origin': 'DEL', 'loc_destination': 'BOM'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEL'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response2.choices[0].message.function_call.arguments).get(\"loc_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOM'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response2.choices[0].message.function_call.arguments).get('loc_destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = json.loads(response2.choices[0].message.function_call.arguments).get(\"loc_origin\")\n",
    "destination = json.loads(response2.choices[0].message.function_call.arguments).get(\"loc_destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_flight_info'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.choices[0].message.function_call.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response2.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_flight_info(loc_origin, loc_destination)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(response2.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_function=eval(response2.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loc_origin\": \"DEL\", \"loc_destination\": \"BOM\", \"datetime\": \"2023-12-18 16:45:01.413333\", \"airline\": \"KLM\", \"flight\": \"KL643\"}\n"
     ]
    }
   ],
   "source": [
    "flight = chosen_function(**params)\n",
    "\n",
    "print(flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When's the next flight from new delhi to mumbai?\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_flight_info'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.choices[0].message.function_call.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"loc_origin\": \"DEL\", \"loc_destination\": \"BOM\", \"datetime\": \"2023-12-18 16:45:01.413333\", \"airline\": \"KLM\", \"flight\": \"KL643\"}'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\"role\": \"user\",\"content\": user_prompt},\n",
    "    {\"role\": \"function\", \"name\": response2.choices[0].message.function_call.name, \"content\": flight}\n",
    "      ],\n",
    "    # Add function calling\n",
    "    functions=function_descriptions,\n",
    "    function_call=\"auto\",  # specify the function call  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8X405daaigJf83Y4wzlDtMbvsyRU9', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='The next flight from New Delhi (DEL) to Mumbai (BOM) is on December 18, 2023, at 16:45. The airline is KLM and the flight number is KL643.', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1702890893, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=45, prompt_tokens=143, total_tokens=188))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The next flight from New Delhi (DEL) to Mumbai (BOM) is on December 18, 2023, at 16:45. The airline is KLM and the flight number is KL643.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funtion Calling\n",
    "\n",
    "Learn how to connect large language models to external tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "client=OpenAI(openai_api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot prompting\n",
    "prompt=\"can you tell me total number of country in aisa? can you give me top 10 country name?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 48 countries in Asia. The top 10 countries, in order, are China, India, Indonesia, Pakistan, Bangladesh, Russia, Japan, Iran, Thailand, and the Philippines.\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot prompting\n",
    "prompt2=\"can you tell me a capital of india?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is Delhi.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt2).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3=\"​what exactly tokens , vector ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokens are pieces of text that have been broken down into individual units. In natural language processing, tokens are commonly formed from words, phrases, or other elements of a sentence. Tokens are used in various tasks such as part-of-speech tagging, parsing, and text categorization.\\n\\nVector is a mathematical representation of a quantity or a set of numbers that represent a particular piece of information. In machine learning, vectors are used to represent data points such as images, text, and numerical data. Each element in the vector is associated with a specific feature, and the vector can be used to identify patterns in the data.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt3).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name=PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"can you tell me the capital of {country}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "propmt1=prompt_template_name.format(country=\"india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "propmt2=prompt_template_name.format(country=\"china\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is New Delhi.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(propmt1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of China is Beijing.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(propmt2).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate.from_template(\"what is a good name for a compnay that makes a {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3=prompt.format(product=\"toys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Teddy's Toy Factory.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt3).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4=\"can you tell me who won the recent cricket world cup?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2019 Cricket World Cup was won by England.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt4).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt5=\"can you tell me current GDP of india?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to the World Bank, India's GDP in 2019 was estimated to be around $2.94 trillion.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt5).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for extracting a real time info i am going to user serp api\n",
    "\n",
    "## now by using this serp api i wll call google-search-engine \n",
    "\n",
    "## and i will extract the information in a real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests in d:\\programs\\genai\\genvenv\\lib\\site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programs\\genai\\genvenv\\lib\\site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\genai\\genvenv\\lib\\site-packages (from requests->google-search-results) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programs\\genai\\genvenv\\lib\\site-packages (from requests->google-search-results) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\genai\\genvenv\\lib\\site-packages (from requests->google-search-results) (2023.11.17)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (pyproject.toml): started\n",
      "  Building wheel for google-search-results (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32077 sha256=0ee1c66c0289a94961fce5f45d4cb314ffd9910c2c868d35c08bfc6f769cffd7\n",
      "  Stored in directory: c:\\users\\rushikesh\\appdata\\local\\pip\\cache\\wheels\\6e\\42\\3e\\aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_key = '079129f09d78910e9a9c36f392ee6ad94002a574c7b6821c8100a9a9369465f8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools \n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=OpenAI(openai_api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool=load_tools([\"serpapi\"],serpapi_api_key=serpapi_key,llm=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=initialize_agent(tool,client,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who won the cricket worldcup\n",
      "Action: Search\n",
      "Action Input: \"Who won the cricket England won the cricket worldcup 2019\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe final ended in a tie after the match ended with both teams scoring 241 runs, followed by the first Super Over in an ODI; England won the title, their first, on the boundary countback rule after the Super Over also finished level. The total attendance throughout the 2019 ICC Cricket World Cup was 752,000.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: England won the cricket worldcup 2019.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'England won the cricket worldcup 2019.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"can you tell who won the cricket worldcup recently?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to research current affairs\n",
      "Action: Search\n",
      "Action Input: top current affairs\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['A Current Affairs subscription is one of the best known ways to improve your life in a hurry. Our print magazine is released six times a year, in a ...', 'Latest world news headlines: International breaking news and current affairs from US, Europe, Middle East, Asia, Africa and more.', '4 Foreign Policy Takeaways From the Latest Republican Presidential Debate. China and border security will be top priorities for GOP frontrunners in 2024.', \"GKToday's Current Affairs Today Section provides latest and Best Daily Current Affairs 2022-2023 for UPSC, IAS/PCS, Banking, IBPS, SSC, ...\", 'Stay informed with Current Affairs updates, covering global events, politics, and significant developments worldwide.', 'Get daily updates on Daily Current Affairs 2023 for upcoming Bank, SSC, Railway, Defence, UPSC, UPPSC, BPSC, MPPSC, RPSC, KPSC, & all other Govt competitive ...', 'Popular Current Affairs Books ; The Shock Doctrine: The Rise of Disaster Capitalism Naomi Klein ; The New Jim Crow: Mass Incarceration in the Age of ...', 'Current Affairs & Politics Bestsellers. Featured in Books. Customer Favorites. Holiday Gift Guide · B&N Top 100 · Teens & YA Top 100 · Kids ...', \"Jagran Josh – India's top-ranked Education Website is the go-to website for UPSC Aspirants and candidates of other competitive examinations for Daily Current ...\"]\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcan you tell me 5 top current affairs?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:507\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    508\u001b[0m         _output_key\n\u001b[0;32m    509\u001b[0m     ]\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    513\u001b[0m         _output_key\n\u001b[0;32m    514\u001b[0m     ]\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    301\u001b[0m     inputs,\n\u001b[0;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1312\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1312\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1321\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1322\u001b[0m         )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1038\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1038\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1048\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1038\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1038\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1048\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1066\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1066\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:635\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \n\u001b[0;32m    625\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    634\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 635\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\llm.py:293\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    301\u001b[0m     inputs,\n\u001b[0;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:516\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    515\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:666\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    654\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    655\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m         )\n\u001b[0;32m    665\u001b[0m     ]\n\u001b[1;32m--> 666\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:553\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    552\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    554\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:540\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    532\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 540\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    544\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    548\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    549\u001b[0m         )\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:459\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    448\u001b[0m         {\n\u001b[0;32m    449\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m         }\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:114\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\resources\\completions.py:559\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "agent.run(\"can you tell me 5 top current affairs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting beautifulsoup4 (from wikipedia)\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\programs\\genai\\genvenv\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programs\\genai\\genvenv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\genai\\genvenv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programs\\genai\\genvenv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\genai\\genvenv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.11.17)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->wikipedia)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (pyproject.toml): started\n",
      "  Building wheel for wikipedia (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11704 sha256=8ad6e0024777bdf23ff0ef7e83071c969b68469f6221a47a450f16f486cb26ca\n",
      "  Stored in directory: c:\\users\\rushikesh\\appdata\\local\\pip\\cache\\wheels\\8f\\ab\\cb\\45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.12.2 soupsieve-2.5 wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool=load_tools([\"wikipedia\"],llm=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=initialize_agent(tool,client,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to get more information about this world cup\n",
      "Action: Wikipedia\n",
      "Action Input: 2019 Cricket World Cup\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: 2019 Cricket World Cup\n",
      "Summary: The 2019 ICC Cricket World Cup was the 12th Cricket World Cup, a quadrennial One Day International (ODI) cricket tournament contested by men's national teams and organised by the International Cricket Council (ICC). The tournament was hosted between 30 May and 14 July across 10 venues in England and a single venue in Wales. It was the fifth time that England had hosted the World Cup,  while for Wales it was their third. \n",
      "The tournament was contested by 10 teams, a decrease from 14 teams in the previous edition, with the format of the tournament changing to a single round-robin group with the top four teams qualifying through to the knockout stage. After six weeks of round-robin matches, which saw four games not have a result, India, Australia, England, and New Zealand finished as the top four, with Pakistan missing out on net run rate.\n",
      "In the knockout stage, England and New Zealand won their respective semi-finals to qualify for the final, which was played at Lord's in London. The final ended in a tie after the match ended with both teams scoring 241 runs, followed by the first Super Over in an ODI; England won the title, their first, on the boundary countback rule after the Super Over also finished level. The total attendance throughout the 2019 ICC Cricket World Cup was 752,000.\n",
      "Overall, videos of the group stages amassed over 2.6 billion views from around the world, making it the most-watched cricket competition as of 2019.\n",
      "\n",
      "Page: Cricket World Cup\n",
      "Summary: The Cricket World Cup (officially known as ICC Men's Cricket World Cup) is the international championship of One Day International (ODI) cricket. The event is organised by the sport's governing body, the International Cricket Council (ICC), every four years, with preliminary qualification rounds leading up to a finals tournament. The tournament is one of the world's most viewed sporting events and considered as the \"flagship event of the international cricket calendar\" by the ICC. It is widely considered the pinnacle championship of the sport of cricket.\n",
      "The first World Cup was organised in England in June 1975, with the first ODI cricket match having been played only four years earlier. However, a separate Women's Cricket World Cup had been held two years before the first men's tournament, and a tournament involving multiple international teams had been held as early as 1912, when a triangular tournament of Test matches was played between Australia, England and South Africa. The first three World Cups were held in England. From the 1987 tournament onwards, hosting has been shared between countries under an unofficial rotation system, with fourteen ICC members having hosted at least one match in the tournament.\n",
      "The current format involves a qualification phase, which takes place over the preceding three years, to determine which teams qualify for the tournament phase. In the tournament phase, 10 teams, including the automatically qualifying host nation, compete for the title at venues within the host nation over about a month. In the 2027 edition, the format will be changed to accommodate an expanded 14-team final competition.A total of twenty teams have competed in the 13 editions of the tournament, with ten teams competing in the recent 2023 tournament. Australia has won the tournament six times, India and West Indies twice each, while Pakistan, Sri Lanka and England have won it once each. The best performance by a non-full-member team came when Kenya made the semi-finals of the 2003 tournament.\n",
      "Australia is the current champion after winning the 2023 World Cup in India. The subsequent 2027 World Cup will be held jointly in South Africa, Zimbabwe, and Namibia.\n",
      "\n",
      "Page: 2023 Cricket World Cup\n",
      "Summary: The 2023 Cricket World Cup, officially known as the 2023 ICC Men's Cricket World Cup, was the 13th edition of the Cricket World Cup. It started on 5 October and concluded on 19 November 2023, with Australia winning the tournament. A quad\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcan you tell me about this recent cricket worldcup?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:507\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    508\u001b[0m         _output_key\n\u001b[0;32m    509\u001b[0m     ]\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    513\u001b[0m         _output_key\n\u001b[0;32m    514\u001b[0m     ]\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    301\u001b[0m     inputs,\n\u001b[0;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1312\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1312\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1321\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1322\u001b[0m         )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1038\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1038\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1048\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1038\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1038\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1048\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1066\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1066\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:635\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \n\u001b[0;32m    625\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    634\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 635\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\llm.py:293\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    301\u001b[0m     inputs,\n\u001b[0;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:516\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    515\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:666\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    654\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    655\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m         )\n\u001b[0;32m    665\u001b[0m     ]\n\u001b[1;32m--> 666\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:553\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    552\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    554\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:540\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    532\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 540\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    544\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    548\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    549\u001b[0m         )\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:459\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    448\u001b[0m         {\n\u001b[0;32m    449\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m         }\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:114\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\resources\\completions.py:559\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "agent.run(\"can you tell me about this recent cricket worldcup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the current GDP of the United States\n",
      "Action: Wikipedia\n",
      "Action Input: US GDP\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Economy of the United States\n",
      "Summary: The United States is a highly developed/advanced market economy. It is the world's largest economy by nominal GDP, and the second-largest by purchasing power parity (PPP) behind China. It has the world's seventh-highest per capita GDP (nominal) and the eighth-highest per capita GDP (PPP) as of 2022. The U.S. accounted for 25.4% of the global economy in 2022 in nominal terms, and around 15.6% in PPP terms. The U.S. dollar is the currency of record most used in international transactions and is the world's reserve currency, backed by a large U.S. treasuries market, its role as the reference standard for the petrodollar system, and its linked eurodollar. Several countries use it as their official currency and in others it is the de facto currency.The American economy is fueled by high productivity, a well developed transportation infrastructure, and extensive natural resources. Americans have the highest average household and employee income among OECD member states. In 2021, they had the highest median household income. The U.S. has one of the world's highest income inequalities among the developed countries. The largest U.S. trading partners are Canada, Mexico, China, Japan, Germany, South Korea, the United Kingdom, Taiwan, India, and Vietnam. The U.S. is the world's largest importer and second-largest exporter. It has free trade agreements with several countries, including Canada and Mexico (through the USMCA), Australia, South Korea, Israel, and several others that are in effect or under negotiation.By 1890, the United States had overtaken the British Empire as the world's most productive economy. It is the world's largest producer of petroleum and natural gas. In 2016, it was the world's largest trading country as well as its third-largest manufacturer, with its manufacturing industry representing a fifth of the global manufacturing output. The U.S. not only has the largest internal market for goods, but also dominates the services trade. U.S. total trade amounted to $4.2 trillion in 2018. Of the world's 500 largest companies, 121 are headquartered in the U.S. The U.S. has the world's highest number of billionaires, with a total wealth of $3.0 trillion. US commercial banks had $22.9 trillion in assets as of December 2022. U.S. global assets under management had more than $30 trillion in assets. During the Great Recession of 2008, the U.S. economy suffered a significant decline. The American Reinvestment and Recovery Act was passed by the US administration, and in the years that followed, the U.S. experienced the longest economic expansion on record by July 2019.The New York Stock Exchange and Nasdaq are the world's largest stock exchanges by market capitalization and trade volume. In 2014, the U.S. economy was ranked first in international ranking on venture capital and global research and development funding. Consumer spending comprised 68% of the U.S. economy in 2022, while its labor share of income was 44% in 2021. The U.S. has the world's largest consumer market. The nation's labor market has attracted immigrants from all over the world and its net migration rate is among the highest in the world. The U.S. is one of the top-performing economies in studies such as the Ease of Doing Business Index, the Global Competitiveness Report, and others.\n",
      "\n",
      "Page: List of U.S. states and territories by GDP\n",
      "Summary: This is a list of U.S. states and territories by gross domestic product (GDP). This article presents the 50 U.S. states and the District of Columbia and their nominal GDP at current prices.\n",
      "The data source for the list is the Bureau of Economic Analysis (BEA) in 2022. The BEA defined GDP by state as \"the sum of value added from all industries in the state.\"Nominal GDP does not take into account differences in the cost of living in different countries, and the results can vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Suc\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcan you tell me what is current GDP of usa?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:507\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    508\u001b[0m         _output_key\n\u001b[0;32m    509\u001b[0m     ]\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    513\u001b[0m         _output_key\n\u001b[0;32m    514\u001b[0m     ]\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    301\u001b[0m     inputs,\n\u001b[0;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1312\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1312\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1321\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1322\u001b[0m         )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1038\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1038\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1048\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1038\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1031\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1038\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1048\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:1066\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1066\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\agents\\agent.py:635\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \n\u001b[0;32m    625\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    634\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 635\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\llm.py:293\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    301\u001b[0m     inputs,\n\u001b[0;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    303\u001b[0m )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    309\u001b[0m     )\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain\\chains\\llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:516\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    515\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:666\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m         )\n\u001b[0;32m    653\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    654\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    655\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m         )\n\u001b[0;32m    665\u001b[0m     ]\n\u001b[1;32m--> 666\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:553\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    552\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    554\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:540\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    532\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 540\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    544\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    548\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    549\u001b[0m         )\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:459\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    448\u001b[0m         {\n\u001b[0;32m    449\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m         }\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:114\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\resources\\completions.py:559\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\GenAI\\genvenv\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-rGeJAvOgfIVeJmMiPZ3ZUOjM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "agent.run(\"can you tell me what is current GDP of usa?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain\n",
    "Central to LangChain is a vital component known as LangChain Chains, forming the core connection among one or several large language models (LLMs). In certain sophisticated applications, it becomes necessary to chain LLMs together, either with each other or with other elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x0000015003E5CA10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x000001507F601190>, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate.from_template(\"what is a good name for a company that makes {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=LLMChain(llm=client,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VinVine Cellars'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Wine\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"i want to open a restaurent for {cuisine} food, suggest a fency name for this\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['cuisine'], template='i want to open a restaurent for {cuisine} food, suggest a fency name for this')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=LLMChain(llm=client, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Spice of India\"'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"indian\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=LLMChain(llm=client,prompt=prompt_template,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mi want to open a restaurent for american food, suggest a fency name for this\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe All-American Diner'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"american\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if we want to combine multiple chain and set a seqence for that we use simplesequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name=PromptTemplate(\n",
    "    input_variables=[\"startup_name\"],\n",
    "    template=\"I want to start a startup for {startup-name} , suggest me a good name for this\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_chain=LLMChain(llm=client,prompt=prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_items=PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"suggest some strategies for {name}\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_chain=LLMChain(llm=client,prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=SimpleSequentialChain(chains=[name_chain,strategies_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Develop a clear and achievable mission statement.\n",
      "2. Identify resources and create a budget.\n",
      "3. Research the industry and competitors.\n",
      "4. Develop a comprehensive business plan.\n",
      "5. Establish a minimum viable product.\n",
      "6. Build a team of experienced professionals.\n",
      "7. Secure capital and funding.\n",
      "8. Find a mentor or advisor.\n",
      "9. Utilize social media and digital marketing.\n",
      "10. Invest in research and development.\n",
      "11. Test the market before launching.\n",
      "12. Network and build relationships.\n",
      "13. Monitor trends and customer feedback.\n",
      "14. Focus on customer service and satisfaction.\n",
      "15. Keep up with technology advancements.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"artifical intelligence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets try to understand the sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name=PromptTemplate(\n",
    "    input_variables=[\"cuisine\"],\n",
    "    template=\"i want to open a restaurant for {cuisine}, suggest a fency name for it\"\n",
    ")\n",
    "\n",
    "name_chain=LLMChain(llm=client, prompt=prompt_template_name,output_key=\"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates_items=PromptTemplate( \n",
    "    input_variables=[\"restaurant_name\"],\n",
    "    template=\"suggest some menu items for {restaurant_name}\"\n",
    "    \n",
    ")\n",
    "\n",
    "food_items_chain=LLMChain(llm=client, prompt=prompt_templates_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=SequentialChain(chains=[name_chain, food_items_chain],\n",
    "    input_variables=[\"cuisine\"],\n",
    "    output_variables=[\"restaurant_name\",\"menu_items\"]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'indian', 'restaurant_name': '\\n\\nThe Taj Mahal Palace Cuisine.', 'menu_items': '\\n\\n-Murg Makhani\\n-Butter Chicken\\n-Dal Bukhara\\n-Tandoori Paneer Tikka\\n-Methi Malai Paneer\\n-Hyderabadi Biryani\\n-Veg Kebab Platter\\n-Matar Paneer\\n-Aloo Gobi\\n-Paneer Lababdar\\n-Chana Masala\\n-Mango Lassi\\n-Gulab Jamun'}\n"
     ]
    }
   ],
   "source": [
    "print(chain({\"cuisine\":\"indian\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.17.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading pypdf-3.17.2-py3-none-any.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/277.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/277.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/277.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/277.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/277.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/277.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/277.9 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 61.4/277.9 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------ -------------------------- 92.2/277.9 kB 880.9 kB/s eta 0:00:01\n",
      "   ------------ -------------------------- 92.2/277.9 kB 880.9 kB/s eta 0:00:01\n",
      "   ------------ -------------------------- 92.2/277.9 kB 880.9 kB/s eta 0:00:01\n",
      "   ------------ -------------------------- 92.2/277.9 kB 880.9 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 328.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 328.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 328.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 328.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 328.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 328.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 328.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 328.2 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 328.2 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 122.9/277.9 kB 156.9 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 122.9/277.9 kB 156.9 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 122.9/277.9 kB 156.9 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 122.9/277.9 kB 156.9 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/277.9 kB 200.1 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 194.6/277.9 kB 200.1 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 225.3/277.9 kB 218.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 225.3/277.9 kB 218.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 225.3/277.9 kB 218.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 225.3/277.9 kB 218.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 225.3/277.9 kB 218.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 225.3/277.9 kB 218.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 225.3/277.9 kB 218.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 245.8/277.9 kB 177.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 245.8/277.9 kB 177.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/277.9 kB 176.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 256.0/277.9 kB 176.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 277.9/277.9 kB 178.5 kB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"C:\\Users\\Rushikesh\\Downloads\\Practical Statistics for DataScientists.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x1ecdc942ad0>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Peter Bruce, Andrew Bruce  \\n& Peter Gedeck\\nSecond  \\nEdition\\nPractical\\nStatistics  \\n for Data Scientists\\n50+ Essential Concepts Using R and Python', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 0}),\n",
       " Document(page_content='Peter Bruce, Andrew Bruce, and Peter GedeckPractical Statistics for\\nData Scientists\\n50+ Essential Concepts Using R and PythonSECOND EDITION\\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 2}),\n",
       " Document(page_content='978-1-492-07294-2\\n[LSI]Practical Statistics for Data Scientists\\nby Peter Bruce, Andrew Bruce, and Peter Gedeck\\nCopyright © 2020 Peter Bruce, Andrew Bruce, and Peter Gedeck. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nEditor:  Nicole Tache\\nProduction Editor:  Kristen Brown\\nCopyeditor:  Piper Editorial\\nProofreader:  Arthur JohnsonIndexer:  Ellen Troutman-Zaig\\nInterior Designer:  David Futato\\nCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nMay 2017:  First Edition\\nMay 2020:  Second Edition\\nRevision History for the Second Edition\\n2020-04-10: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492072942  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Practical Statistics for Data Scientists ,\\nthe cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 3}),\n",
       " Document(page_content='Peter Bruce and Andrew Bruce would like to dedicate this book to the memories of our\\nparents, Victor G. Bruce and Nancy C. Bruce, who cultivated a passion for math and\\nscience; and to our early mentors John W . Tukey and Julian Simon and our lifelong\\nfriend Geoff  Watson, who helped inspire us to pursue a career in statistics.\\nPeter Gedeck would like to dedicate this book to Tim Clark and Christian Kramer, with\\ndeep thanks for their scientific  collaboration and friendship.', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 4}),\n",
       " Document(page_content='Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xiii\\n1.Exploratory Data Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nElements of Structured Data                                                                                            2\\nFurther Reading                                                                                                              4\\nRectangular Data                                                                                                                4\\nData Frames and Indexes                                                                                              6\\nNonrectangular Data Structures                                                                                  6\\nFurther Reading                                                                                                              7\\nEstimates of Location                                                                                                        7\\nMean                                                                                                                                 9\\nMedian and Robust Estimates                                                                                    10\\nExample: Location Estimates of Population and Murder Rates                            12\\nFurther Reading                                                                                                            13\\nEstimates of Variability                                                                                                   13\\nStandard Deviation and Related Estimates                                                               14\\nEstimates Based on Percentiles                                                                                   16\\nExample: Variability Estimates of State Population                                                 18\\nFurther Reading                                                                                                            19\\nExploring the Data Distribution                                                                                    19\\nPercentiles and Boxplots                                                                                             20\\nFrequency Tables and Histograms                                                                             22\\nDensity Plots and Estimates                                                                                        24\\nFurther Reading                                                                                                            26\\nExploring Binary and Categorical Data                                                                        27\\nMode                                                                                                                              29\\nExpected Value                                                                                                              29\\nProbability                                                                                                                     30\\nv', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 6}),\n",
       " Document(page_content='Further Reading                                                                                                            30\\nCorrelation                                                                                                                        30\\nScatterplots                                                                                                                    34\\nFurther Reading                                                                                                            36\\nExploring Two or More Variables                                                                                 36\\nHexagonal Binning and Contours (Plotting Numeric Versus Numeric Data)    36\\nTwo Categorical Variables                                                                                           39\\nCategorical and Numeric Data                                                                                   41\\nVisualizing Multiple Variables                                                                                    43\\nFurther Reading                                                                                                            46\\nSummary                                                                                                                           46\\n2.Data and Sampling Distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  47\\nRandom Sampling and Sample Bias                                                                             48\\nBias                                                                                                                                 50\\nRandom Selection                                                                                                        51\\nSize Versus Quality: When Does Size Matter?                                                         52\\nSample Mean Versus Population Mean                                                                     53\\nFurther Reading                                                                                                            53\\nSelection Bias                                                                                                                    54\\nRegression to the Mean                                                                                               55\\nFurther Reading                                                                                                            57\\nSampling Distribution of a Statistic                                                                              57\\nCentral Limit Theorem                                                                                                60\\nStandard Error                                                                                                              60\\nFurther Reading                                                                                                            61\\nThe Bootstrap                                                                                                                   61\\nResampling Versus Bootstrapping                                                                             65\\nFurther Reading                                                                                                            65\\nConfidence Intervals                                                                                                       65\\nFurther Reading                                                                                                            68\\nNormal Distribution                                                                                                        69\\nStandard Normal and QQ-Plots                                                                                71\\nLong-Tailed Distributions                                                                                              73', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 7}),\n",
       " Document(page_content='Long-Tailed Distributions                                                                                              73\\nFurther Reading                                                                                                            75\\nStudent’s t-Distribution                                                                                                   75\\nFurther Reading                                                                                                            78\\nBinomial Distribution                                                                                                     78\\nFurther Reading                                                                                                            80\\nChi-Square Distribution                                                                                                 80\\nFurther Reading                                                                                                            81\\nF-Distribution                                                                                                                   82\\nvi | Table of Contents', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 7}),\n",
       " Document(page_content='Further Reading                                                                                                            82\\nPoisson and Related Distributions                                                                                82\\nPoisson Distributions                                                                                                  83\\nExponential Distribution                                                                                            84\\nEstimating the Failure Rate                                                                                         84\\nWeibull Distribution                                                                                                    85\\nFurther Reading                                                                                                            86\\nSummary                                                                                                                           86\\n3.Statistical Experiments and Significance  Testing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87\\nA/B Testing                                                                                                                       88\\nWhy Have a Control Group?                                                                                      90\\nWhy Just A/B? Why Not C, D,…?                                                                             91\\nFurther Reading                                                                                                            92\\nHypothesis Tests                                                                                                               93\\nThe Null Hypothesis                                                                                                    94\\nAlternative Hypothesis                                                                                                95\\nOne-Way Versus Two-Way Hypothesis Tests                                                           95\\nFurther Reading                                                                                                            96\\nResampling                                                                                                                        96\\nPermutation Test                                                                                                          97\\nExample: Web Stickiness                                                                                             98\\nExhaustive and Bootstrap Permutation Tests                                                         102\\nPermutation Tests: The Bottom Line for Data Science                                         102\\nFurther Reading                                                                                                          103\\nStatistical Significance and p-Values                                                                           103\\np-Value                                                                                                                         106\\nAlpha                                                                                                                            107\\nType 1 and Type 2 Errors                                                                                          109\\nData Science and p-Values                                                                                        109\\nFurther Reading                                                                                                          110\\nt-Tests                                                                                                                               110\\nFurther Reading                                                                                                          112', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 8}),\n",
       " Document(page_content='Further Reading                                                                                                          112\\nMultiple Testing                                                                                                             112\\nFurther Reading                                                                                                          116\\nDegrees of Freedom                                                                                                       116\\nFurther Reading                                                                                                          118\\nANOV A                                                                                                                           118\\nF-Statistic                                                                                                                     121\\nTwo-Way ANOV A                                                                                                     123\\nFurther Reading                                                                                                          124\\nChi-Square Test                                                                                                              124\\nTable of Contents | vii', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 8}),\n",
       " Document(page_content='Chi-Square Test: A Resampling Approach                                                             124\\nChi-Square Test: Statistical Theory                                                                         127\\nFisher’s Exact Test                                                                                                       128\\nRelevance for Data Science                                                                                       130\\nFurther Reading                                                                                                          131\\nMulti-Arm Bandit Algorithm                                                                                      131\\nFurther Reading                                                                                                          134\\nPower and Sample Size                                                                                                  135\\nSample Size                                                                                                                  136\\nFurther Reading                                                                                                          138\\nSummary                                                                                                                         139\\n4.Regression and Prediction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  141\\nSimple Linear Regression                                                                                             141\\nThe Regression Equation                                                                                          143\\nFitted Values and Residuals                                                                                      146\\nLeast Squares                                                                                                               148\\nPrediction Versus Explanation (Profiling)                                                             149\\nFurther Reading                                                                                                          150\\nMultiple Linear Regression                                                                                          150\\nExample: King County Housing Data                                                                     151\\nAssessing the Model                                                                                                   153\\nCross-Validation                                                                                                         155\\nModel Selection and Stepwise Regression                                                              156\\nWeighted Regression                                                                                                  159\\nFurther Reading                                                                                                          161\\nPrediction Using Regression                                                                                        161\\nThe Dangers of Extrapolation                                                                                  161\\nConfidence and Prediction Intervals                                                                      161\\nFactor Variables in Regression                                                                                     163\\nDummy Variables Representation                                                                           164\\nFactor Variables with Many Levels                                                                          167\\nOrdered Factor Variables                                                                                          169\\nInterpreting the Regression Equation                                                                         169', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 9}),\n",
       " Document(page_content='Interpreting the Regression Equation                                                                         169\\nCorrelated Predictors                                                                                                 170\\nMulticollinearity                                                                                                         172\\nConfounding Variables                                                                                             172\\nInteractions and Main Effects                                                                                  174\\nRegression Diagnostics                                                                                                 176\\nOutliers                                                                                                                        177\\nInfluential Values                                                                                                        179\\nHeteroskedasticity, Non-Normality, and Correlated Errors                                182\\nviii | Table of Contents', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 9}),\n",
       " Document(page_content='Partial Residual Plots and Nonlinearity                                                                  185\\nPolynomial and Spline Regression                                                                              187\\nPolynomial                                                                                                                  188\\nSplines                                                                                                                          189\\nGeneralized Additive Models                                                                                   192\\nFurther Reading                                                                                                          193\\nSummary                                                                                                                         194\\n5.Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  195\\nNaive Bayes                                                                                                                     196\\nWhy Exact Bayesian Classification Is Impractical                                                 197\\nThe Naive Solution                                                                                                    198\\nNumeric Predictor Variables                                                                                    200\\nFurther Reading                                                                                                          201\\nDiscriminant Analysis                                                                                                   201\\nCovariance Matrix                                                                                                      202\\nFisher’s Linear Discriminant                                                                                    203\\nA Simple Example                                                                                                      204\\nFurther Reading                                                                                                          207\\nLogistic Regression                                                                                                        208\\nLogistic Response Function and Logit                                                                    208\\nLogistic Regression and the GLM                                                                            210\\nGeneralized Linear Models                                                                                       212\\nPredicted Values from Logistic Regression                                                            212\\nInterpreting the Coefficients and Odds Ratios                                                      213\\nLinear and Logistic Regression: Similarities and Differences                              214\\nAssessing the Model                                                                                                   216\\nFurther Reading                                                                                                          219\\nEvaluating Classification Models                                                                                 219\\nConfusion Matrix                                                                                                       221\\nThe Rare Class Problem                                                                                            223\\nPrecision, Recall, and Specificity                                                                              223\\nROC Curve                                                                                                                  224\\nAUC                                                                                                                              226', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 10}),\n",
       " Document(page_content='AUC                                                                                                                              226\\nLift                                                                                                                                228\\nFurther Reading                                                                                                          229\\nStrategies for Imbalanced Data                                                                                    230\\nUndersampling                                                                                                           231\\nOversampling and Up/Down Weighting                                                                232\\nData Generation                                                                                                         233\\nCost-Based Classification                                                                                          234\\nExploring the Predictions                                                                                         234\\nTable of Contents | ix', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 10}),\n",
       " Document(page_content='Further Reading                                                                                                          236\\nSummary                                                                                                                         236\\n6.Statistical Machine Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  237\\nK-Nearest Neighbors                                                                                                     238\\nA Small Example: Predicting Loan Default                                                            239\\nDistance Metrics                                                                                                         241\\nOne Hot Encoder                                                                                                       242\\nStandardization (Normalization, z-Scores)                                                            243\\nChoosing K                                                                                                                  246\\nKNN as a Feature Engine                                                                                          247\\nTree Models                                                                                                                    249\\nA Simple Example                                                                                                      250\\nThe Recursive Partitioning Algorithm                                                                    252\\nMeasuring Homogeneity or Impurity                                                                     254\\nStopping the Tree from Growing                                                                             256\\nPredicting a Continuous Value                                                                                257\\nHow Trees Are Used                                                                                                  258\\nFurther Reading                                                                                                          259\\nBagging and the Random Forest                                                                                 259\\nBagging                                                                                                                        260\\nRandom Forest                                                                                                           261\\nVariable Importance                                                                                                  265\\nHyperparameters                                                                                                        269\\nBoosting                                                                                                                          270\\nThe Boosting Algorithm                                                                                           271\\nXGBoost                                                                                                                       272\\nRegularization: Avoiding Overfitting                                                                      274\\nHyperparameters and Cross-Validation                                                                 279\\nSummary                                                                                                                         282\\n7.Unsupervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  283\\nPrincipal Components Analysis                                                                                  284\\nA Simple Example                                                                                                      285\\nComputing the Principal Components                                                                   288', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 11}),\n",
       " Document(page_content='Computing the Principal Components                                                                   288\\nInterpreting Principal Components                                                                        289\\nCorrespondence Analysis                                                                                         292\\nFurther Reading                                                                                                          294\\nK-Means Clustering                                                                                                      294\\nA Simple Example                                                                                                      295\\nK-Means Algorithm                                                                                                   298\\nInterpreting the Clusters                                                                                           299\\nx | Table of Contents', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 11}),\n",
       " Document(page_content='Selecting the Number of Clusters                                                                            302\\nHierarchical Clustering                                                                                                 304\\nA Simple Example                                                                                                      305\\nThe Dendrogram                                                                                                        306\\nThe Agglomerative Algorithm                                                                                 308\\nMeasures of Dissimilarity                                                                                         309\\nModel-Based Clustering                                                                                               311\\nMultivariate Normal Distribution                                                                           311\\nMixtures of Normals                                                                                                  312\\nSelecting the Number of Clusters                                                                            315\\nFurther Reading                                                                                                          318\\nScaling and Categorical Variables                                                                                318\\nScaling the Variables                                                                                                  319\\nDominant Variables                                                                                                   321\\nCategorical Data and Gower’s Distance                                                                  322\\nProblems with Clustering Mixed Data                                                                    325\\nSummary                                                                                                                         326\\nBibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  327\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  329\\nTable of Contents | xi', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 12}),\n",
       " Document(page_content='Preface\\nThis book is aimed at the data scientist with some familiarity with the R and/or\\nPython  programming languages, and with some prior (perhaps spotty or ephemeral)\\nexposure to statistics. Two of the authors came to the world of data science from the\\nworld of statistics, and have some appreciation of the contribution that statistics can\\nmake to the art of data science. At the same time, we are well aware of the limitations\\nof traditional statistics instruction: statistics as a discipline is a century and a half old,\\nand most statistics textbooks and courses are laden with the momentum and inertia\\nof an ocean liner. All the methods in this book have some connection—historical or\\nmethodological—to the discipline of statistics. Methods that evolved mainly out of\\ncomputer science, such as neural nets, are not included.\\nTwo goals underlie this book:\\n•To lay out, in digestible, navigable, and easily referenced form, key concepts from\\nstatistics that are relevant to data science.\\n•To explain which concepts are important and useful from a data science perspec‐\\ntive, which are less so, and why.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nxiii', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 14}),\n",
       " Document(page_content='Constant width bold\\nShows commands or other text that should be typed literally by the user.\\nKey Terms\\nData science is a fusion of multiple disciplines, including statistics, computer science,\\ninformation technology, and domain-specific fields. As a result, several different\\nterms could be used to reference a given concept. Key terms and their synonyms will\\nbe highlighted throughout the book in a sidebar such as this.\\nThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nUsing Code Examples\\nIn all cases, this book gives code examples first in R and then in Python . In order to\\navoid unnecessary repetition, we generally show only output and plots created by the\\nR code. We also skip the code required to load the required packages and data sets.\\nY ou can find the complete code as well as the data sets for download at https://\\ngithub.com/gedeck/practical-statistics-for-data-scientists .\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing examples from O’Reilly\\nbooks does require permission. Answering a question by citing this book and quoting\\nexample code does not require permission. Incorporating a significant amount of\\nxiv | Preface', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 15}),\n",
       " Document(page_content='example code from this book into your product’s documentation does require per‐\\nmission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Practical Statistics for Data Scientists\\nby Peter Bruce, Andrew Bruce, and Peter Gedeck (O’Reilly). Copyright 2020 Peter\\nBruce, Andrew Bruce, and Peter Gedeck, 978-1-492-07294-2. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com .\\nO’Reilly Online Learning\\nFor more than 40 years, O’Reilly Media  has provided technol‐\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at https://oreil.ly/practicalStats_dataSci_2e .\\nEmail bookquestions@oreilly.com  to comment or ask technical questions about this\\nbook.\\nFor news and more information about our books and courses, see our website at\\nhttp://oreilly.com .\\nPreface | xv', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 16}),\n",
       " Document(page_content='Find us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nAcknowledgments\\nThe authors acknowledge the many people who helped make this book a reality.\\nGerhard Pilcher, CEO of the data mining firm Elder Research, saw early drafts of the\\nbook and gave us detailed and helpful corrections and comments. Likewise, Anya\\nMcGuirk and Wei Xiao, statisticians at SAS, and Jay Hilfiger, fellow O’Reilly author,\\nprovided helpful feedback on initial drafts of the book. Toshiaki Kurokawa, who\\ntranslated the first edition into Japanese, did a comprehensive job of reviewing and\\ncorrecting in the process. Aaron Schumacher and Walter Paczkowski thoroughly\\nreviewed the second edition of the book and provided numerous helpful and valuable\\nsuggestions for which we are extremely grateful. Needless to say, any errors that\\nremain are ours alone.\\nAt O’Reilly, Shannon Cutt has shepherded us through the publication process with\\ngood cheer and the right amount of prodding, while Kristen Brown smoothly took\\nour book through the production phase. Rachel Monaghan and Eliahu Sussman cor‐\\nrected and improved our writing with care and patience, while Ellen Troutman-Zaig\\nprepared the index. Nicole Tache took over the reins for the second edition and has\\nboth guided the process effectively and provided many good editorial suggestions to\\nimprove the readability of the book for a broad audience. We also thank Marie Beau‐\\ngureau, who initiated our project at O’Reilly, as well as Ben Bengfort, O’Reilly author\\nand Statistics.com instructor, who introduced us to O’Reilly.\\nWe, and this book, have also benefited from the many conversations Peter has had\\nover the years with Galit Shmueli, coauthor on other book projects.\\nFinally, we would like to especially thank Elizabeth Bruce and Deborah Donnell,\\nwhose patience and support made this endeavor possible.\\nxvi | Preface', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 17}),\n",
       " Document(page_content='CHAPTER 1\\nExploratory Data Analysis\\nThis chapter focuses on the first step in any data science project: exploring the data.\\nClassical statistics focused almost exclusively on inference , a sometimes complex set\\nof procedures for drawing conclusions about large populations based on small sam‐\\nples. In 1962, John W . Tukey  (Figure 1-1 ) called for a reformation of statistics in his\\nseminal paper “The Future of Data Analysis” [Tukey-1962] . He proposed a new scien‐\\ntific discipline called data analysis  that included statistical inference as just one com‐\\nponent. Tukey forged links to the engineering and computer science communities (he\\ncoined the terms bit, short for binary digit, and software ), and his original tenets are\\nsurprisingly durable and form part of the foundation for data science.  The field of\\nexploratory data analysis was established with Tukey’s 1977 now-classic book Explor‐\\natory Data Analysis  [Tukey-1977] . Tukey presented simple plots (e.g., boxplots, scat‐\\nterplots) that, along with summary statistics (mean, median, quantiles, etc.), help\\npaint a picture of a data set.\\nWith the ready availability of computing power and expressive data analysis software,\\nexploratory data analysis has evolved well beyond its original scope. Key drivers of\\nthis discipline have been the rapid development of new technology, access to more\\nand bigger data, and the greater use of quantitative analysis in a variety of disciplines.\\nDavid Donoho, professor of statistics at Stanford University and former undergradu‐\\nate student of Tukey’s, authored an excellent article based on his presentation at the\\nTukey Centennial workshop in Princeton, New Jersey [Donoho-2015] . Donoho traces\\nthe genesis of data science back to Tukey’s pioneering work in data analysis.\\n1', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 18}),\n",
       " Document(page_content='Figure 1-1. John Tukey, the eminent statistician whose ideas developed over 50 years ago\\nform the foundation of data science\\nElements of Structured Data\\nData comes from many sources: sensor measurements, events, text, images, and vid‐\\neos. The Internet of Things  (IoT) is spewing out streams of information. Much of this\\ndata is unstructured: images are a collection of pixels, with each pixel containing RGB\\n(red, green, blue) color information. Texts are sequences of words and nonword char‐\\nacters, often organized by sections, subsections, and so on. Clickstreams are sequen‐\\nces of actions by a user interacting with an app or a web page. In fact, a major\\nchallenge of data science is to harness this torrent of raw data into actionable infor‐\\nmation. To apply the statistical concepts covered in this book, unstructured raw data\\nmust be processed and manipulated into a structured form. One of the commonest\\nforms of structured data is a table with rows and columns—as data might emerge\\nfrom a relational database or be collected for a study.\\nThere are two basic types of structured data: numeric and categorical. Numeric data\\ncomes in two forms: continuous , such as wind speed or time duration, and discrete ,\\nsuch as the count of the occurrence of an event. Categorical  data takes only a fixed set\\nof values, such as a type of TV screen (plasma, LCD, LED, etc.) or a state name (Ala‐\\nbama, Alaska, etc.). Binary  data is an important special case of categorical data that\\ntakes on only one of two values, such as 0/1, yes/no, or true/false.  Another useful type\\nof categorical data is ordinal  data in which the categories are ordered; an example of\\nthis is a numerical rating (1, 2, 3, 4, or 5).\\nWhy do we bother with a taxonomy of data types? It turns out that for the purposes\\nof data analysis and predictive modeling, the data type is important to help determine\\nthe type of visual display, data analysis, or statistical model. In fact, data science\\nsoftware , such as R and Python , uses these data types to improve computational per‐\\nformance. More important, the data type for a variable determines how software will\\nhandle computations for that variable.\\n2 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 19}),\n",
       " Document(page_content='Key Terms for Data Types\\nNumeric\\nData that are expressed on a numeric scale.\\nContinuous\\nData that can take on any value in an interval. ( Synonyms : interval, float,\\nnumeric)\\nDiscrete\\nData that can take on only integer values, such as counts. ( Synonyms : integer,\\ncount)\\nCategorical\\nData that can take on only a specific set of values representing a set of possible\\ncategories. ( Synonyms : enums, enumerated, factors, nominal)\\nBinary\\nA special case of categorical data with just two categories of values, e.g., 0/1,\\ntrue/false. ( Synonyms : dichotomous, logical, indicator, boolean)\\nOrdinal\\nCategorical data that has an explicit ordering. ( Synonym : ordered factor)\\nSoftware engineers and database programmers may wonder why we even need the\\nnotion of categorical  and ordinal  data for analytics. After all, categories are merely a\\ncollection of text (or numeric) values, and the underlying database automatically han‐\\ndles the internal representation. However, explicit identification of data as categorical,\\nas distinct from text, does offer some advantages:\\n•Knowing that data is categorical can act as a signal telling software how statistical\\nprocedures, such as producing a chart or fitting a model, should behave. In par‐\\nticular, ordinal data can be represented as an ordered.factor  in R, preserving a\\nuser-specified ordering in charts, tables, and models. In Python , scikit-learn\\nsupports ordinal data with the sklearn.preprocessing.OrdinalEncoder .\\n•Storage and indexing can be optimized (as in a relational database).\\n•The possible values a given categorical variable can take are enforced in the soft‐\\nware (like an enum).\\nThe third “benefit” can lead to unintended or unexpected behavior: the default\\nbehavior of data import functions in R (e.g., read.csv ) is to automatically convert a\\ntext column into a factor . Subsequent operations on that column will assume that\\nthe only allowable values for that column are the ones originally imported, and\\nassigning a new text value will introduce a warning and produce an NA (missing\\nElements of Structured Data | 3', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 20}),\n",
       " Document(page_content='value). The pandas  package in Python  will not make such a conversion automatically.\\nHowever, you can specify a column as categorical explicitly in the read_csv  function.\\nKey Ideas\\n•Data is typically classified in software by type.\\n•Data types include numeric (continuous, discrete) and categorical (binary,\\nordinal).\\n•Data typing in software acts as a signal to the software on how to process the\\ndata.\\nFurther Reading\\n•The pandas  documentation  describes  the different data types and how they can\\nbe manipulated in Python .\\n•Data types can be confusing, since types may overlap, and the taxonomy in one\\nsoftware may differ from that in another. The R Tutorial website  covers the\\ntaxonomy for R. The pandas  documentation  describes the different data types\\nand how they can be manipulated in Python .\\n•Databases are more detailed in their classification of data types, incorporating\\nconsiderations of precision levels, fixed- or variable-length fields, and more;  see\\nthe W3Schools guide to SQL .\\nRectangular Data\\nThe typical frame of reference for an analysis in data science is a rectangular data\\nobject, like a spreadsheet or database table.\\nRectangular data  is the general term for a two-dimensional matrix with rows indicat‐\\ning records (cases) and columns indicating features (variables); data frame  is the spe‐\\ncific format in R and Python . The data doesn’t always start in this form: unstructured\\ndata (e.g., text) must be processed and manipulated so that it can be represented as a\\nset of features in the rectangular data (see “Elements of Structured Data”  on page 2).\\nData in relational databases must be extracted and put into a single table for most\\ndata analysis and modeling tasks.\\n4 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 21}),\n",
       " Document(page_content='Key Terms for Rectangular Data\\nData frame\\nRectangular data (like a spreadsheet) is the basic data structure for statistical and\\nmachine learning models.\\nFeature\\nA column within a table is commonly referred to as a feature .\\nSynonyms\\nattribute, input, predictor, variable\\nOutcome\\nMany data science projects involve predicting an outcome —often a yes/no out‐\\ncome (in Table 1-1 , it is “auction was competitive or not”). The features  are some‐\\ntimes used to predict the outcome  in an experiment or a study.\\nSynonyms\\ndependent variable, response, target, output\\nRecords\\nA row within a table is commonly referred to as a record .\\nSynonyms\\ncase, example, instance, observation, pattern, sample\\nTable 1-1. A typical data frame format\\nCategory currency sellerRating Duration endDay ClosePrice OpenPrice Competitive?\\nMusic/Movie/Game US 3249 5 Mon 0.01 0.01 0\\nMusic/Movie/Game US 3249 5 Mon 0.01 0.01 0\\nAutomotive US 3115 7 Tue 0.01 0.01 0\\nAutomotive US 3115 7 Tue 0.01 0.01 0\\nAutomotive US 3115 7 Tue 0.01 0.01 0\\nAutomotive US 3115 7 Tue 0.01 0.01 0\\nAutomotive US 3115 7 Tue 0.01 0.01 1\\nAutomotive US 3115 7 Tue 0.01 0.01 1\\nIn Table 1-1 , there is a mix of measured or counted data (e.g., duration and price) and\\ncategorical data (e.g., category and currency). As mentioned earlier, a special form of\\ncategorical variable is a binary (yes/no or 0/1) variable, seen in the rightmost column\\nin Table 1-1 —an indicator variable showing whether an auction was competitive (had\\nmultiple bidders) or not. This indicator variable also happens to be an outcome  vari‐\\nable, when the scenario is to predict whether an auction is competitive or not.\\nRectangular Data | 5', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 22}),\n",
       " Document(page_content='Data Frames and Indexes\\nTraditional database tables have one or more columns designated as an index, essen‐\\ntially a row number. This can vastly improve the efficiency of certain database quer‐\\nies. In Python , with the pandas  library, the basic rectangular data structure is a\\nDataFrame  object. By default, an automatic integer index is created for a DataFrame\\nbased on the order of the rows. In pandas , it is also possible to set multilevel/hier‐\\narchical indexes to improve the efficiency of certain operations.\\nIn R, the basic rectangular data structure is a data.frame  object. A data.frame  also\\nhas an implicit integer index based on the row order. The native R data.frame  does\\nnot support user-specified or multilevel indexes, though a custom key can be created\\nthrough the row.names  attribute. To overcome this deficiency, two new packages are\\ngaining widespread use: data.table  and dplyr . Both support multilevel indexes and\\noffer significant speedups in working with a data.frame .\\nTerminology Differences\\nTerminology for rectangular data can be confusing. Statisticians\\nand data scientists use different terms for the same thing. For a sta‐\\ntistician, predictor variables  are used in a model to predict a\\nresponse  or dependent variable . For a data scientist, features  are used\\nto predict a target . One synonym is particularly confusing: com‐\\nputer scientists will use the term sample  for a single row; a sample\\nto a statistician means a collection of rows.\\nNonrectangular Data Structures\\nThere are other data structures besides rectangular data.\\nTime series data records successive measurements of the same variable. It is the raw\\nmaterial for statistical forecasting methods, and it is also a key component of the data\\nproduced by devices—the Internet of Things.\\nSpatial data structures, which are used in mapping and location analytics, are more\\ncomplex and varied than rectangular data structures. In the object  representation, the\\nfocus of the data is an object (e.g., a house) and its spatial coordinates.  The field view,\\nby contrast, focuses on small units of space and the value of a relevant metric (pixel\\nbrightness, for example).\\n6 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 23}),\n",
       " Document(page_content='Graph (or network) data structures are used to represent physical, social, and abstract\\nrelationships.  For example, a graph of a social network, such as Facebook or\\nLinkedIn, may represent connections between people on the network. Distribution\\nhubs connected by roads are an example of a physical network. Graph structures are\\nuseful for certain types of problems, such as network optimization and recommender\\nsystems.\\nEach of these data types has its specialized methodology in data science. The focus of\\nthis book is on rectangular data, the fundamental building block of predictive\\nmodeling.\\nGraphs in Statistics\\nIn computer science and information technology, the term graph\\ntypically refers to a depiction of the connections among entities,\\nand to the underlying data structure. In statistics, graph  is used to\\nrefer to a variety of plots and visualizations , not just of connections\\namong entities, and the term applies only to the visualization, not\\nto the data structure.\\nKey Ideas\\n•The basic data structure in data science is a rectangular matrix in which rows are\\nrecords and columns are variables (features).\\n•Terminology can be confusing; there are a variety of synonyms arising from the\\ndifferent disciplines that contribute to data science (statistics, computer science,\\nand information technology).\\nFurther Reading\\n•Documentation on data frames in R\\n•Documentation on data frames in Python\\nEstimates of Location\\nVariables with measured or count data might have thousands of distinct values. A\\nbasic step in exploring your data is getting a “typical value” for each feature (variable):\\nan estimate of where most of the data is located (i.e., its central tendency).\\nEstimates of Location | 7', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 24}),\n",
       " Document(page_content='Key Terms for Estimates of Location\\nMean\\nThe sum of all values divided by the number of values.\\nSynonym\\naverage\\nWeighted mean\\nThe sum of all values times a weight divided by the sum of the weights.\\nSynonym\\nweighted average\\nMedian\\nThe value such that one-half of the data lies above and below.\\nSynonym\\n50th percentile\\nPercentile\\nThe value such that P percent of the data lies below.\\nSynonym\\nquantile\\nWeighted median\\nThe value such that one-half of the sum of the weights lies above and below the\\nsorted data.\\nTrimmed mean\\nThe average of all values after dropping a fixed number of extreme values.\\nSynonym\\ntruncated mean\\nRobust\\nNot sensitive to extreme values.\\nSynonym\\nresistant\\nOutlier\\nA data value that is very different from most of the data.\\nSynonym\\nextreme value\\n8 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 25}),\n",
       " Document(page_content='At first glance, summarizing data might seem fairly trivial: just take the mean  of the\\ndata.  In fact, while the mean is easy to compute and expedient to use, it may not\\nalways be the best measure for a central value. For this reason, statisticians have\\ndeveloped and promoted several alternative estimates to the mean.\\nMetrics and Estimates\\nStatisticians often use the term estimate  for a value calculated from\\nthe data at hand, to draw a distinction between what we see from\\nthe data and the theoretical true or exact state of affairs.  Data scien‐\\ntists and business analysts are more likely to refer to such a value as\\na metric . The difference reflects the approach of statistics versus\\nthat of data science: accounting for uncertainty lies at the heart of\\nthe discipline of statistics, whereas concrete business or organiza‐\\ntional objectives are the focus of data science. Hence, statisticians\\nestimate, and data scientists measure.\\nMean\\nThe most basic estimate of location is the mean, or average  value.  The mean is the\\nsum of all values divided by the number of values. Consider the following set of num‐\\nbers: {3 5 1 2}. The mean is (3 + 5 + 1 + 2) / 4 = 11 / 4 = 2.75. Y ou will encounter the\\nsymbol x (pronounced “x-bar”) being used to represent the mean of a sample from a\\npopulation. The formula to compute the mean for a set of n values x1,x2, ...,xn is:\\nMean = x=∑i=1nxi\\nn\\nN (or n) refers to the total number of records or observations. In\\nstatistics it is capitalized if it is referring to a population, and lower‐\\ncase if it refers to a sample from a population. In data science, that\\ndistinction is not vital, so you may see it both ways.\\nA variation of the mean is a trimmed mean , which you calculate by dropping a fixed\\nnumber of sorted values at each end and then taking an average of the remaining val‐\\nues. Representing the sorted values by x1,x2, ...,xn where x1 is the smallest value\\nand xn the largest, the formula to compute the trimmed mean with p smallest and\\nlargest values omitted is:\\nTrimmed mean = x=∑i=p+ 1n−pxi\\nn− 2p\\nEstimates of Location | 9', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 26}),\n",
       " Document(page_content='A trimmed mean eliminates the influence of extreme values. For example, in interna‐\\ntional diving the top score and bottom score from five judges are dropped, and the\\nfinal score is the average of the scores from the three remaining judges . This makes it\\ndifficult for a single judge to manipulate the score, perhaps to favor their country’s\\ncontestant. Trimmed means are widely used, and in many cases are preferable to\\nusing the ordinary mean—see “Median and Robust Estimates”  on page 10 for further\\ndiscussion.\\nAnother  type of mean is a weighted mean , which you calculate by multiplying each\\ndata value xi by a user-specified weight wi and dividing their sum by the sum of the\\nweights. The formula for a weighted mean is:\\nWeighted mean = xw=∑i= 1nwixi\\n∑i= 1nwi\\nThere are two main motivations for using a weighted mean:\\n•Some values are intrinsically more variable than others, and highly variable\\nobservations are given a lower weight. For example, if we are taking the average\\nfrom multiple sensors and one of the sensors is less accurate, then we might\\ndownweight the data from that sensor.\\n•The data collected does not equally represent the different groups that we are\\ninterested in measuring. For example, because of the way an online experiment\\nwas conducted, we may not have a set of data that accurately reflects all groups in\\nthe user base. To correct that, we can give a higher weight to the values from the\\ngroups that were underrepresented.\\nMedian and Robust Estimates\\nThe median  is the middle number on a sorted list of the data.  If there is an even num‐\\nber of data values, the middle value is one that is not actually in the data set, but\\nrather the average of the two values that divide the sorted data into upper and lower\\nhalves. Compared to the mean, which uses all observations, the median depends only\\non the values in the center of the sorted data. While this might seem to be a disadvan‐\\ntage, since the mean is much more sensitive to the data, there are many instances in\\nwhich the median is a better metric for location. Let’s say we want to look at typical\\nhousehold incomes in neighborhoods around Lake Washington in Seattle. In com‐\\nparing the Medina neighborhood to the Windermere neighborhood, using the mean\\nwould produce very different results because Bill Gates lives in Medina. If we use the\\nmedian, it won’t matter how rich Bill Gates is—the position of the middle observation\\nwill remain the same.\\n10 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 27}),\n",
       " Document(page_content='For the same reasons that one uses a weighted mean, it is also possible to compute a\\nweighted median . As with the median, we first sort the data, although each data value\\nhas an associated weight. Instead of the middle number, the weighted median is a\\nvalue such that the sum of the weights is equal for the lower and upper halves of the\\nsorted list. Like the median, the weighted median is robust to outliers.\\nOutliers\\nThe median is referred to as a robust  estimate of location since it is not influenced by\\noutliers  (extreme cases) that could skew the results.  An outlier is any value that is very\\ndistant from the other values in a data set. The exact definition of an outlier is some‐\\nwhat subjective, although certain conventions are used in various data summaries\\nand plots (see “Percentiles and Boxplots” on page 20). Being an outlier in itself does\\nnot make a data value invalid or erroneous (as in the previous example with Bill\\nGates). Still, outliers are often the result of data errors such as mixing data of different\\nunits (kilometers versus meters) or bad readings from a sensor. When outliers are the\\nresult of bad data, the mean will result in a poor estimate of location, while the\\nmedian will still be valid. In any case, outliers should be identified and are usually\\nworthy of further investigation.\\nAnomaly Detection\\nIn contrast to typical data analysis, where outliers are sometimes\\ninformative and sometimes a nuisance, in anomaly detection  the\\npoints of interest are the outliers, and the greater mass of data\\nserves primarily to define the “normal” against which anomalies\\nare measured.\\nThe median is not the only robust estimate of location. In fact, a trimmed mean is\\nwidely used to avoid the influence of outliers. For example, trimming the bottom and\\ntop 10% (a common choice) of the data will provide protection against outliers in all\\nbut the smallest data sets. The trimmed mean can be thought of as a compromise\\nbetween the median and the mean: it is robust to extreme values in the data, but uses\\nmore data to calculate the estimate for location.\\nOther Robust Metrics for Location\\nStatisticians have developed a plethora of other estimators for loca‐\\ntion, primarily with the goal of developing an estimator more\\nrobust than the mean and also more efficient (i.e., better able to\\ndiscern small location differences between data sets). While these\\nmethods are potentially useful for small data sets, they are not\\nlikely to provide added benefit for large or even moderately sized\\ndata sets.\\nEstimates of Location | 11', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 28}),\n",
       " Document(page_content=\"Example: Location Estimates of Population and Murder Rates\\nTable 1-2  shows the first few rows in the data set containing population and murder\\nrates (in units of murders per 100,000 people per year) for each US state (2010\\nCensus).\\nTable 1-2. A few rows of the data.frame  state of population and murder rate by state\\nState Population Murder rate Abbreviation\\n1 Alabama 4,779,736 5.7 AL\\n2 Alaska 710,231 5.6 AK\\n3 Arizona 6,392,017 4.7 AZ\\n4 Arkansas 2,915,918 5.6 AR\\n5 California 37,253,956 4.4 CA\\n6 Colorado 5,029,196 2.8 CO\\n7 Connecticut 3,574,097 2.4 CT\\n8 Delaware 897,934 5.8 DE\\nCompute the mean, trimmed mean, and median for the population using R:\\n> state <- read.csv ('state.csv' )\\n> mean(state[['Population' ]])\\n[1] 6162876\\n> mean(state[['Population' ]], trim=0.1)\\n[1] 4783697\\n> median(state[['Population' ]])\\n[1] 4436370\\nTo compute mean and median in Python  we can use the pandas  methods of the data\\nframe. The trimmed mean requires the trim_mean  function in scipy.stats :\\nstate = pd.read_csv ('state.csv' )\\nstate['Population' ].mean()\\ntrim_mean (state['Population' ], 0.1)\\nstate['Population' ].median()\\nThe mean is bigger than the trimmed mean, which is bigger than the median.\\nThis is because the trimmed mean excludes the largest and smallest five states\\n(trim=0.1  drops 10% from each end). If we want to compute the average murder rate\\nfor the country, we need to use a weighted mean or median to account for different\\npopulations in the states. Since base R doesn’t have a function for weighted median,\\nwe need to install a package such as matrixStats :\\n> weighted.mean (state[['Murder.Rate' ]], w=state[['Population' ]])\\n[1] 4.445834\\n> library('matrixStats' )\\n12 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 29}),\n",
       " Document(page_content=\"> weightedMedian (state[['Murder.Rate' ]], w=state[['Population' ]])\\n[1] 4.4\\nWeighted mean is available with NumPy . For weighted median, we can use the special‐\\nized package wquantiles :\\nnp.average(state['Murder.Rate' ], weights=state['Population' ])\\nwquantiles .median(state['Murder.Rate' ], weights=state['Population' ])\\nIn this case, the weighted mean and the weighted median are about the same.\\nKey Ideas\\n•The basic metric for location is the mean, but it can be sensitive to extreme\\nvalues (outlier).\\n•Other metrics (median, trimmed mean) are less sensitive to outliers and unusual\\ndistributions and hence are more robust.\\nFurther Reading\\n•The Wikipedia article on central tendency  contains an extensive discussion of\\nvarious measures of location.\\n•John Tukey’s 1977 classic Exploratory Data Analysis  (Pearson) is still widely read.\\nEstimates of Variability\\nLocation is just one dimension in summarizing a feature.  A second dimension, varia‐\\nbility , also referred to as dispersion , measures whether the data values are tightly clus‐\\ntered or spread out.  At the heart of statistics lies variability: measuring it, reducing it,\\ndistinguishing random from real variability, identifying the various sources of real\\nvariability, and making decisions in the presence of it.\\nKey Terms for Variability Metrics\\nDeviations\\nThe difference between the observed values and the estimate of location.\\nSynonyms\\nerrors, residuals\\nVariance\\nThe sum of squared deviations from the mean divided by n – 1 where n is the\\nnumber of data values.\\nEstimates of Variability | 13\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 30}),\n",
       " Document(page_content='Synonym\\nmean-squared-error\\nStandard deviation\\nThe square root of the variance.\\nMean absolute deviation\\nThe mean of the absolute values of the deviations from the mean.\\nSynonyms\\nl1-norm, Manhattan norm\\nMedian absolute deviation from the median\\nThe median of the absolute values of the deviations from the median.\\nRange\\nThe difference between the largest and the smallest value in a data set.\\nOrder statistics\\nMetrics based on the data values sorted from smallest to biggest.\\nSynonym\\nranks\\nPercentile\\nThe value such that P percent of the values take on this value or less and (100–P)\\npercent take on this value or more.\\nSynonym\\nquantile\\nInterquartile range\\nThe difference between the 75th percentile and the 25th percentile.\\nSynonym\\nIQR\\nJust as there are different ways to measure location (mean, median, etc.), there are\\nalso different ways to measure variability.\\nStandard Deviation and Related Estimates\\nThe most widely used estimates of variation are based on the differences, or devia‐\\ntions , between the estimate of location and the observed data. For a set of data\\n{1, 4, 4}, the mean is 3 and the median is 4. The deviations from the mean are the\\ndifferences: 1 – 3 = –2, 4 – 3 = 1, 4 – 3 = 1. These deviations tell us how dispersed the\\ndata is around the central value.\\n14 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 31}),\n",
       " Document(page_content='One way to measure variability is to estimate a typical value for these deviations.\\nAveraging the deviations themselves would not tell us much—the negative deviations\\noffset the positive ones. In fact, the sum of the deviations from the mean is precisely\\nzero. Instead, a simple approach is to take the average of the absolute values of the\\ndeviations from the mean. In the preceding example, the absolute value of the devia‐\\ntions is {2 1 1}, and their average is (2 + 1 + 1) / 3 = 1.33. This is known as the mean\\nabsolute deviation  and is computed with the formula:\\nMean absolute deviation =∑i= 1nxi−x\\nn\\nwhere x is the sample mean.\\nThe best-known estimates of variability are the variance  and the standard deviation ,\\nwhich are based on squared deviations. The variance is an average of the squared\\ndeviations, and the standard deviation is the square root of the variance:\\nVariance = s2=∑i= 1nxi−x2\\nn− 1\\nStandard deviation = s=Variance\\nThe standard deviation is much easier to interpret than the variance since it is on the\\nsame scale as the original data. Still, with its more complicated and less intuitive for‐\\nmula, it might seem peculiar that the standard deviation is preferred in statistics over\\nthe mean absolute deviation. It owes its preeminence to statistical theory: mathemati‐\\ncally, working with squared values is much more convenient than absolute values,\\nespecially for statistical models.\\nDegrees of Freedom, and n or n – 1?\\nIn statistics books, there is always some discussion of why we have n – 1 in the\\ndenominator in the variance formula, instead of n, leading into the concept of degrees\\nof freedom . This distinction is not important since n is generally large enough that it\\nwon’t make much difference whether you divide by n or n – 1. But in case you are\\ninterested, here is the story. It is based on the premise that you want to make esti‐\\nmates about a population, based on a sample.\\nIf you use the intuitive denominator of n in the variance formula, you will underesti‐\\nmate the true value of the variance and the standard deviation in the population. This\\nis referred to as a biased  estimate.  However, if you divide by n – 1 instead of n, the\\nvariance becomes an unbiased  estimate.\\nEstimates of Variability | 15', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 32}),\n",
       " Document(page_content='To fully explain why using n leads to a biased estimate involves the notion of degrees\\nof freedom, which takes into account the number of constraints in computing an esti‐\\nmate. In this case, there are n – 1 degrees of freedom since there is one constraint: the\\nstandard deviation depends on calculating the sample mean. For most problems, data\\nscientists do not need to worry about degrees of freedom.\\nNeither the variance, the standard deviation, nor the mean absolute deviation is\\nrobust to outliers and extreme values (see “Median and Robust Estimates”  on page 10\\nfor a discussion of robust estimates for location). The variance and standard devia‐\\ntion are especially sensitive to outliers since they are based on the squared deviations.\\nA robust estimate of variability is the median absolute deviation from the median  or\\nMAD:\\nMedian absolute deviation = Median x1−m,x2−m, ..., xN−m\\nwhere m is the median. Like the median, the MAD is not influenced by extreme val‐\\nues. It is also possible to compute a trimmed standard deviation analogous to the\\ntrimmed mean (see “Mean” on page 9 ).\\nThe variance, the standard deviation, the mean absolute deviation,\\nand the median absolute deviation from the median are not equiv‐\\nalent estimates, even in the case where the data comes from a nor‐\\nmal distribution. In fact, the standard deviation is always greater\\nthan the mean absolute deviation, which itself is greater than the\\nmedian absolute deviation. Sometimes, the median absolute devia‐\\ntion is multiplied by a constant scaling factor to put the MAD on\\nthe same scale as the standard deviation in the case of a normal dis‐\\ntribution. The commonly used factor of 1.4826 means that 50% of\\nthe normal distribution fall within the range ±MAD  (see, e.g.,\\nhttps://oreil.ly/SfDk2 ).\\nEstimates Based on Percentiles\\nA different approach to estimating dispersion is based on looking at the spread of the\\nsorted data. Statistics based on sorted (ranked) data are referred to as order statistics .\\nThe most basic measure is the range : the difference between the largest and smallest\\nnumbers. The minimum and maximum values themselves are useful to know and are\\nhelpful in identifying outliers, but the range is extremely sensitive to outliers and not\\nvery useful as a general measure of dispersion in the data.\\nTo avoid the sensitivity to outliers, we can look at the range of the data after dropping\\nvalues from each end. Formally, these types of estimates are based on differences\\n16 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 33}),\n",
       " Document(page_content='between percentiles . In a data set, the Pth percentile is a value such that at least P per‐\\ncent of the values take on this value or less and at least (100 – P) percent of the values\\ntake on this value or more. For example, to find the 80th percentile, sort the data.\\nThen, starting with the smallest value, proceed 80 percent of the way to the largest\\nvalue. Note that the median is the same thing as the 50th percentile. The percentile is\\nessentially the same as a quantile , with quantiles indexed by fractions (so the .8 quan‐\\ntile is the same as the 80th percentile).\\nA common measurement of variability is the difference between the 25th percentile\\nand the 75th percentile, called the interquartile range  (or IQR). Here is a simple exam‐\\nple: {3,1,5,3,6,7,2,9}. We sort these to get {1,2,3,3,5,6,7,9}. The 25th percentile is at 2.5,\\nand the 75th percentile is at 6.5, so the interquartile range is 6.5 – 2.5 = 4. Software\\ncan have slightly differing approaches that yield different answers (see the following\\ntip); typically, these differences are smaller.\\nFor very large data sets, calculating exact percentiles can be computationally very\\nexpensive since it requires sorting all the data values. Machine learning and statistical\\nsoftware use special algorithms, such as [Zhang-Wang-2007] , to get an approximate\\npercentile that can be calculated very quickly and is guaranteed to have a certain\\naccuracy.\\nPercentile: Precise Definition\\nIf we have an even number of data ( n is even), then the percentile is\\nambiguous under the preceding definition. In fact, we could take\\non any value between the order statistics xj and xj+ 1 where j\\nsatisfies:\\n100 *j\\nn≤P< 100 *j+ 1\\nn\\nFormally, the percentile is the weighted average:\\nPercentile P=1 −wxj+wxj+ 1\\nfor some weight w between 0 and 1. Statistical software has slightly\\ndiffering approaches to choosing w. In fact, the R function quan\\ntile  offers nine different alternatives to compute the quantile.\\nExcept for small data sets, you don’t usually need to worry about\\nthe precise way a percentile is calculated. At the time of this writ‐\\ning, Python ’s numpy.quantile  supports only one approach, linear\\ninterpolation.\\nEstimates of Variability | 17', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 34}),\n",
       " Document(page_content=\"Example: Variability Estimates of State Population\\nTable 1-3  (repeated from Table 1-2  for convenience) shows the first few rows in the\\ndata set containing population and murder rates for each state.\\nTable 1-3. A few rows of the data.frame  state of population and murder rate by state\\nState Population Murder rate Abbreviation\\n1 Alabama 4,779,736 5.7 AL\\n2 Alaska 710,231 5.6 AK\\n3 Arizona 6,392,017 4.7 AZ\\n4 Arkansas 2,915,918 5.6 AR\\n5 California 37,253,956 4.4 CA\\n6 Colorado 5,029,196 2.8 CO\\n7 Connecticut 3,574,097 2.4 CT\\n8 Delaware 897,934 5.8 DE\\nUsing R’s built-in functions for the standard deviation, the interquartile range (IQR),\\nand the median absolute deviation from the median (MAD), we can compute esti‐\\nmates of variability for the state population data:\\n> sd(state[['Population' ]])\\n[1] 6848235\\n> IQR(state[['Population' ]])\\n[1] 4847308\\n> mad(state[['Population' ]])\\n[1] 3849870\\nThe pandas  data frame provides methods for calculating standard deviation and\\nquantiles. Using the quantiles, we can easily determine the IQR. For the robust MAD,\\nwe use the function robust.scale.mad  from the statsmodels  package:\\nstate['Population' ].std()\\nstate['Population' ].quantile (0.75) - state['Population' ].quantile (0.25)\\nrobust.scale.mad(state['Population' ])\\nThe standard deviation is almost twice as large as the MAD (in R, by default, the scale\\nof the MAD is adjusted to be on the same scale as the mean). This is not surprising\\nsince the standard deviation is sensitive to outliers.\\n18 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 35}),\n",
       " Document(page_content='Key Ideas\\n•Variance and standard deviation are the most widespread and routinely reported\\nstatistics of variability.\\n•Both are sensitive to outliers.\\n•More robust metrics include mean absolute deviation, median absolute deviation\\nfrom the median, and percentiles (quantiles).\\nFurther Reading\\n•David Lane’s online statistics resource has a section on percentiles .\\n•Kevin Davenport has a useful post on R-Bloggers  about deviations from the\\nmedian and their robust properties.\\nExploring the Data Distribution\\nEach of the estimates we’ve covered sums up the data in a single number to describe\\nthe location or variability of the data. It is also useful to explore how the data is dis‐\\ntributed overall.\\nKey Terms for Exploring the Distribution\\nBoxplot\\nA plot introduced by Tukey as a quick way to visualize the distribution of data.\\nSynonym\\nbox and whiskers plot\\nFrequency table\\nA tally of the count of numeric data values that fall into a set of intervals (bins).\\nHistogram\\nA plot of the frequency table with the bins on the x-axis and the count (or pro‐\\nportion) on the y-axis. While visually similar, bar charts should not be confused\\nwith histograms. See “Exploring Binary and Categorical Data” on page 27 for a\\ndiscussion of the difference.\\nDensity plot\\nA smoothed version of the histogram, often based on a kernel density estimate .\\nExploring the Data Distribution | 19', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 36}),\n",
       " Document(page_content=\"Percentiles and Boxplots\\nIn “Estimates Based on Percentiles” on page 16, we explored how percentiles can be\\nused to measure the spread of the data. Percentiles are also valuable for summarizing\\nthe entire distribution. It is common to report the quartiles (25th, 50th, and 75th per‐\\ncentiles) and the deciles (the 10th, 20th, …, 90th percentiles). Percentiles are espe‐\\ncially valuable for summarizing the tails (the outer range) of the distribution.  Popular\\nculture has coined the term one-percenters  to refer to the people in the top 99th per‐\\ncentile of wealth.\\nTable 1-4  displays some percentiles of the murder rate by state. In R, this would be\\nproduced by the quantile  function:\\nquantile (state[['Murder.Rate' ]], p=c(.05, .25, .5, .75, .95))\\n   5%   25%   50%   75%   95%\\n1.600 2.425 4.000 5.550 6.510\\nThe pandas  data frame method quantile  provides it in Python :\\nstate['Murder.Rate' ].quantile ([0.05, 0.25, 0.5, 0.75, 0.95])\\nTable 1-4. Percentiles of murder rate by state\\n5% 25% 50% 75% 95%\\n1.60 2.42 4.00 5.55 6.51\\nThe median is 4 murders per 100,000 people, although there is quite a bit of variabil‐\\nity: the 5th percentile is only 1.6 and the 95th percentile is 6.51.\\nBoxplots , introduced by Tukey [Tukey-1977] , are based on percentiles and give a\\nquick way to visualize the distribution of data. Figure 1-2  shows a boxplot of the pop‐\\nulation by state produced by R:\\nboxplot(state[['Population' ]]/1000000, ylab='Population (millions)' )\\npandas  provides a number of basic exploratory plots for data frame; one of them is\\nboxplots:\\nax = (state['Population' ]/1_000_000 ).plot.box()\\nax.set_ylabel ('Population (millions)' )\\n20 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 37}),\n",
       " Document(page_content='Figure 1-2. Boxplot of state populations\\nFrom this boxplot we can immediately see that the median state population is about 5\\nmillion, half the states fall between about 2 million and about 7 million, and there are\\nsome high population outliers. The top and bottom of the box are the 75th and 25th\\npercentiles, respectively. The median is shown by the horizontal line in the box. The\\ndashed lines, referred to as whiskers , extend from the top and bottom of the box to\\nindicate the range for the bulk of the data. There are many variations of a boxplot;\\nsee, for example, the documentation for the R function boxplot  [R-base-2015] . By\\ndefault, the R function extends the whiskers to the furthest point beyond the box,\\nexcept that it will not go beyond 1.5 times the IQR. Matplotlib  uses the same imple‐\\nmentation; other software may use a different rule.\\nAny data outside of the whiskers is plotted as single points or circles (often consid‐\\nered outliers).\\nExploring the Data Distribution | 21', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 38}),\n",
       " Document(page_content=\"Frequency Tables and Histograms\\nA frequency table of a variable divides up the variable range into equally spaced seg‐\\nments and tells us how many values fall within each segment.  Table 1-5  shows a fre‐\\nquency table of the population by state computed in R:\\nbreaks <- seq(from=min(state[['Population' ]]),\\n                to=max(state[['Population' ]]), length=11)\\npop_freq  <- cut(state[['Population' ]], breaks=breaks,\\n                right=TRUE, include.lowest =TRUE)\\ntable(pop_freq )\\nThe function pandas.cut  creates a series that maps the values into the segments.\\nUsing the method value_counts , we get the frequency table:\\nbinnedPopulation  = pd.cut(state['Population' ], 10)\\nbinnedPopulation .value_counts ()\\nTable 1-5. A frequency table of population by state\\nBinNumber BinRange Count States\\n1 563,626–4,232,658 24 WY,VT,ND,AK,SD,DE,MT,RI,NH,ME,HI,ID,NE,WV,NM,NV,UT,KS,AR,MS,IA,CT,OK,OR\\n2 4,232,659–\\n7,901,69114 KY,LA,SC,AL,CO,MN,WI,MD,MO,TN,AZ,IN,MA,WA\\n3 7,901,692–\\n11,570,7246 VA,NJ,NC,GA,MI,OH\\n4 11,570,725–\\n15,239,7572 PA,IL\\n5 15,239,758–\\n18,908,7901 FL\\n6 18,908,791–\\n22,577,8231 NY\\n7 22,577,824–\\n26,246,8561 TX\\n8 26,246,857–\\n29,915,8890\\n9 29,915,890–\\n33,584,9220\\n10 33,584,923–\\n37,253,9561 CA\\nThe least populous state is Wyoming, with 563,626 people, and the most populous is\\nCalifornia, with 37,253,956 people. This gives us a range of 37,253,956 – 563,626 =\\n36,690,330, which we must divide up into equal size bins—let’s say 10 bins. With 10\\nequal size bins, each bin will have a width of 3,669,033, so the first bin will span from\\n563,626 to 4,232,658. By contrast, the top bin, 33,584,923 to 37,253,956, has only one\\nstate: California. The two bins immediately below California are empty, until we\\n22 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 39}),\n",
       " Document(page_content=\"reach Texas. It is important to include the empty bins; the fact that there are no values\\nin those bins is useful information. It can also be useful to experiment with different\\nbin sizes. If they are too large, important features of the distribution can be obscured.\\nIf they are too small, the result is too granular, and the ability to see the bigger picture\\nis lost.\\nBoth frequency tables and percentiles summarize the data by creat‐\\ning bins. In general, quartiles and deciles will have the same count\\nin each bin (equal-count bins), but the bin sizes will be different.\\nThe frequency table, by contrast, will have different counts in the\\nbins (equal-size bins), and the bin sizes will be the same.\\nA histogram is a way to visualize a frequency table, with bins on the x-axis and the\\ndata count on the y-axis. In Figure 1-3 , for example, the bin centered at 10 million\\n(1e+07) runs from roughly 8 million to 12 million, and there are six states in that bin.\\nTo create a histogram corresponding to Table 1-5  in R, use the hist  function with the\\nbreaks  argument:\\nhist(state[['Population' ]], breaks=breaks)\\npandas  supports histograms for data frames with the DataFrame.plot.hist  method. \\nUse the keyword argument bins  to define the number of bins. The various plot meth‐\\nods return an axis object that allows further fine-tuning of the visualization using\\nMatplotlib :\\nax = (state['Population' ] / 1_000_000 ).plot.hist(figsize=(4, 4))\\nax.set_xlabel ('Population (millions)' )\\nThe histogram is shown in Figure 1-3 . In general, histograms are plotted such that:\\n•Empty bins are included in the graph.\\n•Bins are of equal width.\\n•The number of bins (or, equivalently, bin size) is up to the user.\\n•Bars are contiguous—no empty space shows between bars, unless there is an\\nempty bin.\\nExploring the Data Distribution | 23\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 40}),\n",
       " Document(page_content='Figure 1-3. Histogram of state populations\\nStatistical Moments\\nIn statistical theory, location and variability are referred to as the\\nfirst and second moments  of a distribution. The third and fourth\\nmoments are called skewness  and kurtosis . Skewness refers to\\nwhether the data is skewed to larger or smaller values, and kurtosis\\nindicates the propensity of the data to have extreme values. Gener‐\\nally, metrics are not used to measure skewness and kurtosis;\\ninstead, these are discovered through visual displays such as Fig‐\\nures 1-2 and 1-3.\\nDensity Plots and Estimates\\nRelated to the histogram is a density plot, which shows the distribution of data values\\nas a continuous line. A density plot can be thought of as a smoothed histogram,\\nalthough it is typically computed directly from the data through a kernel density esti‐\\nmate  (see [Duong-2001]  for a short tutorial). Figure 1-4  displays a density estimate\\nsuperposed on a histogram. In R, you can compute a density estimate using the\\ndensity  function:\\n24 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 41}),\n",
       " Document(page_content=\"hist(state[['Murder.Rate' ]], freq=FALSE)\\nlines(density(state[['Murder.Rate' ]]), lwd=3, col='blue')\\npandas  provides the density  method to create a density plot. Use the argument\\nbw_method  to control the smoothness of the density curve:\\nax = state['Murder.Rate' ].plot.hist(density=True, xlim=[0,12], bins=range(1,12))\\nstate['Murder.Rate' ].plot.density(ax=ax) \\nax.set_xlabel ('Murder Rate (per 100,000)' )\\nPlot functions often take an optional axis ( ax) argument, which will cause the\\nplot to be added to the same graph.\\nA key distinction from the histogram plotted in Figure 1-3  is the scale of the y-axis: a\\ndensity plot corresponds to plotting the histogram as a proportion rather than counts\\n(you specify this in R using the argument freq=FALSE ). Note that the total area under\\nthe density curve = 1, and instead of counts in bins you calculate areas under the\\ncurve between any two points on the x-axis, which correspond to the proportion of\\nthe distribution lying between those two points.\\nFigure 1-4. Density of state murder rates\\nExploring the Data Distribution | 25\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 42}),\n",
       " Document(page_content='Density Estimation\\nDensity estimation is a rich topic with a long history in statistical\\nliterature. In fact, over 20 R packages have been published that\\noffer functions for density estimation. [Deng-Wickham-2011]  give\\na comprehensive review of R packages, with a particular recom‐\\nmendation for ASH or KernSmooth . The density estimation methods\\nin pandas  and scikit-learn  also offer good implementations. For\\nmany data science problems, there is no need to worry about the\\nvarious types of density estimates; it suffices to use the base\\nfunctions.\\nKey Ideas\\n•A frequency histogram plots frequency counts on the y-axis and variable values\\non the x-axis; it gives a sense of the distribution of the data at a glance.\\n•A frequency table is a tabular version of the frequency counts found in a\\nhistogram.\\n•A boxplot—with the top and bottom of the box at the 75th and 25th percentiles,\\nrespectively—also gives a quick sense of the distribution of the data; it is often\\nused in side-by-side displays to compare distributions.\\n•A density plot is a smoothed version of a histogram; it requires a function to esti‐\\nmate a plot based on the data (multiple estimates are possible, of course).\\nFurther Reading\\n•A SUNY Oswego professor provides a step-by-step guide to creating a boxplot .\\n•Density estimation in R is covered in Henry Deng and Hadley Wickham’s paper\\nof the same name .\\n•R-Bloggers has a useful post on histograms in R, including customization ele‐\\nments, such as binning (breaks).\\n•R-Bloggers also has a similar post on boxplots in R.\\n•Matthew Conlen published an interactive presentation  that demonstrates the\\neffect of choosing different kernels and bandwidth on kernel density estimates.\\n26 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 43}),\n",
       " Document(page_content=\"Exploring Binary and Categorical Data\\nFor categorical data, simple proportions or percentages tell the story of the data.\\nKey Terms for Exploring Categorical Data\\nMode\\nThe most commonly occurring category or value in a data set.\\nExpected value\\nWhen the categories can be associated with a numeric value, this gives an average\\nvalue based on a category’s probability of occurrence.\\nBar charts\\nThe frequency or proportion for each category plotted as bars.\\nPie charts\\nThe frequency or proportion for each category plotted as wedges in a pie.\\nGetting a summary of a binary variable or a categorical variable with a few categories\\nis a fairly easy matter: we just figure out the proportion of 1s, or the proportions of\\nthe important categories. For example, Table 1-6  shows the percentage of delayed\\nflights by the cause of delay at Dallas/Fort Worth Airport since 2010. Delays are cate‐\\ngorized as being due to factors under carrier control, air traffic control (ATC) system\\ndelays, weather, security, or a late inbound aircraft.\\nTable 1-6. Percentage of delays by cause at Dallas/Fort Worth Airport\\nCarrier ATC Weather Security Inbound\\n23.02 30.40 4.03 0.12 42.43\\nBar charts, seen often in the popular press, are a common visual tool for displaying a\\nsingle categorical variable. Categories are listed on the x-axis, and frequencies or pro‐\\nportions on the y-axis. Figure 1-5  shows the airport delays per year by cause for\\nDallas/Fort Worth (DFW), and it is produced with the R function barplot :\\nbarplot(as.matrix (dfw) / 6, cex.axis =0.8, cex.names =0.7,\\n        xlab='Cause of delay' , ylab='Count')\\npandas  also supports bar charts for data frames:\\nax = dfw.transpose ().plot.bar(figsize=(4, 4), legend=False)\\nax.set_xlabel ('Cause of delay' )\\nax.set_ylabel ('Count')\\nExploring Binary and Categorical Data | 27\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 44}),\n",
       " Document(page_content='Figure 1-5. Bar chart of airline delays at DFW by cause\\nNote that a bar chart resembles a histogram; in a bar chart the x-axis represents dif‐\\nferent categories of a factor variable, while in a histogram the x-axis represents values\\nof a single variable on a numeric scale. In a histogram, the bars are typically shown\\ntouching each other, with gaps indicating values that did not occur in the data. In a\\nbar chart, the bars are shown separate from one another.\\nPie charts are an alternative to bar charts, although statisticians and data visualization\\nexperts generally eschew pie charts as less visually informative (see [Few-2007] ).\\n28 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 45}),\n",
       " Document(page_content='Numerical Data as Categorical Data\\nIn “Frequency Tables and Histograms” on page 22, we looked at\\nfrequency tables based on binning the data. This implicitly converts\\nthe numeric data to an ordered factor. In this sense, histograms and\\nbar charts are similar, except that the categories on the x-axis in the\\nbar chart are not ordered. Converting numeric data to categorical\\ndata is an important and widely used step in data analysis since it\\nreduces the complexity (and size) of the data. This aids in the dis‐\\ncovery of relationships between features, particularly at the initial\\nstages of an analysis.\\nMode\\nThe mode is the value—or values in case of a tie—that appears most often in the data.\\nFor example, the mode of the cause of delay at Dallas/Fort Worth airport is\\n“Inbound. ” As another example, in most parts of the United States, the mode for reli‐\\ngious preference would be Christian. The mode is a simple summary statistic for\\ncategorical data, and it is generally not used for numeric data.\\nExpected Value\\nA special type of categorical data is data in which the categories represent or can be\\nmapped to discrete values on the same scale. A marketer for a new cloud technology,\\nfor example, offers two levels of service, one priced at $300/month and another at\\n$50/month. The marketer offers free webinars to generate leads, and the firm figures\\nthat 5% of the attendees will sign up for the $300 service, 15% will sign up for the $50\\nservice, and 80% will not sign up for anything. This data can be summed up, for\\nfinancial purposes, in a single “expected value, ” which is a form of weighted mean, in\\nwhich the weights are probabilities.\\nThe expected value is calculated as follows:\\n1.Multiply each outcome by its probability of occurrence.\\n2.Sum these values.\\nIn the cloud service example, the expected value of a webinar attendee is thus $22.50\\nper month, calculated as follows:\\nEV =0 . 05 300 +0 . 15 50+0 . 80 0= 22 . 5\\nThe expected value is really a form of weighted mean: it adds the ideas of future\\nexpectations and probability weights, often based on subjective judgment. Expected\\nvalue is a fundamental concept in business valuation and capital budgeting—for\\nExploring Binary and Categorical Data | 29', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 46}),\n",
       " Document(page_content='example, the expected value of five years of profits from a new acquisition, or the\\nexpected cost savings from new patient management software at a clinic.\\nProbability\\nWe referred above to the probability  of a value occurring. Most people have an intu‐\\nitive understanding of probability, encountering the concept frequently in weather\\nforecasts (the chance of rain) or sports analysis (the probability of winning). Sports\\nand games are more often expressed as odds, which are readily convertible to proba‐\\nbilities (if the odds that a team will win are 2 to 1, its probability of winning is 2/(2+1)\\n= 2/3). Surprisingly, though, the concept of probability can be the source of deep\\nphilosophical discussion when it comes to defining it. Fortunately, we do not need a\\nformal mathematical or philosophical definition here. For our purposes, the probabil‐\\nity that an event will happen is the proportion of times it will occur if the situation\\ncould be repeated over and over, countless times. Most often this is an imaginary con‐\\nstruction, but it is an adequate operational understanding of probability.\\nKey Ideas\\n•Categorical data is typically summed up in proportions and can be visualized in a\\nbar chart.\\n•Categories might represent distinct things (apples and oranges, male and female),\\nlevels of a factor variable (low, medium, and high), or numeric data that has been\\nbinned.\\n•Expected value is the sum of values times their probability of occurrence, often\\nused to sum up factor variable levels.\\nFurther Reading\\nNo statistics course is complete without a lesson on misleading graphs , which often\\ninvolves bar charts and pie charts.\\nCorrelation\\nExploratory data analysis in many modeling projects (whether in data science or in\\nresearch) involves examining correlation among predictors, and between predictors\\nand a target variable. Variables X and Y (each with measured data) are said to be posi‐\\ntively correlated if high values of X go with high values of Y , and low values of X go\\nwith low values of Y . If high values of X go with low values of Y , and vice versa, the\\nvariables are negatively correlated.\\n30 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 47}),\n",
       " Document(page_content='Key Terms for Correlation\\nCorrelation coefficient\\nA metric that measures the extent to which numeric variables are associated with\\none another (ranges from –1 to +1).\\nCorrelation matrix\\nA table where the variables are shown on both rows and columns, and the cell\\nvalues are the correlations between the variables.\\nScatterplot\\nA plot in which the x-axis is the value of one variable, and the y-axis the value of\\nanother.\\nConsider these two variables, perfectly correlated in the sense that each goes from low\\nto high:\\nv1: {1, 2, 3}\\nv2: {4, 5, 6}\\nThe vector sum of products is 1 · 4 + 2 · 5 + 3 · 6 = 32 . Now try shuffling one of them\\nand recalculating—the vector sum of products will never be higher than 32. So this\\nsum of products could be used as a metric; that is, the observed sum of 32 could be\\ncompared to lots of random shufflings (in fact, this idea relates to a resampling-based\\nestimate; see “Permutation Test” on page 97 ). Values produced by this metric, though,\\nare not that meaningful, except by reference to the resampling distribution.\\nMore useful is a standardized variant: the correlation coefficient , which gives an esti‐\\nmate of the correlation between two variables that always lies on the same scale. To\\ncompute Pearson’s correlation coefficient , we multiply deviations from the mean  for\\nvariable 1 times those for variable 2, and divide by the product of the standard\\ndeviations:\\nr=∑i= 1nxi−xyi−y\\nn− 1sxsy\\nNote that we divide by n – 1 instead of n; see “Degrees of Freedom, and n or n – 1?”\\non page 15 for more details. The correlation coefficient always lies between +1\\n(perfect  positive correlation) and –1 (perfect negative correlation); 0 indicates no\\ncorrelation.\\nVariables can have an association that is not linear, in which case the correlation coef‐\\nficient may not be a useful metric. The relationship between tax rates and revenue\\nCorrelation | 31', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 48}),\n",
       " Document(page_content=\"raised is an example: as tax rates increase from zero, the revenue raised also increases.\\nHowever, once tax rates reach a high level and approach 100%, tax avoidance increa‐\\nses and tax revenue actually declines.\\nTable 1-7 , called a correlation matrix , shows the correlation between the daily returns\\nfor telecommunication stocks from July 2012 through June 2015.  From the table, you\\ncan see that Verizon (VZ) and ATT (T) have the highest correlation. Level 3 (LVLT),\\nwhich is an infrastructure company, has the lowest correlation with the others. Note\\nthe diagonal of 1s (the correlation of a stock with itself is 1) and the redundancy of\\nthe information above and below the diagonal.\\nTable 1-7. Correlation between telecommunication stock returns\\nT CTL FTR VZ LVLT\\nT 1.000 0.475 0.328 0.678 0.279\\nCTL 0.475 1.000 0.420 0.417 0.287\\nFTR 0.328 0.420 1.000 0.287 0.260\\nVZ 0.678 0.417 0.287 1.000 0.242\\nLVLT 0.279 0.287 0.260 0.242 1.000\\nA table of correlations like Table 1-7  is commonly plotted to visually display the rela‐\\ntionship between multiple variables. Figure 1-6  shows the correlation between the\\ndaily returns for major exchange-traded funds (ETFs). In R, we can easily create this\\nusing the package corrplot :\\netfs <- sp500_px [row.names (sp500_px ) > '2012-07-01' ,\\n                 sp500_sym [sp500_sym $sector == 'etf', 'symbol' ]]\\nlibrary(corrplot )\\ncorrplot (cor(etfs), method='ellipse' )\\nIt is possible to create the same graph in Python , but there is no implementation in\\nthe common packages. However, most support the visualization of correlation matri‐\\nces using heatmaps. The following code demonstrates this using the seaborn.heat\\nmap package. In the accompanying source code repository, we include Python  code to\\ngenerate the more comprehensive visualization:\\netfs = sp500_px .loc[sp500_px .index > '2012-07-01' ,\\n                    sp500_sym [sp500_sym ['sector' ] == 'etf']['symbol' ]]\\nsns.heatmap(etfs.corr(), vmin=-1, vmax=1,\\n            cmap=sns.diverging_palette (20, 220, as_cmap=True))\\nThe ETFs for the S&P 500 (SPY) and the Dow Jones Index (DIA) have a high correla‐\\ntion. Similarly, the QQQ and the XLK, composed mostly of technology companies,\\nare positively correlated. Defensive ETFs, such as those tracking gold prices (GLD),\\noil prices (USO), or market volatility (VXX), tend to be weakly or negatively correla‐\\nted with the other ETFs. The orientation of the ellipse indicates whether two variables\\n32 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 49}),\n",
       " Document(page_content='are positively correlated (ellipse is pointed to the top right) or negatively correlated\\n(ellipse is pointed to the top left). The shading and width of the ellipse indicate the\\nstrength of the association: thinner and darker ellipses correspond to stronger\\nrelationships.\\nFigure 1-6. Correlation between ETF returns\\nLike the mean and standard deviation, the correlation coefficient is sensitive to outli‐\\ners in the data. Software packages offer robust alternatives to the classical correlation\\ncoefficient. For example, the R package robust  uses the function covRob  to compute a\\nrobust estimate of correlation. The methods in the scikit-learn  module\\nsklearn.covariance  implement a variety of approaches.\\nCorrelation | 33', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 50}),\n",
       " Document(page_content=\"Other Correlation Estimates\\nStatisticians long ago proposed other types of correlation coeffi‐\\ncients, such as Spearman’s rho  or Kendall’s tau . These are correla‐\\ntion coefficients based on the rank of the data. Since they work\\nwith ranks rather than values, these estimates are robust to outliers\\nand can handle certain types of nonlinearities. However, data scien‐\\ntists can generally stick to Pearson’s correlation coefficient, and its\\nrobust alternatives, for exploratory analysis. The appeal of rank-\\nbased estimates is mostly for smaller data sets and specific hypothe‐\\nsis tests.\\nScatterplots\\nThe standard way to visualize the relationship between two measured data variables is\\nwith a scatterplot. The x-axis represents one variable and the y-axis another, and each\\npoint on the graph is a record. See Figure 1-7  for a plot of the correlation between the\\ndaily returns for ATT and Verizon. This is produced in R with the command:\\nplot(telecom$T, telecom$VZ, xlab='ATT (T)' , ylab='Verizon (VZ)' )\\nThe same graph can be generated in Python  using the pandas  scatter method:\\nax = telecom.plot.scatter(x='T', y='VZ', figsize=(4, 4), marker='$\\\\u25EF$')\\nax.set_xlabel ('ATT (T)' )\\nax.set_ylabel ('Verizon (VZ)' )\\nax.axhline(0, color='grey', lw=1)\\nax.axvline(0, color='grey', lw=1)\\nThe returns have a positive relationship: while they cluster around zero, on most\\ndays, the stocks go up or go down in tandem (upper-right and lower-left quadrants).\\nThere are fewer days where one stock goes down significantly while the other stock\\ngoes up, or vice versa (lower-right and upper-left quadrants).\\nWhile the plot Figure 1-7  displays only 754 data points, it’s already obvious how diffi‐\\ncult it is to identify details in the middle of the plot. We will see later how adding\\ntransparency to the points, or using hexagonal binning and density plots, can help to\\nfind additional structure in the data.\\n34 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 51}),\n",
       " Document(page_content='Figure 1-7. Scatterplot of correlation between returns for ATT and Verizon\\nKey Ideas\\n•The correlation coefficient measures the extent to which two paired variables\\n(e.g., height and weight for individuals) are associated with one another.\\n•When high values of v1 go with high values of v2, v1 and v2 are positively\\nassociated.\\n•When high values of v1 go with low values of v2, v1 and v2 are negatively\\nassociated.\\n•The correlation coefficient is a standardized metric, so that it always ranges from\\n–1 (perfect negative correlation) to +1 (perfect positive correlation).\\n•A correlation coefficient of zero indicates no correlation, but be aware that ran‐\\ndom arrangements of data will produce both positive and negative values for the\\ncorrelation coefficient just by chance.\\nCorrelation | 35', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 52}),\n",
       " Document(page_content='Further Reading\\nStatistics , 4th ed., by David Freedman, Robert Pisani, and Roger Purves (W . W . Nor‐\\nton, 2007) has an excellent discussion of correlation.\\nExploring Two or More Variables\\nFamiliar estimators like mean and variance look at variables one at a time ( univariate\\nanalysis ). Correlation analysis (see “Correlation”  on page 30) is an important method\\nthat compares two variables ( bivariate analysis ). In this section we look at additional\\nestimates and plots, and at more than two variables ( multivariate analysis ).\\nKey Terms for Exploring Two or More Variables\\nContingency table\\nA tally of counts between two or more categorical variables.\\nHexagonal binning\\nA plot of two numeric variables with the records binned into hexagons.\\nContour plot\\nA plot showing the density of two numeric variables like a topographical map.\\nViolin plot\\nSimilar to a boxplot but showing the density estimate.\\nLike univariate analysis, bivariate analysis involves both computing summary statis‐\\ntics and producing visual displays. The appropriate type of bivariate or multivariate\\nanalysis depends on the nature of the data: numeric versus categorical.\\nHexagonal Binning and Contours\\n(Plotting Numeric Versus Numeric Data)\\nScatterplots are fine when there is a relatively small number of data values. The plot\\nof stock returns in Figure 1-7  involves only about 750 points. For data sets with hun‐\\ndreds of thousands or millions of records, a scatterplot will be too dense, so we need a\\ndifferent way to visualize the relationship. To illustrate, consider the data set kc_tax ,\\nwhich contains the tax-assessed values for residential properties in King County,\\nWashington. In order to focus on the main part of the data, we strip out very expen‐\\nsive and very small or large residences using the subset  function:\\n36 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 53}),\n",
       " Document(page_content=\"kc_tax0 <- subset(kc_tax, TaxAssessedValue  < 750000 &\\n                  SqFtTotLiving  > 100 &\\n                  SqFtTotLiving  < 3500)\\nnrow(kc_tax0)\\n432693\\nIn pandas , we filter the data set as follows:\\nkc_tax0 = kc_tax.loc[(kc_tax.TaxAssessedValue  < 750000) &\\n                     (kc_tax.SqFtTotLiving  > 100) &\\n                     (kc_tax.SqFtTotLiving  < 3500), :]\\nkc_tax0.shape\\n(432693, 3)\\nFigure 1-8  is a hexagonal binning  plot of the relationship between the finished square\\nfeet and the tax-assessed value for homes in King County. Rather than plotting\\npoints, which would appear as a monolithic dark cloud, we grouped the records into\\nhexagonal bins and plotted the hexagons with a color indicating the number of\\nrecords in that bin. In this chart, the positive relationship between square feet and\\ntax-assessed value is clear. An interesting feature is the hint of additional bands above\\nthe main (darkest) band at the bottom, indicating homes that have the same square\\nfootage as those in the main band but a higher tax-assessed value.\\nFigure 1-8  was generated by the powerful R package ggplot2 , developed by Hadley\\nWickham [ggplot2] . ggplot2  is one of several new software libraries for advanced\\nexploratory visual analysis of data; see “Visualizing Multiple Variables” on page 43 :\\nggplot(kc_tax0, (aes(x=SqFtTotLiving , y=TaxAssessedValue ))) +\\n  stat_binhex (color='white') +\\n  theme_bw () +\\n  scale_fill_gradient (low='white', high='black') +\\n  labs(x='Finished Square Feet' , y='Tax-Assessed Value' )\\nIn Python , hexagonal binning plots are readily available using the pandas  data frame\\nmethod hexbin :\\nax = kc_tax0.plot.hexbin(x='SqFtTotLiving' , y='TaxAssessedValue' ,\\n                         gridsize =30, sharex=False, figsize=(5, 4))\\nax.set_xlabel ('Finished Square Feet' )\\nax.set_ylabel ('Tax-Assessed Value' )\\nExploring Two or More Variables | 37\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 54}),\n",
       " Document(page_content=\"Figure 1-8. Hexagonal binning for tax-assessed value versus finished  square feet\\nFigure 1-9  uses contours overlaid onto a scatterplot to visualize the relationship\\nbetween two numeric variables. The contours are essentially a topographical map to\\ntwo variables; each contour band represents a specific density of points, increasing as\\none nears a “peak. ” This plot shows a similar story as Figure 1-8 : there is a secondary\\npeak “north” of the main peak. This chart was also created using ggplot2  with the\\nbuilt-in geom_density2d  function:\\nggplot(kc_tax0, aes(SqFtTotLiving , TaxAssessedValue )) +\\n  theme_bw () +\\n  geom_point (alpha=0.1) +\\n  geom_density2d (color='white') +\\n  labs(x='Finished Square Feet' , y='Tax-Assessed Value' )\\nThe seaborn  kdeplot  function in Python  creates a contour plot:\\nax = sns.kdeplot(kc_tax0.SqFtTotLiving , kc_tax0.TaxAssessedValue , ax=ax)\\nax.set_xlabel ('Finished Square Feet' )\\nax.set_ylabel ('Tax-Assessed Value' )\\n38 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 55}),\n",
       " Document(page_content='Figure 1-9. Contour plot for tax-assessed value versus finished  square feet\\nOther types of charts are used to show the relationship between two numeric vari‐\\nables, including heat maps . Heat maps, hexagonal binning, and contour plots all give\\na visual representation of a two-dimensional density. In this way, they are natural\\nanalogs to histograms and density plots.\\nTwo Categorical Variables\\nA useful way to summarize two categorical variables is a contingency table—a table of\\ncounts by category. Table 1-8  shows the contingency table between the grade of a per‐\\nsonal loan and the outcome of that loan. This is taken from data provided by Lending\\nClub, a leader in the peer-to-peer lending business. The grade goes from A (high) to\\nG (low). The outcome is either fully paid, current, late, or charged off (the balance of\\nthe loan is not expected to be collected). This table shows the count and row percen‐\\ntages. High-grade loans have a very low late/charge-off percentage as compared with\\nlower-grade loans.\\nExploring Two or More Variables | 39', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 56}),\n",
       " Document(page_content=\"Table 1-8. Contingency table of loan grade and status\\nGrade Charged off Current Fully paid Late Total\\nA 1562 50051 20408 469 72490\\n0.022 0.690 0.282 0.006 0.161\\nB 5302 93852 31160 2056 132370\\n0.040 0.709 0.235 0.016 0.294\\nC 6023 88928 23147 2777 120875\\n0.050 0.736 0.191 0.023 0.268\\nD 5007 53281 13681 2308 74277\\n0.067 0.717 0.184 0.031 0.165\\nE 2842 24639 5949 1374 34804\\n0.082 0.708 0.171 0.039 0.077\\nF 1526 8444 2328 606 12904\\n0.118 0.654 0.180 0.047 0.029\\nG 409 1990 643 199 3241\\n0.126 0.614 0.198 0.061 0.007\\nTotal 22671 321185 97316 9789 450961\\nContingency tables can look only at counts, or they can also include column and total\\npercentages. Pivot tables in Excel are perhaps the most common tool used to create\\ncontingency tables. In R, the CrossTable  function in the descr  package produces\\ncontingency tables, and the following code was used to create Table 1-8 :\\nlibrary(descr)\\nx_tab <- CrossTable (lc_loans $grade, lc_loans $status,\\n                    prop.c=FALSE, prop.chisq =FALSE, prop.t=FALSE)\\nThe pivot_table  method creates the pivot table in Python . The aggfunc  argument\\nallows us to get the counts. Calculating the percentages is a bit more involved:\\ncrosstab  = lc_loans .pivot_table (index='grade', columns='status' ,\\n                                aggfunc=lambda x: len(x), margins=True) \\ndf = crosstab .loc['A':'G',:].copy() \\ndf.loc[:,'Charged Off' :'Late'] = df.loc[:,'Charged Off' :'Late'].div(df['All'],\\n                                                                    axis=0) \\ndf['All'] = df['All'] / sum(df['All']) \\nperc_crosstab  = df\\nThe margins  keyword argument will add the column and row sums.\\nWe create a copy of the pivot table, ignoring the column sums.\\nWe divide the rows with the row sum.\\n40 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 57}),\n",
       " Document(page_content=\"We divide the 'All'  column by its sum.\\nCategorical and Numeric Data\\nBoxplots (see “Percentiles and Boxplots”  on page 20) are a simple way to visually\\ncompare the distributions of a numeric variable grouped according to a categorical\\nvariable.  For example, we might want to compare how the percentage of flight delays\\nvaries across airlines. Figure 1-10  shows the percentage of flights in a month that\\nwere delayed where the delay was within the carrier’s control:\\nboxplot(pct_carrier_delay  ~ airline, data=airline_stats , ylim=c(0, 50))\\nThe pandas  boxplot  method takes the by argument that splits the data set into groups\\nand creates the individual boxplots:\\nax = airline_stats .boxplot(by='airline' , column='pct_carrier_delay' )\\nax.set_xlabel ('')\\nax.set_ylabel ('Daily % of Delayed Flights' )\\nplt.suptitle ('')\\nFigure 1-10. Boxplot of percent of airline delays by carrier\\nExploring Two or More Variables | 41\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 58}),\n",
       " Document(page_content=\"Alaska stands out as having the fewest delays, while American has the most delays:\\nthe lower quartile for American is higher than the upper quartile for Alaska.\\nA violin plot , introduced by [Hintze-Nelson-1998] , is an enhancement to the boxplot\\nand plots the density estimate with the density on the y-axis.  The density is mirrored\\nand flipped over, and the resulting shape is filled in, creating an image resembling a\\nviolin. The advantage of a violin plot is that it can show nuances in the distribution\\nthat aren’t perceptible in a boxplot. On the other hand, the boxplot more clearly\\nshows the outliers in the data. In ggplot2 , the function geom_violin  can be used to\\ncreate a violin plot as follows:\\nggplot(data=airline_stats , aes(airline, pct_carrier_delay )) +\\n  ylim(0, 50) +\\n  geom_violin () +\\n  labs(x='', y='Daily % of Delayed Flights' )\\nViolin plots are available with the violinplot  method of the seaborn  package:\\nax = sns.violinplot (airline_stats .airline, airline_stats .pct_carrier_delay ,\\n                    inner='quartile' , color='white')\\nax.set_xlabel ('')\\nax.set_ylabel ('Daily % of Delayed Flights' )\\nThe corresponding plot is shown in Figure 1-11 . The violin plot shows a concentra‐\\ntion in the distribution near zero for Alaska and, to a lesser extent, Delta. This phe‐\\nnomenon is not as obvious in the boxplot. Y ou can combine a violin plot with a\\nboxplot by adding geom_boxplot  to the plot (although this works best when colors\\nare used).\\n42 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 59}),\n",
       " Document(page_content='Figure 1-11. Violin plot of percent of airline delays by carrier\\nVisualizing Multiple Variables\\nThe types of charts used to compare two variables—scatterplots, hexagonal binning,\\nand boxplots—are readily extended to more variables through the notion of condi‐\\ntioning . As an example, look back at Figure 1-8 , which showed the relationship\\nbetween homes’ finished square feet and their tax-assessed values. We observed that\\nthere appears to be a cluster of homes that have higher tax-assessed value per square\\nfoot. Diving deeper, Figure 1-12  accounts for the effect of location by plotting the\\ndata for a set of zip codes. Now the picture is much clearer: tax-assessed value is\\nmuch higher in some zip codes (98105, 98126) than in others (98108, 98188). This\\ndisparity gives rise to the clusters observed in Figure 1-8 .\\nExploring Two or More Variables | 43', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 60}),\n",
       " Document(page_content=\"We created Figure 1-12  using ggplot2  and the idea of facets , or a conditioning vari‐\\nable (in this case, zip code):\\nggplot(subset(kc_tax0, ZipCode %in% c(98188, 98105, 98108, 98126)),\\n         aes(x=SqFtTotLiving , y=TaxAssessedValue )) +\\n  stat_binhex (color='white') +\\n  theme_bw () +\\n  scale_fill_gradient (low='white', high='blue') +\\n  labs(x='Finished Square Feet' , y='Tax-Assessed Value' ) +\\n  facet_wrap ('ZipCode' ) \\nUse the ggplot  functions facet_wrap  and facet_grid  to specify the condition‐\\ning variable.\\nFigure 1-12. Tax-assessed value versus finished  square feet by zip code\\nMost Python  packages base their visualizations on Matplotlib . While it is in princi‐\\nple possible to create faceted graphs using Matplotlib , the code can get complicated.\\nFortunately, seaborn  has a relatively straightforward way of creating these graphs:\\n44 | Chapter 1: Exploratory Data Analysis\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 61}),\n",
       " Document(page_content=\"zip_codes  = [98188, 98105, 98108, 98126]\\nkc_tax_zip  = kc_tax0.loc[kc_tax0.ZipCode.isin(zip_codes ),:]\\nkc_tax_zip\\ndef hexbin(x, y, color, **kwargs):\\n    cmap = sns.light_palette (color, as_cmap=True)\\n    plt.hexbin(x, y, gridsize =25, cmap=cmap, **kwargs)\\ng = sns.FacetGrid (kc_tax_zip , col='ZipCode' , col_wrap =2) \\ng.map(hexbin, 'SqFtTotLiving' , 'TaxAssessedValue' ,\\n      extent=[0, 3500, 0, 700000]) \\ng.set_axis_labels ('Finished Square Feet' , 'Tax-Assessed Value' )\\ng.set_titles ('Zip code {col_name:.0f}' )\\nUse the arguments col and row to specify the conditioning variables. For a single\\nconditioning variable, use col together with col_wrap  to wrap the faceted graphs\\ninto multiple rows.\\nThe map method calls the hexbin  function with subsets of the original data set for\\nthe different zip codes. extent  defines the limits of the x- and y-axes.\\nThe concept of conditioning variables in a graphics system was pioneered with Trellis\\ngraphics , developed by Rick Becker, Bill Cleveland, and others at Bell Labs [Trellis-\\nGraphics] . This idea has propagated to various modern graphics systems, such as the\\nlattice  [lattice]  and ggplot2  packages in R and the seaborn  [seaborn]  and Bokeh\\n[bokeh]  modules in Python . Conditioning variables are also integral to business intel‐\\nligence platforms such as Tableau and Spotfire. With the advent of vast computing\\npower, modern visualization platforms have moved well beyond the humble begin‐\\nnings of exploratory data analysis. However, key concepts and tools developed a half\\ncentury ago (e.g., simple boxplots) still form a foundation for these systems.\\nKey Ideas\\n•Hexagonal binning and contour plots are useful tools that permit graphical\\nexamination of two numeric variables at a time, without being overwhelmed by\\nhuge amounts of data.\\n•Contingency tables are the standard tool for looking at the counts of two catego‐\\nrical variables.\\n•Boxplots and violin plots allow you to plot a numeric variable against a categori‐\\ncal variable.\\nExploring Two or More Variables | 45\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 62}),\n",
       " Document(page_content='Further Reading\\n•Modern Data Science with R  by Benjamin Baumer, Daniel Kaplan, and Nicholas\\nHorton (Chapman & Hall/CRC Press, 2017) has an excellent presentation of “a\\ngrammar for graphics” (the “gg” in ggplot ).\\n•ggplot2: Elegant Graphics for Data Analysis  by Hadley Wickham (Springer, 2009)\\nis an excellent resource from the creator of ggplot2 .\\n•Josef Fruehwald has a web-based tutorial on ggplot2 .\\nSummary\\nExploratory data analysis (EDA), pioneered by John Tukey, set a foundation for the\\nfield of data science. The key idea of EDA is that the first and most important step in\\nany project based on data is to look at the data . By summarizing and visualizing the\\ndata, you can gain valuable intuition and understanding of the project.\\nThis chapter has reviewed concepts ranging from simple metrics, such as estimates of\\nlocation and variability, to rich visual displays that explore the relationships between\\nmultiple variables, as in Figure 1-12 . The diverse set of tools and techniques being\\ndeveloped by the open source community, combined with the expressiveness of the R\\nand Python  languages, has created a plethora of ways to explore and analyze data.\\nExploratory analysis should be a cornerstone of any data science project.\\n46 | Chapter 1: Exploratory Data Analysis', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 63}),\n",
       " Document(page_content='CHAPTER 2\\nData and Sampling Distributions\\nA popular misconception holds that the era of big data means the end of a need for\\nsampling. In fact, the proliferation of data of varying quality and relevance reinforces\\nthe need for sampling as a tool to work efficiently with a variety of data and to mini‐\\nmize bias. Even in a big data project, predictive models are typically developed and\\npiloted with samples. Samples are also used in tests of various sorts (e.g., comparing\\nthe effect of web page designs on clicks).\\nFigure 2-1  shows a schematic that underpins the concepts we will discuss in this\\nchapter—data and sampling distributions. The lefthand side represents a population\\nthat, in statistics, is assumed to follow an underlying but unknown  distribution. All\\nthat is available is the sample  data and its empirical distribution, shown on the right‐\\nhand side. To get from the lefthand side to the righthand side, a sampling  procedure is\\nused (represented by an arrow). Traditional statistics focused very much on the left‐\\nhand side, using theory based on strong assumptions about the population. Modern\\nstatistics has moved to the righthand side, where such assumptions are not needed.\\nIn general, data scientists need not worry about the theoretical nature of the lefthand\\nside and instead should focus on the sampling procedures and the data at hand.\\nThere are some notable exceptions. Sometimes data is generated from a physical pro‐\\ncess that can be modeled. The simplest example is flipping a coin: this follows a bino‐\\nmial distribution. Any real-life binomial situation (buy or don’t buy, fraud or no\\nfraud, click or don’t click) can be modeled effectively by a coin (with modified proba‐\\nbility of landing heads, of course). In these cases, we can gain additional insight by\\nusing our understanding of the population.\\n47', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 64}),\n",
       " Document(page_content='Figure 2-1. Population versus sample\\nRandom Sampling and Sample Bias\\nA sample  is a subset of data from a larger data set; statisticians call this larger data set\\nthe population . A population in statistics is not the same thing as in biology—it is a\\nlarge, defined (but sometimes theoretical or imaginary) set of data.\\nRandom sampling  is a process in which each available member of the population\\nbeing sampled has an equal chance of being chosen for the sample at each draw. The\\nsample that results is called a simple random sample . Sampling can be done with\\nreplacement , in which observations are put back in the population after each draw for\\npossible future reselection. Or it can be done without replacement , in which case\\nobservations, once selected, are unavailable for future draws.\\nData quality often matters more than data quantity when making an estimate or a\\nmodel based on a sample. Data quality in data science involves completeness, consis‐\\ntency of format, cleanliness, and accuracy of individual data points. Statistics adds the\\nnotion of representativeness .\\n48 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 65}),\n",
       " Document(page_content='Key Terms for Random Sampling\\nSample\\nA subset from a larger data set.\\nPopulation\\nThe larger data set or idea of a data set.\\nN (n)\\nThe size of the population (sample).\\nRandom sampling\\nDrawing elements into a sample at random.\\nStratified  sampling\\nDividing the population into strata and randomly sampling from each strata.\\nStratum (pl., strata)\\nA homogeneous subgroup of a population with common characteristics.\\nSimple random sample\\nThe sample that results from random sampling without stratifying the\\npopulation.\\nBias\\nSystematic error.\\nSample bias\\nA sample that misrepresents the population.\\nThe classic example is the Literary Digest  poll of 1936 that predicted a victory of Alf\\nLandon over Franklin Roosevelt. The Literary Digest , a leading periodical of the day,\\npolled its entire subscriber base plus additional lists of individuals, a total of over 10\\nmillion people, and predicted a landslide victory for Landon.  George Gallup, founder\\nof the Gallup Poll, conducted biweekly polls of just 2,000 people and accurately pre‐\\ndicted a Roosevelt victory. The difference lay in the selection of those polled.\\nThe Literary Digest  opted for quantity, paying little attention to the method of selec‐\\ntion. They ended up polling those with relatively high socioeconomic status (their\\nown subscribers, plus those who, by virtue of owning luxuries like telephones and\\nautomobiles, appeared in marketers’ lists). The result was sample bias ; that is, the\\nsample was different in some meaningful and nonrandom way from the larger popu‐\\nlation it was meant to represent. The term nonrandom  is important—hardly any sam‐\\nple, including random samples, will be exactly representative of the population.\\nSample bias occurs when the difference is meaningful, and it can be expected to con‐\\ntinue for other samples drawn in the same way as the first.\\nRandom Sampling and Sample Bias | 49', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 66}),\n",
       " Document(page_content='Self-Selection Sampling Bias\\nThe reviews of restaurants, hotels, cafés, and so on that you read on\\nsocial media sites like Y elp are prone to bias because the people\\nsubmitting them are not randomly selected; rather, they themselves\\nhave taken the initiative to write. This leads to self-selection bias—\\nthe people motivated to write reviews may have had poor experien‐\\nces, may have an association with the establishment, or may simply\\nbe a different type of person from those who do not write reviews.\\nNote that while self-selection samples can be unreliable indicators\\nof the true state of affairs, they may be more reliable in simply com‐\\nparing one establishment to a similar one; the same self-selection\\nbias might apply to each.\\nBias\\nStatistical bias refers to measurement or sampling errors that are systematic and pro‐\\nduced by the measurement or sampling process. An important distinction should be\\nmade between errors due to random chance and errors due to bias. Consider the\\nphysical process of a gun shooting at a target. It will not hit the absolute center of the\\ntarget every time, or even much at all. An unbiased process will produce error, but it\\nis random and does not tend strongly in any direction (see Figure 2-2 ). The results\\nshown in Figure 2-3  show a biased process—there is still random error in both the x\\nand y direction, but there is also a bias. Shots tend to fall in the upper-right quadrant.\\nFigure 2-2. Scatterplot of shots from a gun with true aim\\n50 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 67}),\n",
       " Document(page_content='Figure 2-3. Scatterplot of shots from a gun with biased aim\\nBias comes in different forms, and may be observable or invisible. When a result does\\nsuggest bias (e.g., by reference to a benchmark or actual values), it is often an indica‐\\ntor that a statistical or machine learning model has been misspecified, or an impor‐\\ntant variable left out.\\nRandom Selection\\nTo avoid the problem of sample bias that led the Literary Digest  to predict Landon\\nover Roosevelt, George Gallup (shown in Figure 2-4 ) opted for more scientifically\\nchosen methods to achieve a sample that was representative of the US voting elector‐\\nate. There are now a variety of methods to achieve representativeness, but at the heart\\nof all of them lies random sampling .\\nFigure 2-4. George Gallup, catapulted to fame by the Literary Digest’s “big data” failure\\nRandom sampling is not always easy. Proper definition of an accessible population is\\nkey. Suppose we want to generate a representative profile of customers and we need\\nto conduct a pilot customer survey. The survey needs to be representative but is labor\\nintensive.\\nRandom Sampling and Sample Bias | 51', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 68}),\n",
       " Document(page_content='First, we need to define who a customer is. We might select all customer records\\nwhere purchase amount > 0. Do we include all past customers? Do we include\\nrefunds? Internal test purchases? Resellers? Both billing agent and customer?\\nNext, we need to specify a sampling procedure. It might be “select 100 customers at\\nrandom. ” Where a sampling from a flow is involved (e.g., real-time customer transac‐\\ntions or web visitors), timing considerations may be important (e.g., a web visitor at\\n10 a.m. on a weekday may be different from a web visitor at 10 p.m. on a weekend).\\nIn stratified  sampling , the population is divided up into strata , and random samples\\nare taken from each stratum. Political pollsters might seek to learn the electoral pref‐\\nerences of whites, blacks, and Hispanics. A simple random sample taken from the\\npopulation would yield too few blacks and Hispanics, so those strata could be over‐\\nweighted in stratified sampling to yield equivalent sample sizes.\\nSize Versus Quality: When Does Size Matter?\\nIn the era of big data, it is sometimes surprising that smaller is better.  Time and effort\\nspent on random sampling not only reduces bias but also allows greater attention to\\ndata exploration and data quality.  For example, missing data and outliers may contain\\nuseful information. It might be prohibitively expensive to track down missing values\\nor evaluate outliers in millions of records, but doing so in a sample of several thou‐\\nsand records may be feasible. Data plotting and manual inspection bog down if there\\nis too much data.\\nSo when are massive amounts of data needed?\\nThe classic scenario for the value of big data is when the data is not only big but\\nsparse as well. Consider the search queries received by Google, where columns are\\nterms, rows are individual search queries, and cell values are either 0 or 1, depending\\non whether a query contains a term. The goal is to determine the best predicted\\nsearch destination for a given query. There are over 150,000 words in the English lan‐\\nguage, and Google processes over one trillion queries per year. This yields a huge\\nmatrix, the vast majority of whose entries are “0. ”\\nThis is a true big data problem—only when such enormous quantities of data are\\naccumulated can effective search results be returned for most queries. And the more\\ndata accumulates, the better the results. For popular search terms this is not such a\\nproblem—effective data can be found fairly quickly for the handful of extremely pop‐\\nular topics trending at a particular time. The real value of modern search technology\\nlies in the ability to return detailed and useful results for a huge variety of search\\nqueries, including those that occur with a frequency, say, of only one in a million.\\nConsider the search phrase “Ricky Ricardo and Little Red Riding Hood. ” In the early\\ndays of the internet, this query would probably have returned results on the band‐\\nleader Ricky Ricardo, the television show I Love Lucy  in which that character\\n52 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 69}),\n",
       " Document(page_content='appeared, and the children’s story Little Red Riding Hood . Both of those individual\\nitems would have had many searches to refer to, but the combination would have had\\nvery few. Later, now that trillions of search queries have been accumulated, this search\\nquery returns the exact I Love Lucy  episode in which Ricky narrates, in dramatic fash‐\\nion, the Little Red Riding Hood  story to his infant son in a comic mix of English and\\nSpanish.\\nKeep in mind that the number of actual pertinent  records—ones in which this exact\\nsearch query, or something very similar, appears (together with information on what\\nlink people ultimately clicked on)—might need only be in the thousands to be effec‐\\ntive. However, many trillions of data points are needed to obtain these pertinent\\nrecords (and random sampling, of course, will not help). See also “Long-Tailed Dis‐\\ntributions” on page 73 .\\nSample Mean Versus Population Mean\\nThe symbol x (pronounced “x-bar”) is used to represent the mean of a sample from a\\npopulation, whereas μ is used to represent the mean of a population.  Why make the\\ndistinction? Information about samples is observed, and information about large\\npopulations is often inferred from smaller samples. Statisticians like to keep the two\\nthings separate in the symbology.\\nKey Ideas\\n•Even in the era of big data, random sampling remains an important arrow in the\\ndata scientist’s quiver.\\n•Bias occurs when measurements or observations are systematically in error\\nbecause they are not representative of the full population.\\n•Data quality is often more important than data quantity, and random sampling\\ncan reduce bias and facilitate quality improvement that would otherwise be pro‐\\nhibitively expensive.\\nFurther Reading\\n•A useful review of sampling procedures can be found in Ronald Fricker’s chapter\\n“Sampling Methods for Online Surveys” in The SAGE Handbook of Online\\nResearch Methods , 2nd ed., edited by Nigel G. Fielding, Raymond M. Lee, and\\nGrant Blank (SAGE Publications, 2016). This chapter includes a review of the\\nmodifications to random sampling that are often used for practical reasons of\\ncost or feasibility.\\nRandom Sampling and Sample Bias | 53', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 70}),\n",
       " Document(page_content='•The story of the Literary Digest  poll failure can be found on the Capital Century\\nwebsite .\\nSelection Bias\\nTo paraphrase Y ogi Berra: if you don’t know what you’re looking for, look hard\\nenough and you’ll find it.\\nSelection bias refers to the practice of selectively choosing data—consciously or\\nunconsciously—in a way that leads to a conclusion that is misleading or ephemeral.\\nKey Terms for Selection Bias\\nSelection bias\\nBias resulting from the way in which observations are selected.\\nData snooping\\nExtensive hunting through data in search of something interesting.\\nVast search effect\\nBias or nonreproducibility resulting from repeated data modeling, or modeling\\ndata with large numbers of predictor variables.\\nIf you specify a hypothesis and conduct a well-designed experiment to test it, you can\\nhave high confidence in the conclusion. This is frequently not what occurs, however.\\nOften, one looks at available data and tries to discern patterns. But are the patterns\\nreal? Or are they just the product of data snooping —that is, extensive hunting\\nthrough the data until something interesting emerges?  There is a saying among statis‐\\nticians: “If you torture the data long enough, sooner or later it will confess. ”\\nThe difference between a phenomenon that you verify when you test a hypothesis\\nusing an experiment and a phenomenon that you discover by perusing available data\\ncan be illuminated with the following thought experiment.\\nImagine that someone tells you they can flip a coin and have it land heads on the next\\n10 tosses. Y ou challenge them (the equivalent of an experiment), and they proceed to\\ntoss the coin 10 times, with all flips landing heads. Clearly you ascribe some special\\ntalent to this person—the probability that 10 coin tosses will land heads just by\\nchance is 1 in 1,000.\\nNow imagine that the announcer at a sports stadium asks the 20,000 people in attend‐\\nance each to toss a coin 10 times, and to report to an usher if they get 10 heads in a\\nrow. The chance that somebody  in the stadium will get 10 heads is extremely high\\n(more than 99%—it’s 1 minus the probability that nobody gets 10 heads). Clearly,\\n54 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 71}),\n",
       " Document(page_content='selecting after the fact the person (or persons) who gets 10 heads at the stadium does\\nnot indicate they have any special talent—it’s most likely luck.\\nSince repeated review of large data sets is a key value proposition in data science,\\nselection bias is something to worry about. A form of selection bias of particular con‐\\ncern to data scientists is what John Elder (founder of Elder Research, a respected data\\nmining consultancy) calls the vast search effect . If you repeatedly run different models\\nand ask different questions with a large data set, you are bound to find something\\ninteresting. But is the result you found truly something interesting, or is it the chance\\noutlier?\\nWe can guard against this by using a holdout set, and sometimes more than one hold‐\\nout set, against which to validate performance. Elder also advocates the use of what\\nhe calls target shuffling  (a permutation test, in essence) to test the validity of predic‐\\ntive associations that a data mining model suggests.\\nTypical forms of selection bias in statistics, in addition to the vast search effect,\\ninclude nonrandom sampling (see “Random Sampling and Sample Bias”  on page 48),\\ncherry-picking data, selection of time intervals that accentuate a particular statistical\\neffect, and stopping an experiment when the results look “interesting. ”\\nRegression to the Mean\\nRegression to the mean  refers to a phenomenon involving successive measurements\\non a given variable: extreme observations tend to be followed by more central ones.\\nAttaching special focus and meaning to the extreme value can lead to a form of selec‐\\ntion bias.\\nSports fans are familiar with the “rookie of the year, sophomore slump” phenomenon.\\nAmong the athletes who begin their career in a given season (the rookie class), there\\nis always one who performs better than all the rest. Generally, this “rookie of the year”\\ndoes not do as well in his second year. Why not?\\nIn nearly all major sports, at least those played with a ball or puck, there are two ele‐\\nments that play a role in overall performance:\\n•Skill\\n•Luck\\nRegression to the mean is a consequence of a particular form of selection bias. When\\nwe select the rookie with the best performance, skill and good luck are probably con‐\\ntributing. In his next season, the skill will still be there, but very often the luck will\\nnot be, so his performance will decline—it will regress. The phenomenon was first\\nidentified by Francis Galton in 1886 [Galton-1886] , who wrote of it in connection\\nSelection Bias | 55', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 72}),\n",
       " Document(page_content='with genetic tendencies; for example, the children of extremely tall men tend not to\\nbe as tall as their father (see Figure 2-5 ).\\nFigure 2-5. Galton’s study that identified  the phenomenon of regression to the mean\\nRegression to the mean, meaning to “go back, ” is distinct from the\\nstatistical modeling method of linear regression, in which a linear\\nrelationship is estimated between predictor variables and an out‐\\ncome variable.\\nKey Ideas\\n•Specifying a hypothesis and then collecting data following randomization and\\nrandom sampling principles ensures against bias.\\n•All other forms of data analysis run the risk of bias resulting from the data collec‐\\ntion/analysis process (repeated running of models in data mining, data snooping\\nin research, and after-the-fact selection of interesting events).\\n56 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 73}),\n",
       " Document(page_content='Further Reading\\n•Christopher J. Pannucci and Edwin G. Wilkins’ article “Identifying and Avoiding\\nBias in Research” in (surprisingly not a statistics journal) Plastic and Reconstruc‐\\ntive Surgery  (August 2010) has an excellent review of various types of bias that\\ncan enter into research, including selection bias.\\n•Michael Harris’s article “Fooled by Randomness Through Selection Bias”  pro‐\\nvides an interesting review of selection bias considerations in stock market trad‐\\ning schemes, from the perspective of traders.\\nSampling Distribution of a Statistic\\nThe term sampling distribution  of a statistic refers to the distribution of some sample\\nstatistic over many samples drawn from the same population.  Much of classical statis‐\\ntics is concerned with making inferences from (small) samples to (very large) popula‐\\ntions.\\nKey Terms for Sampling Distribution\\nSample statistic\\nA metric calculated for a sample of data drawn from a larger population.\\nData distribution\\nThe frequency distribution of individual values  in a data set.\\nSampling distribution\\nThe frequency distribution of a sample statistic  over many samples or resamples.\\nCentral limit theorem\\nThe tendency of the sampling distribution to take on a normal shape as sample\\nsize rises.\\nStandard error\\nThe variability (standard deviation) of a sample statistic  over many samples (not\\nto be confused with standard deviation , which by itself, refers to variability of\\nindividual data values ).\\nTypically, a sample is drawn with the goal of measuring something (with a sample sta‐\\ntistic ) or modeling something (with a statistical or machine learning model). Since\\nour estimate or model is based on a sample, it might be in error; it might be different\\nif we were to draw a different sample. We are therefore interested in how different it\\nmight be—a key concern is sampling variability . If we had lots of data, we could draw\\nadditional samples and observe the distribution of a sample statistic directly.\\nSampling Distribution of a Statistic | 57', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 74}),\n",
       " Document(page_content='Typically, we will calculate our estimate or model using as much data as is easily avail‐\\nable, so the option of drawing additional samples from the population is not readily\\navailable.\\nIt is important to distinguish between the distribution of the indi‐\\nvidual data points, known as the data distribution , and the distribu‐\\ntion of a sample statistic, known as the sampling distribution .\\nThe distribution of a sample statistic such as the mean is likely to be more regular and\\nbell-shaped than the distribution of the data itself. The larger the sample the statistic\\nis based on, the more this is true. Also, the larger the sample, the narrower the distri‐\\nbution of the sample statistic.\\nThis is illustrated in an example using annual income for loan applicants to Lending‐\\nClub (see “ A Small Example: Predicting Loan Default” on page 239 for a description\\nof the data). Take three samples from this data: a sample of 1,000 values, a sample of\\n1,000 means of 5 values, and a sample of 1,000 means of 20 values. Then plot a histo‐\\ngram of each sample to produce Figure 2-6 .\\nFigure 2-6. Histogram of annual incomes of 1,000 loan applicants (top), then 1,000\\nmeans of n=5 applicants (middle), and finally  1,000 means of n=20 applicants (bottom)\\n58 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 75}),\n",
       " Document(page_content=\"The histogram of the individual data values is broadly spread out and skewed toward\\nhigher values, as is to be expected with income data. The histograms of the means of\\n5 and 20 are increasingly compact and more bell-shaped. Here is the R code to gener‐\\nate these histograms, using the visualization package ggplot2 :\\nlibrary(ggplot2)\\n# take a simple random sample\\nsamp_data  <- data.frame (income=sample(loans_income , 1000),\\n                        type='data_dist' )\\n# take a sample of means of 5 values\\nsamp_mean_05  <- data.frame (\\n  income = tapply(sample(loans_income , 1000*5),\\n                  rep(1:1000, rep(5, 1000)), FUN=mean),\\n  type = 'mean_of_5' )\\n# take a sample of means of 20 values\\nsamp_mean_20  <- data.frame (\\n  income = tapply(sample(loans_income , 1000*20),\\n                  rep(1:1000, rep(20, 1000)), FUN=mean),\\n  type = 'mean_of_20' )\\n# bind the data.frames and convert type to a factor\\nincome <- rbind(samp_data , samp_mean_05 , samp_mean_20 )\\nincome$type = factor(income$type,\\n                     levels=c('data_dist' , 'mean_of_5' , 'mean_of_20' ),\\n                     labels=c('Data', 'Mean of 5' , 'Mean of 20' ))\\n# plot the histograms\\nggplot(income, aes(x=income)) +\\n  geom_histogram (bins=40) +\\n  facet_grid (type ~ .)\\nThe Python  code uses seaborn ’s FacetGrid  to show the three histograms:\\nimport pandas as pd\\nimport seaborn as sns\\nsample_data  = pd.DataFrame ({\\n    'income' : loans_income .sample(1000),\\n    'type': 'Data',\\n})\\nsample_mean_05  = pd.DataFrame ({\\n    'income' : [loans_income .sample(5).mean() for _ in range(1000)],\\n    'type': 'Mean of 5' ,\\n})\\nsample_mean_20  = pd.DataFrame ({\\n    'income' : [loans_income .sample(20).mean() for _ in range(1000)],\\n    'type': 'Mean of 20' ,\\n})\\nresults = pd.concat([sample_data , sample_mean_05 , sample_mean_20 ])\\ng = sns.FacetGrid (results, col='type', col_wrap =1, height=2, aspect=2)\\ng.map(plt.hist, 'income' , range=[0, 200000], bins=40)\\nSampling Distribution of a Statistic | 59\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 76}),\n",
       " Document(page_content=\"g.set_axis_labels ('Income' , 'Count')\\ng.set_titles ('{col_name}' )\\nCentral Limit Theorem\\nThe phenomenon we’ve just described is termed the central limit theorem . It says that\\nthe means drawn from multiple samples will resemble the familiar bell-shaped nor‐\\nmal curve (see “Normal Distribution”  on page 69), even if the source population is\\nnot normally distributed, provided that the sample size is large enough and the\\ndeparture of the data from normality is not too great. The central limit theorem\\nallows normal-approximation formulas like the t-distribution to be used in calculat‐\\ning sampling distributions for inference—that is, confidence intervals and hypothesis\\ntests.\\nThe central limit theorem receives a lot of attention in traditional statistics texts\\nbecause it underlies the machinery of hypothesis tests and confidence intervals,\\nwhich themselves consume half the space in such texts. Data scientists should be\\naware of this role; however, since formal hypothesis tests and confidence intervals\\nplay a small role in data science, and the bootstrap  (see “The Bootstrap” on page 61 ) is\\navailable in any case, the central limit theorem is not so central in the practice of data\\nscience.\\nStandard Error\\nThe standard error  is a single metric that sums up the variability in the sampling dis‐\\ntribution for a statistic.  The standard error can be estimated using a statistic based on\\nthe standard deviation s of the sample values, and the sample size n:\\nStandard error = SE=s\\nn\\nAs the sample size increases, the standard error decreases, corresponding to what was\\nobserved in Figure 2-6 . The relationship between standard error and sample size is\\nsometimes referred to as the square root of n  rule: to reduce the standard error by a\\nfactor of 2, the sample size must be increased by a factor of 4.\\nThe validity of the standard error formula arises from the central limit theorem. In\\nfact, you don’t need to rely on the central limit theorem to understand standard error.\\nConsider the following approach to measuring standard error:\\n1.Collect a number of brand-new samples from the population.\\n2.For each new sample, calculate the statistic (e.g., mean).\\n60 | Chapter 2: Data and Sampling Distributions\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 77}),\n",
       " Document(page_content='3.Calculate the standard deviation of the statistics computed in step 2; use this as\\nyour estimate of standard error.\\nIn practice, this approach of collecting new samples to estimate the standard error is\\ntypically not feasible (and statistically very wasteful). Fortunately, it turns out that it is\\nnot necessary to draw brand new samples; instead, you can use bootstrap  resamples.\\nIn modern statistics, the bootstrap has become the standard way to estimate standard\\nerror. It can be used for virtually any statistic and does not rely on the central limit\\ntheorem or other distributional assumptions.\\nStandard Deviation Versus Standard Error\\nDo not confuse standard deviation (which measures the variability\\nof individual data points) with standard error (which measures the\\nvariability of a sample metric).\\nKey Ideas\\n•The frequency distribution of a sample statistic tells us how that metric would\\nturn out differently from sample to sample.\\n•This sampling distribution can be estimated via the bootstrap, or via formulas\\nthat rely on the central limit theorem.\\n•A key metric that sums up the variability of a sample statistic is its standard error.\\nFurther Reading\\nDavid Lane’s online multimedia resource in statistics  has a useful simulation that\\nallows you to select a sample statistic, a sample size, and the number of iterations and\\nvisualize a histogram of the resulting frequency distribution.\\nThe Bootstrap\\nOne easy and effective way to estimate the sampling distribution of a statistic, or of\\nmodel parameters, is to draw additional samples, with replacement, from the sample\\nitself and recalculate the statistic or model for each resample.  This procedure is called\\nthe bootstrap , and it does not necessarily involve any assumptions about the data or\\nthe sample statistic being normally distributed.\\nThe Bootstrap | 61', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 78}),\n",
       " Document(page_content='Key Terms for the Bootstrap\\nBootstrap sample\\nA sample taken with replacement from an observed data set.\\nResampling\\nThe process of taking repeated samples from observed data; includes both boot‐\\nstrap and permutation (shuffling) procedures.\\nConceptually, you can imagine the bootstrap as replicating the original sample thou‐\\nsands or millions of times so that you have a hypothetical population that embodies\\nall the knowledge from your original sample (it’s just larger). Y ou can then draw sam‐\\nples from this hypothetical population for the purpose of estimating a sampling dis‐\\ntribution; see Figure 2-7 .\\nFigure 2-7. The idea of the bootstrap\\nIn practice, it is not necessary to actually replicate the sample a huge number of\\ntimes.  We simply replace each observation after each draw; that is, we sample with\\nreplacement . In this way we effectively create an infinite population in which the\\nprobability of an element being drawn remains unchanged from draw to draw.  The\\nalgorithm for a bootstrap resampling of the mean, for a sample of size n, is as follows:\\n1.Draw a sample value, record it, and then replace it.\\n2.Repeat n times.\\n3.Record the mean of the n resampled values.\\n4.Repeat steps 1–3 R times.\\n5.Use the R results to:\\n62 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 79}),\n",
       " Document(page_content=\"a.Calculate their standard deviation (this estimates sample mean standard\\nerror).\\nb.Produce a histogram or boxplot.\\nc.Find a confidence interval.\\nR, the number of iterations of the bootstrap, is set somewhat arbitrarily. The more\\niterations you do, the more accurate the estimate of the standard error, or the confi‐\\ndence interval. The result from this procedure is a bootstrap set of sample statistics or\\nestimated model parameters, which you can then examine to see how variable they\\nare.\\nThe R package boot  combines these steps in one function. For example, the following\\napplies the bootstrap to the incomes of people taking out loans:\\nlibrary(boot)\\nstat_fun  <- function (x, idx) median(x[idx])\\nboot_obj  <- boot(loans_income , R=1000, statistic =stat_fun )\\nThe function stat_fun  computes the median for a given sample specified by the\\nindex idx. The result is as follows:\\nBootstrap  Statistics  :\\n    original    bias    std. error\\nt1*    62000 -70.5595     209.1515\\nThe original estimate of the median is $62,000. The bootstrap distribution indicates\\nthat the estimate has a bias of about –$70 and a standard error of $209. The results\\nwill vary slightly between consecutive runs of the algorithm.\\nThe major Python  packages don’t provide implementations of the bootstrap\\napproach. It can be implemented using the scikit-learn  method resample :\\nresults = []\\nfor nrepeat in range(1000):\\n    sample = resample (loans_income )\\n    results.append(sample.median())\\nresults = pd.Series(results)\\nprint('Bootstrap Statistics:' )\\nprint(f'original: {loans_income.median()}' )\\nprint(f'bias: {results.mean() - loans_income.median()}' )\\nprint(f'std. error: {results.std()}' )\\nThe bootstrap can be used with multivariate data, where the rows are sampled as\\nunits (see Figure 2-8 ). A model  might then be run on the bootstrapped data, for\\nexample, to estimate the stability (variability) of model parameters, or to improve\\npredictive power. With classification and regression trees (also called decision trees ),\\nrunning multiple trees on bootstrap samples and then averaging their predictions (or,\\nwith classification, taking a majority vote) generally performs better than using a\\nThe Bootstrap | 63\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 80}),\n",
       " Document(page_content='single tree. This process is called bagging  (short for “bootstrap aggregating”; see “Bag‐\\nging and the Random Forest” on page 259 ).\\nFigure 2-8. Multivariate bootstrap sampling\\nThe repeated resampling of the bootstrap is conceptually simple, and Julian Simon,\\nan economist and demographer, published a compendium of resampling examples,\\nincluding the bootstrap, in his 1969 text Basic Research Methods in Social Science\\n(Random House). However, it is also computationally intensive and was not a feasible\\noption before the widespread availability of computing power. The technique gained\\nits name and took off with the publication of several journal articles and a book by\\nStanford statistician Bradley Efron in the late 1970s and early 1980s. It was particu‐\\nlarly popular among researchers who use statistics but are not statisticians, and for\\nuse with metrics or models where mathematical approximations are not readily avail‐\\nable. The sampling distribution of the mean has been well established since 1908; the\\nsampling distribution of many other metrics has not. The bootstrap can be used for\\nsample size determination; experiment with different values for n to see how the sam‐\\npling distribution is affected.\\nThe bootstrap was met with considerable skepticism when it was first introduced; it\\nhad the aura to many of spinning gold from straw. This skepticism stemmed from a\\nmisunderstanding of the bootstrap’s purpose.\\nThe bootstrap does not compensate for a small sample size; it does\\nnot create new data, nor does it fill in holes in an existing data set.\\nIt merely informs us about how lots of additional samples would\\nbehave when drawn from a population like our original sample.\\n64 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 81}),\n",
       " Document(page_content='Resampling Versus Bootstrapping\\nSometimes the term resampling  is used synonymously with the term bootstrapping , as\\njust outlined. More often, the term resampling  also includes permutation procedures\\n(see “Permutation Test” on page 97), where multiple samples are combined and the\\nsampling may be done without replacement. In any case, the term bootstrap  always\\nimplies sampling with replacement from an observed data set.\\nKey Ideas\\n•The bootstrap (sampling with replacement from a data set) is a powerful tool for\\nassessing the variability of a sample statistic.\\n•The bootstrap can be applied in similar fashion in a wide variety of circumstan‐\\nces, without extensive study of mathematical approximations to sampling distri‐\\nbutions.\\n•It also allows us to estimate sampling distributions for statistics where no mathe‐\\nmatical approximation has been developed.\\n•When applied to predictive models, aggregating multiple bootstrap sample pre‐\\ndictions (bagging) outperforms the use of a single model.\\nFurther Reading\\n•An Introduction to the Bootstrap  by Bradley Efron and Robert Tibshirani (Chap‐\\nman & Hall, 1993) was the first book-length treatment of the bootstrap. It is still\\nwidely read.\\n•The retrospective on the bootstrap in the May 2003 issue of Statistical Science\\n(vol. 18, no. 2), discusses (among other antecedents, in Peter Hall’s “ A Short Pre‐\\nhistory of the Bootstrap”) Julian Simon’s initial publication of the bootstrap in\\n1969.\\n•See An Introduction to Statistical Learning  by Gareth James, Daniela Witten, Tre‐\\nvor Hastie, and Robert Tibshirani (Springer, 2013) for sections on the bootstrap\\nand, in particular, bagging.\\nConfidence  Intervals\\nFrequency tables, histograms, boxplots, and standard errors are all ways to under‐\\nstand the potential error in a sample estimate. Confidence intervals are another.\\nConfidence  Intervals | 65', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 82}),\n",
       " Document(page_content='Key Terms for Confidence  Intervals\\nConfidence  level\\nThe percentage of confidence intervals, constructed in the same way from the\\nsame population, that are expected to contain the statistic of interest.\\nInterval endpoints\\nThe top and bottom of the confidence interval.\\nThere is a natural human aversion to uncertainty; people (especially experts) say “I\\ndon’t know” far too rarely. Analysts and managers, while acknowledging uncertainty,\\nnonetheless place undue faith in an estimate when it is presented as a single number\\n(a point estimate ). Presenting an estimate not as a single number but as a range is one\\nway to counteract this tendency. Confidence intervals do this in a manner grounded\\nin statistical sampling principles.\\nConfidence intervals always come with a coverage level, expressed as a (high) per‐\\ncentage, say 90% or 95%. One way to think of a 90% confidence interval is as follows:\\nit is the interval that encloses the central 90% of the bootstrap sampling distribution\\nof a sample statistic (see “The Bootstrap”  on page 61). More generally, an x% confi‐\\ndence interval around a sample estimate should, on average, contain similar sample\\nestimates x% of the time (when a similar sampling procedure is followed).\\nGiven a sample of size n, and a sample statistic of interest, the algorithm for a boot‐\\nstrap confidence interval is as follows:\\n1.Draw a random sample of size n with replacement from the data (a resample).\\n2.Record the statistic of interest for the resample.\\n3.Repeat steps 1–2 many ( R) times.\\n4.For an x% confidence interval, trim [(100- x) / 2]% of the R resample results from\\neither end of the distribution.\\n5.The trim points are the endpoints of an x% bootstrap confidence interval.\\nFigure 2-9  shows a 90% confidence interval for the mean annual income of loan\\napplicants, based on a sample of 20 for which the mean was $62,231.\\n66 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 83}),\n",
       " Document(page_content='Figure 2-9. Bootstrap confidence  interval for the annual income of loan applicants,\\nbased on a sample of 20\\nThe bootstrap is a general tool that can be used to generate confidence intervals for\\nmost statistics, or model parameters. Statistical textbooks and software, with roots in\\nover a half century of computerless statistical analysis, will also reference confidence\\nintervals generated by formulas, especially the t-distribution (see “Student’s t-\\nDistribution” on page 75 ).\\nOf course, what we are really interested in when we have a sample\\nresult is, “What is the probability that the true value lies within a\\ncertain interval?” This is not really the question that a confidence\\ninterval answers, but it ends up being how most people interpret\\nthe answer.\\nThe probability question associated with a confidence interval\\nstarts out with the phrase “Given a sampling procedure and a pop‐\\nulation, what is the probability that… ” To go in the opposite direc‐\\ntion, “Given a sample result, what is the probability that\\n(something is true about the population)?” involves more complex\\ncalculations and deeper imponderables.\\nConfidence  Intervals | 67', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 84}),\n",
       " Document(page_content='The percentage associated with the confidence interval is termed the level of confi‐\\ndence . The higher the level of confidence, the wider the interval. Also, the smaller the\\nsample, the wider the interval (i.e., the greater the uncertainty). Both make sense: the\\nmore confident you want to be, and the less data you have, the wider you must make\\nthe confidence interval to be sufficiently assured of capturing the true value.\\nFor a data scientist, a confidence interval is a tool that can be used\\nto get an idea of how variable a sample result might be.  Data scien‐\\ntists would use this information not to publish a scholarly paper or\\nsubmit a result to a regulatory agency (as a researcher might) but\\nmost likely to communicate the potential error in an estimate, and\\nperhaps to learn whether a larger sample is needed.\\nKey Ideas\\n•Confidence intervals are the typical way to present estimates as an interval range.\\n•The more data you have, the less variable a sample estimate will be.\\n•The lower the level of confidence you can tolerate, the narrower the confidence\\ninterval will be.\\n•The bootstrap is an effective way to construct confidence intervals.\\nFurther Reading\\n•For a bootstrap approach to confidence intervals, see Introductory Statistics and\\nAnalytics: A Resampling Perspective  by Peter Bruce (Wiley, 2014) or Statistics:\\nUnlocking the Power of Data , 2nd ed., by Robin Lock and four other Lock family\\nmembers (Wiley, 2016).\\n•Engineers, who have a need to understand the precision of their measurements,\\nuse confidence intervals perhaps more than most disciplines, and Modern Engi‐\\nneering Statistics  by Thomas Ryan (Wiley, 2007) discusses confidence intervals.  It\\nalso reviews a tool that is just as useful and gets less attention: prediction intervals\\n(intervals around a single value, as opposed to a mean or other summary\\nstatistic).\\n68 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 85}),\n",
       " Document(page_content='1The bell curve is iconic but perhaps overrated. George W . Cobb, the Mount Holyoke statistician noted for his\\ncontribution to the philosophy of teaching introductory statistics, argued in a November 2015 editorial in the\\nAmerican Statistician  that the “standard introductory course, which puts the normal distribution at its center,\\nhad outlived the usefulness of its centrality. ”\\nNormal Distribution\\nThe bell-shaped normal distribution is iconic in traditional statistics.1 The fact that\\ndistributions of sample statistics are often normally shaped has made it a powerful\\ntool in the development of mathematical formulas that approximate those\\ndistributions.\\nKey Terms for Normal Distribution\\nError\\nThe difference between a data point and a predicted or average value.\\nStandardize\\nSubtract the mean and divide by the standard deviation.\\nz-score\\nThe result of standardizing an individual data point.\\nStandard normal\\nA normal distribution with mean = 0 and standard deviation = 1.\\nQQ-Plot\\nA plot to visualize how close a sample distribution is to a specified distribution,\\ne.g., the normal distribution.\\nIn a normal distribution ( Figure 2-10 ), 68% of the data lies within one standard devi‐\\nation of the mean, and 95% lies within two standard deviations.\\nIt is a common misconception that the normal distribution is\\ncalled that because most data follows a normal distribution—that\\nis, it is the normal thing. Most of the variables used in a typical data\\nscience project—in fact, most raw data as a whole—are not nor‐\\nmally distributed: see “Long-Tailed Distributions” on page 73. The\\nutility of the normal distribution derives from the fact that many\\nstatistics are normally distributed in their sampling distribution.\\nEven so, assumptions of normality are generally a last resort, used\\nwhen empirical probability distributions, or bootstrap distribu‐\\ntions, are not available.\\nNormal Distribution | 69', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 86}),\n",
       " Document(page_content='Figure 2-10. Normal curve\\nThe normal distribution is also referred to as a Gaussian  distribu‐\\ntion after Carl Friedrich Gauss, a prodigious German mathemati‐\\ncian from the late 18th and early 19th centuries. Another name\\npreviously used for the normal distribution was the “error” distri‐\\nbution. Statistically speaking, an error  is the difference between an\\nactual value and a statistical estimate like the sample mean. For\\nexample, the standard deviation (see “Estimates of Variability” on\\npage 13) is based on the errors from the mean of the data. Gauss’s\\ndevelopment of the normal distribution came from his study of the\\nerrors of astronomical measurements that were found to be nor‐\\nmally distributed.\\n70 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 87}),\n",
       " Document(page_content=\"Standard Normal and QQ-Plots\\nA standard normal  distribution is one in which the units on the x-axis are expressed\\nin terms of standard deviations away from the mean. To compare data to a standard\\nnormal distribution, you subtract the mean and then divide by the standard devia‐\\ntion; this is also called normalization  or standardization  (see “Standardization (Nor‐\\nmalization, z-Scores)” on page 243). Note that “standardization” in this sense is\\nunrelated to database record standardization (conversion to a common format).  The\\ntransformed value is termed a z-score , and the normal distribution is sometimes\\ncalled the z-distribution .\\nA QQ-Plot  is used to visually determine how close a sample is to a specified distribu‐\\ntion—in this case, the normal distribution. The QQ-Plot orders the z-scores from low\\nto high and plots each value’s z-score on the y-axis; the x-axis is the corresponding\\nquantile of a normal distribution for that value’s rank. Since the data is normalized,\\nthe units correspond to the number of standard deviations away from the mean. If\\nthe points roughly fall on the diagonal line, then the sample distribution can be con‐\\nsidered close to normal. Figure 2-11  shows a QQ-Plot for a sample of 100 values ran‐\\ndomly generated from a normal distribution; as expected, the points closely follow\\nthe line. This figure can be produced in R with the qqnorm  function:\\nnorm_samp  <- rnorm(100)\\nqqnorm(norm_samp )\\nabline(a=0, b=1, col='grey')\\nIn Python , use the method scipy.stats.probplot  to create the QQ-Plot:\\nfig, ax = plt.subplots (figsize=(4, 4))\\nnorm_sample  = stats.norm.rvs(size=100)\\nstats.probplot (norm_sample , plot=ax)\\nNormal Distribution | 71\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 88}),\n",
       " Document(page_content='Figure 2-11. QQ-Plot of a sample of 100 values drawn from a standard normal\\ndistribution\\nConverting data to z-scores (i.e., standardizing or normalizing the\\ndata) does not make the data normally distributed. It just puts the\\ndata on the same scale as the standard normal distribution, often\\nfor comparison purposes.\\nKey Ideas\\n•The normal distribution was essential to the historical development of statistics,\\nas it permitted mathematical approximation of uncertainty and variability.\\n•While raw data is typically not normally distributed, errors often are, as are aver‐\\nages and totals in large samples.\\n•To convert data to z-scores, you subtract the mean of the data and divide by the\\nstandard deviation; you can then compare the data to a normal distribution.\\n72 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 89}),\n",
       " Document(page_content=\"Long-Tailed Distributions\\nDespite the importance of the normal distribution historically in statistics, and in\\ncontrast to what the name would suggest, data is generally not normally distributed.\\nKey Terms for Long-Tailed Distributions\\nTail\\nThe long narrow portion of a frequency distribution, where relatively extreme\\nvalues occur at low frequency.\\nSkew\\nWhere one tail of a distribution is longer than the other.\\nWhile the normal distribution is often appropriate and useful with respect to the dis‐\\ntribution of errors and sample statistics, it typically does not characterize the distribu‐\\ntion of raw data. Sometimes, the distribution is highly skewed  (asymmetric), such as\\nwith income data; or the distribution can be discrete, as with binomial data. Both\\nsymmetric and asymmetric distributions may have long tails . The tails of a distribu‐\\ntion correspond to the extreme values (small and large). Long tails, and guarding\\nagainst them, are widely recognized in practical work. Nassim Taleb has proposed the\\nblack swan  theory, which predicts that anomalous events, such as a stock market\\ncrash, are much more likely to occur than would be predicted by the normal\\ndistribution.\\nA good example to illustrate the long-tailed nature of data is stock returns.\\nFigure 2-12  shows the QQ-Plot for the daily stock returns for Netflix (NFLX). This is\\ngenerated in R by:\\nnflx <- sp500_px [,'NFLX']\\nnflx <- diff(log(nflx[nflx>0]))\\nqqnorm(nflx)\\nabline(a=0, b=1, col='grey')\\nThe corresponding Python  code is:\\nnflx = sp500_px .NFLX\\nnflx = np.diff(np.log(nflx[nflx>0]))\\nfig, ax = plt.subplots (figsize=(4, 4))\\nstats.probplot (nflx, plot=ax)\\nLong-Tailed Distributions | 73\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 90}),\n",
       " Document(page_content='Figure 2-12. QQ-Plot of the returns for Netflix  (NFLX)\\nIn contrast to Figure 2-11 , the points are far below the line for low values and far\\nabove the line for high values, indicating the data are not normally distributed. This\\nmeans that we are much more likely to observe extreme values than would be\\nexpected if the data had a normal distribution. Figure 2-12  shows another common\\nphenomenon: the points are close to the line for the data within one standard devia‐\\ntion of the mean. Tukey refers to this phenomenon as data being “normal in the mid‐\\ndle” but having much longer tails (see [Tukey-1987] ).\\n74 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 91}),\n",
       " Document(page_content='There is much statistical literature about the task of fitting statisti‐\\ncal distributions to observed data. Beware an excessively data-\\ncentric approach to this job, which is as much art as science.  Data is\\nvariable, and often consistent, on its face, with more than one\\nshape and type of distribution. It is typically the case that domain\\nand statistical knowledge must be brought to bear to determine\\nwhat type of distribution is appropriate to model a given situation.\\nFor example, we might have data on the level of internet traffic on a\\nserver over many consecutive five-second periods. It is useful to\\nknow that the best distribution to model “events per time period” is\\nthe Poisson (see “Poisson Distributions” on page 83 ).\\nKey Ideas\\n•Most data is not normally distributed.\\n•Assuming a normal distribution can lead to underestimation of extreme events\\n(“black swans”).\\nFurther Reading\\n•The Black Swan , 2nd ed., by Nassim Nicholas Taleb (Random House, 2010)\\n•Handbook of Statistical Distributions with Applications , 2nd ed., by K. Krishna‐\\nmoorthy (Chapman & Hall/CRC Press, 2016)\\nStudent’s t-Distribution\\nThe t-distribution  is a normally shaped distribution, except that it is a bit thicker and\\nlonger on the tails. It is used extensively in depicting distributions of sample statistics.\\nDistributions of sample means are typically shaped like a t-distribution, and there is a\\nfamily of t-distributions that differ depending on how large the sample is. The larger\\nthe sample, the more normally shaped the t-distribution becomes.\\nStudent’s t-Distribution | 75', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 92}),\n",
       " Document(page_content='Key Terms for Student’s t-Distribution\\nn\\nSample size.\\nDegrees of freedom\\nA parameter that allows the t-distribution to adjust to different sample sizes, sta‐\\ntistics, and numbers of groups.\\nThe t-distribution is often called Student’s t  because it was published in 1908 in Bio‐\\nmetrika  by W . S. Gosset under the name “Student. ” Gosset’s employer, the Guinness\\nbrewery, did not want competitors to know that it was using statistical methods, so it\\ninsisted that Gosset not use his name on the article.\\nGosset wanted to answer the question “What is the sampling distribution of the mean\\nof a sample, drawn from a larger population?” He started out with a resampling\\nexperiment—drawing random samples of 4 from a data set of 3,000 measurements of\\ncriminals’ height and left-middle-finger length. (This being the era of eugenics, there\\nwas much interest in data on criminals, and in discovering correlations between\\ncriminal tendencies and physical or psychological attributes.) Gosset plotted the stan‐\\ndardized results (the z-scores) on the x-axis and the frequency on the y-axis. Sepa‐\\nrately, he had derived a function, now known as Student’s t , and he fit this function\\nover the sample results, plotting the comparison (see Figure 2-13 ).\\nFigure 2-13. Gosset’s resampling experiment results and fitted  t-curve (from his 1908\\nBiometrika paper)\\n76 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 93}),\n",
       " Document(page_content='A number of different statistics can be compared, after standardization, to the t-\\ndistribution, to estimate confidence intervals in light of sampling variation. Consider\\na sample of size n for which the sample mean x has been calculated. If s is the sample\\nstandard deviation, a 90% confidence interval around the sample mean is given by:\\nx±tn− 10 . 05 ·s\\nn\\nwhere tn− 1. 05 is the value of the t-statistic, with ( n – 1) degrees of freedom (see\\n“Degrees of Freedom” on page 116 ), that “chops off ” 5% of the t-distribution at either\\nend.  The t-distribution has been used as a reference for the distribution of a sample\\nmean, the difference between two sample means, regression parameters, and other\\nstatistics.\\nHad computing power been widely available in 1908, statistics would no doubt have\\nrelied much more heavily on computationally intensive resampling methods from the\\nstart. Lacking computers, statisticians turned to mathematics and functions such as\\nthe t-distribution to approximate sampling distributions. Computer power enabled\\npractical resampling experiments in the 1980s, but by then, use of the t-distribution\\nand similar distributions had become deeply embedded in textbooks and software.\\nThe t-distribution’s accuracy in depicting the behavior of a sample statistic requires\\nthat the distribution of that statistic for that sample be shaped like a normal distribu‐\\ntion. It turns out that sample statistics are often normally distributed, even when the\\nunderlying population data is not (a fact which led to widespread application of the t-\\ndistribution). This brings us back to the phenomenon known as the central limit theo‐\\nrem (see “Central Limit Theorem” on page 60 ).\\nWhat do data scientists need to know about the t-distribution and\\nthe central limit theorem? Not a whole lot. The t-distribution is\\nused in classical statistical inference but is not as central to the pur‐\\nposes of data science. Understanding and quantifying uncertainty\\nand variation are important to data scientists, but empirical boot‐\\nstrap sampling can answer most questions about sampling error.\\nHowever, data scientists will routinely encounter t-statistics in out‐\\nput from statistical software and statistical procedures in R—for\\nexample, in A/B tests and regressions—so familiarity with its pur‐\\npose is helpful.\\nStudent’s t-Distribution | 77', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 94}),\n",
       " Document(page_content='Key Ideas\\n•The t-distribution is actually a family of distributions resembling the normal dis‐\\ntribution but with thicker tails.\\n•The t-distribution is widely used as a reference basis for the distribution of sam‐\\nple means, differences between two sample means, regression parameters, and\\nmore.\\nFurther Reading\\n•The original W .S. Gosset paper as published in Biometrika  in 1908 is available as\\na PDF .\\n•A standard treatment of the t-distribution can be found in David Lane’s online\\nresource .\\nBinomial Distribution\\nY es/no (binomial) outcomes lie at the heart of analytics since they are often the cul‐\\nmination of a decision or other process; buy/don’t buy, click/don’t click, survive/die,\\nand so on. Central to understanding the binomial distribution is the idea of a set of\\ntrials , each trial having two possible outcomes with definite probabilities.\\nFor example, flipping a coin 10 times is a binomial experiment with 10 trials, each\\ntrial having two possible outcomes (heads or tails); see Figure 2-14 . Such yes/no or\\n0/1 outcomes are termed binary  outcomes, and they need not have 50/50 probabili‐\\nties. Any probabilities that sum to 1.0 are possible. It is conventional in statistics to\\nterm the “1” outcome the success  outcome; it is also common practice to assign “1” to\\nthe more rare outcome. Use of the term success  does not imply that the outcome is\\ndesirable or beneficial, but it does tend to indicate the outcome of interest.  For exam‐\\nple, loan defaults or fraudulent transactions are relatively uncommon events that we\\nmay be interested in predicting, so they are termed “1s” or “successes. ”\\nFigure 2-14. The tails side of a buffalo  nickel\\n78 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 95}),\n",
       " Document(page_content='Key Terms for Binomial Distribution\\nTrial\\nAn event with a discrete outcome (e.g., a coin flip).\\nSuccess\\nThe outcome of interest for a trial.\\nSynonym\\n“1” (as opposed to “0”)\\nBinomial\\nHaving two outcomes.\\nSynonyms\\nyes/no, 0/1, binary\\nBinomial trial\\nA trial with two outcomes.\\nSynonym\\nBernoulli trial\\nBinomial distribution\\nDistribution of number of successes in x trials.\\nSynonym\\nBernoulli distribution\\nThe binomial distribution is the frequency distribution of the number of successes ( x)\\nin a given number of trials ( n) with specified probability ( p) of success in each trial.\\nThere is a family of binomial distributions, depending on the values of n and p. The\\nbinomial distribution would answer a question like:\\nIf the probability of a click converting to a sale is 0.02, what is the probability of\\nobserving 0 sales in 200 clicks?\\nThe R function dbinom  calculates binomial probabilities. For example:\\ndbinom(x=2, size=5, p=0.1)\\nwould return 0.0729, the probability of observing exactly x = 2 successes in size = 5\\ntrials, where the probability of success for each trial is p = 0.1. For our example above,\\nwe use x = 0, size = 200, and p = 0.02. With these arguments, dbinom  returns a proba‐\\nbility of 0.0176.\\nOften we are interested in determining the probability of x or fewer successes in n\\ntrials. In this case, we use the function pbinom :\\nBinomial Distribution | 79', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 96}),\n",
       " Document(page_content='pbinom(2, 5, 0.1)\\nThis would return 0.9914, the probability of observing two or fewer successes in five\\ntrials, where the probability of success for each trial is 0.1.\\nThe scipy.stats  module implements a large variety of statistical distributions. For\\nthe binomial distribution, use the functions stats.binom.pmf  and stats.binom.cdf :\\nstats.binom.pmf(2, n=5, p=0.1)\\nstats.binom.cdf(2, n=5, p=0.1)\\nThe mean of a binomial distribution is n×p; you can also think of this as the\\nexpected number of successes in n trials, for success probability = p.\\nThe variance is n×p1 −p. With a large enough number of trials (particularly when\\np is close to 0.50), the binomial distribution is virtually indistinguishable from the\\nnormal distribution. In fact, calculating binomial probabilities with large sample sizes\\nis computationally demanding, and most statistical procedures use the normal distri‐\\nbution, with mean and variance, as an approximation.\\nKey Ideas\\n•Binomial outcomes are important to model, since they represent, among other\\nthings, fundamental decisions (buy or don’t buy, click or don’t click, survive or\\ndie, etc.).\\n•A binomial trial is an experiment with two possible outcomes: one with probabil‐\\nity p and the other with probability 1 – p .\\n•With large n, and provided p is not too close to 0 or 1, the binomial distribution\\ncan be approximated by the normal distribution.\\nFurther Reading\\n•Read about the “quincunx” , a pinball-like simulation device for illustrating the\\nbinomial distribution.\\n•The binomial distribution is a staple of introductory statistics, and all introduc‐\\ntory statistics texts will have a chapter or two on it.\\nChi-Square Distribution\\nAn important idea in statistics is departure from expectation , especially with respect to\\ncategory counts. Expectation is defined loosely as “nothing unusual or of note in the\\ndata” (e.g., no correlation between variables or predictable patterns). This is also\\n80 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 97}),\n",
       " Document(page_content='termed the “null hypothesis” or “null model” (see “The Null Hypothesis”  on page 94).\\nFor example, you might want to test whether one variable (say, a row variable repre‐\\nsenting gender) is independent of another (say, a column variable representing “was\\npromoted in job”), and you have counts of the number in each of the cells of the data\\ntable. The statistic that measures the extent to which results depart from the null\\nexpectation of independence is the chi-square statistic. It is the difference between the\\nobserved and expected values, divided by the square root of the expected value,\\nsquared, then summed across all categories. This process standardizes the statistic so\\nit can be compared to a reference distribution. A more general way of putting this is\\nto note that the chi-square statistic is a measure of the extent to which a set of\\nobserved values “fits” a specified distribution (a “goodness-of-fit” test). It is useful for\\ndetermining whether multiple treatments (an “ A/B/C… test”) differ from one another\\nin their effects.\\nThe chi-square distribution is the distribution of this statistic under repeated resam‐\\npled draws from the null model—see “Chi-Square Test” on page 124 for a detailed\\nalgorithm, and the chi-square formula for a data table. A low chi-square value for a\\nset of counts indicates that they closely follow the expected distribution. A high chi-\\nsquare indicates that they differ markedly from what is expected. There are a variety\\nof chi-square distributions associated with different degrees of freedom (e.g., number\\nof observations—see “Degrees of Freedom” on page 116 ).\\nKey Ideas\\n•The chi-square distribution is typically concerned with counts of subjects or\\nitems falling into categories.\\n•The chi-square statistic measures the extent of departure from what you would\\nexpect in a null model.\\nFurther Reading\\n•The chi-square distribution owes its place in modern statistics to the great statis‐\\ntician Karl Pearson and the birth of hypothesis testing—read about this and more\\nin David Salsburg’s The Lady Tasting Tea: How Statistics Revolutionized Science in\\nthe Twentieth Century  (W . H. Freeman, 2001).\\n•For a more detailed exposition, see the section in this book on the chi-square test\\n(“Chi-Square Test” on page 124 ).\\nChi-Square Distribution | 81', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 98}),\n",
       " Document(page_content='F-Distribution\\nA common procedure in scientific experimentation is to test multiple treatments\\nacross groups—say, different fertilizers on different blocks of a field.  This is similar to\\nthe A/B/C test referred to in the chi-square distribution (see “Chi-Square Distribu‐\\ntion” on page 80 ), except we are dealing with measured continuous values rather than\\ncounts. In this case we are interested in the extent to which differences among group\\nmeans are greater than we might expect under normal random variation. The F-\\nstatistic measures this and is the ratio of the variability among the group means to the\\nvariability within each group (also called residual variability). This comparison is\\ntermed an analysis of variance  (see “ ANOV A ” on page 118). The distribution of the F-\\nstatistic is the frequency distribution of all the values that would be produced by ran‐\\ndomly permuting data in which all the group means are equal (i.e., a null model).\\nThere are a variety of F-distributions associated with different degrees of freedom\\n(e.g., numbers of groups—see “Degrees of Freedom”  on page 116). The calculation of\\nF is illustrated in the section on ANOV A. The F-statistic is also used in linear regres‐\\nsion to compare the variation accounted for by the regression model to the overall\\nvariation in the data. F-statistics are produced automatically by R and Python  as part\\nof regression and ANOV A routines.\\nKey Ideas\\n•The F-distribution is used with experiments and linear models involving meas‐\\nured data.\\n•The F-statistic compares variation due to factors of interest to overall variation.\\nFurther Reading\\nGeorge Cobb’s Introduction to Design and Analysis of Experiments  (Wiley, 2008) con‐\\ntains an excellent exposition of the decomposition of variance components, which\\nhelps in understanding ANOV A and the F-statistic.\\nPoisson and Related Distributions\\nMany processes produce events randomly at a given overall rate—visitors arriving at\\na website, or cars arriving at a toll plaza (events spread over time); imperfections in a\\nsquare meter of fabric, or typos per 100 lines of code (events spread over space).\\n82 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 99}),\n",
       " Document(page_content='Key Terms for Poisson and Related Distributions\\nLambda\\nThe rate (per unit of time or space) at which events occur.\\nPoisson distribution\\nThe frequency distribution of the number of events in sampled units of time or\\nspace.\\nExponential distribution\\nThe frequency distribution of the time or distance from one event to the next\\nevent.\\nWeibull distribution\\nA generalized version of the exponential distribution in which the event rate is\\nallowed to shift over time.\\nPoisson Distributions\\nFrom prior aggregate data (for example, number of flu infections per year), we can\\nestimate the average number of events per unit of time or space (e.g., infections per\\nday, or per census unit). We might also want to know how different this might be\\nfrom one unit of time/space to another. The Poisson distribution tells us the distribu‐\\ntion of events per unit of time or space when we sample many such units. It is useful\\nwhen addressing queuing questions such as “How much capacity do we need to be\\n95% sure of fully processing the internet traffic that arrives on a server in any five-\\nsecond period?”\\nThe key parameter in a Poisson distribution is λ, or lambda. This is the mean number\\nof events that occurs in a specified interval of time or space. The variance for a Pois‐\\nson distribution is also λ.\\nA common technique is to generate random numbers from a Poisson distribution as\\npart of a queuing simulation. The rpois  function in R does this, taking only two\\narguments—the quantity of random numbers sought, and lambda:\\nrpois(100, lambda=2)\\nThe corresponding scipy  function is stats.poisson.rvs :\\nstats.poisson.rvs(2, size=100)\\nThis code will generate 100 random numbers from a Poisson distribution with λ = 2.\\nFor example, if incoming customer service calls average two per minute, this code\\nwill simulate 100 minutes, returning the number of calls in each of those 100 minutes.\\nPoisson and Related Distributions | 83', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 100}),\n",
       " Document(page_content='Exponential Distribution\\nUsing the same parameter λ that we used in the Poisson distribution, we can also\\nmodel the distribution of the time between events: time between visits to a website or\\nbetween cars arriving at a toll plaza. It is also used in engineering to model time to\\nfailure, and in process management to model, for example, the time required per ser‐\\nvice call. The R code to generate random numbers from an exponential distribution\\ntakes two arguments: n (the quantity of numbers to be generated) and rate  (the num‐\\nber of events per time period). For example:\\nrexp(n=100, rate=0.2)\\nIn the function stats.expon.rvs , the order of the arguments is reversed:\\nstats.expon.rvs(0.2, size=100)\\nThis code would generate 100 random numbers from an exponential distribution\\nwhere the mean number of events per time period is 0.2. So you could use it to simu‐\\nlate 100 intervals, in minutes, between service calls, where the average rate of incom‐\\ning calls is 0.2 per minute.\\nA key assumption in any simulation study for either the Poisson or exponential distri‐\\nbution is that the rate, λ, remains constant over the period being considered. This is\\nrarely reasonable in a global sense; for example, traffic on roads or data networks\\nvaries by time of day and day of week. However, the time periods, or areas of space,\\ncan usually be divided into segments that are sufficiently homogeneous so that analy‐\\nsis or simulation within those periods is valid.\\nEstimating the Failure Rate\\nIn many applications, the event rate, λ, is known or can be estimated from prior data.\\nHowever, for rare events, this is not necessarily so. Aircraft engine failure, for exam‐\\nple, is sufficiently rare (thankfully) that, for a given engine type, there may be little\\ndata on which to base an estimate of time between failures. With no data at all, there\\nis little basis on which to estimate an event rate. However, you can make some\\nguesses: if no events have been seen after 20 hours, you can be pretty sure that the\\nrate is not 1 per hour. Via simulation, or direct calculation of probabilities, you can\\nassess different hypothetical event rates and estimate threshold values below which\\nthe rate is very unlikely to fall. If there is some data but not enough to provide a\\nprecise,  reliable estimate of the rate, a goodness-of-fit test (see “Chi-Square Test”  on\\npage 124) can be applied to various rates to determine how well they fit the observed\\ndata.\\n84 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 101}),\n",
       " Document(page_content='Weibull Distribution\\nIn many cases, the event rate does not remain constant over time. If the period over\\nwhich it changes is much longer than the typical interval between events, there is no\\nproblem; you just subdivide the analysis into the segments where rates are relatively\\nconstant, as mentioned before. If, however, the event rate changes over the time of the\\ninterval, the exponential (or Poisson) distributions are no longer useful. This is likely\\nto be the case in mechanical failure—the risk of failure increases as time goes by. The\\nWeibull  distribution is an extension of the exponential distribution in which the event\\nrate is allowed to change, as specified by a shape parameter , β. If β > 1, the probability\\nof an event increases over time; if β < 1, the probability decreases. Because the Wei‐\\nbull distribution is used with time-to-failure analysis instead of event rate, the second\\nparameter is expressed in terms of characteristic life, rather than in terms of the rate\\nof events per interval. The symbol used is η, the Greek letter eta. It is also called the\\nscale  parameter.\\nWith the Weibull, the estimation task now includes estimation of both parameters, β\\nand η. Software is used to model the data and yield an estimate of the best-fitting\\nWeibull distribution.\\nThe R code to generate random numbers from a Weibull distribution takes three\\narguments: n (the quantity of numbers to be generated), shape , and scale . For exam‐\\nple, the following code would generate 100 random numbers (lifetimes) from a Wei‐\\nbull distribution with shape of 1.5 and characteristic life of 5,000:\\nrweibull (100, 1.5, 5000)\\nTo achieve the same in Python , use the function stats.weibull_min.rvs :\\nstats.weibull_min .rvs(1.5, scale=5000, size=100)\\nKey Ideas\\n•For events that occur at a constant rate, the number of events per unit of time or\\nspace can be modeled as a Poisson distribution.\\n•Y ou can also model the time or distance between one event and the next as an\\nexponential distribution.\\n•A changing event rate over time (e.g., an increasing probability of device failure)\\ncan be modeled with the Weibull distribution.\\nPoisson and Related Distributions | 85', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 102}),\n",
       " Document(page_content='Further Reading\\n•Modern Engineering Statistics  by Thomas Ryan (Wiley, 2007) has a chapter devo‐\\nted to the probability distributions used in engineering applications.\\n•Read an engineering-based perspective on the use of the Weibull distribution\\nhere  and here .\\nSummary\\nIn the era of big data, the principles of random sampling remain important when\\naccurate estimates are needed. Random selection of data can reduce bias and yield a\\nhigher quality data set than would result from just using the conveniently available\\ndata. Knowledge of various sampling and data-generating distributions allows us to\\nquantify potential errors in an estimate that might be due to random variation. At the\\nsame time, the bootstrap (sampling with replacement from an observed data set) is an\\nattractive “one size fits all” method to determine possible error in sample estimates.\\n86 | Chapter 2: Data and Sampling Distributions', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 103}),\n",
       " Document(page_content='CHAPTER 3\\nStatistical Experiments and\\nSignificance  Testing\\nDesign of experiments is a cornerstone of the practice of statistics, with applications\\nin virtually all areas of research. The goal is to design an experiment in order to con‐\\nfirm or reject a hypothesis. Data scientists often need to conduct continual experi‐\\nments, particularly regarding user interface and product marketing. This chapter\\nreviews traditional experimental design and discusses some common challenges in\\ndata science. It also covers some oft-cited concepts in statistical inference and\\nexplains their meaning and relevance (or lack of relevance) to data science.\\nWhenever you see references to statistical significance, t-tests, or p-values, it is typi‐\\ncally in the context of the classical statistical inference “pipeline” (see Figure 3-1 ).\\nThis process starts with a hypothesis (“drug A is better than the existing standard\\ndrug, ” or “price A is more profitable than the existing price B”). An experiment (it\\nmight be an A/B test) is designed to test the hypothesis—designed in such a way that\\nit hopefully will deliver conclusive results. The data is collected and analyzed, and\\nthen a conclusion is drawn. The term inference  reflects the intention to apply the\\nexperiment results, which involve a limited set of data, to a larger process or\\npopulation.\\nFigure 3-1. The classical statistical inference pipeline\\n87', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 104}),\n",
       " Document(page_content='A/B Testing\\nAn A/B test is an experiment with two groups to establish which of two treatments,\\nproducts, procedures, or the like is superior. Often one of the two treatments is the\\nstandard existing treatment, or no treatment. If a standard (or no) treatment is used,\\nit is called the control . A typical hypothesis is that a new treatment is better than the\\ncontrol.\\nKey Terms for A/B Testing\\nTreatment\\nSomething (drug, price, web headline) to which a subject is exposed.\\nTreatment group\\nA group of subjects exposed to a specific treatment.\\nControl group\\nA group of subjects exposed to no (or standard) treatment.\\nRandomization\\nThe process of randomly assigning subjects to treatments.\\nSubjects\\nThe items (web visitors, patients, etc.) that are exposed to treatments.\\nTest statistic\\nThe metric used to measure the effect of the treatment.\\nA/B tests are common in web design and marketing, since results are so readily meas‐\\nured. Some examples of A/B testing include:\\n•Testing two soil treatments to determine which produces better seed germination\\n•Testing two therapies to determine which suppresses cancer more effectively\\n•Testing two prices to determine which yields more net profit\\n•Testing two web headlines to determine which produces more clicks ( Figure 3-2 )\\n•Testing two web ads to determine which generates more conversions\\n88 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 105}),\n",
       " Document(page_content='Figure 3-2. Marketers continually test one web presentation against another\\nA proper A/B test has subjects  that can be assigned to one treatment or another. The\\nsubject might be a person, a plant seed, a web visitor; the key is that the subject is\\nexposed to the treatment. Ideally, subjects are randomized  (assigned randomly) to\\ntreatments. In this way, you know that any difference between the treatment groups is\\ndue to one of two things:\\n•The effect of the different treatments\\n•Luck of the draw in which subjects are assigned to which treatments (i.e., the ran‐\\ndom assignment may have resulted in the naturally better-performing subjects\\nbeing concentrated in A or B)\\nA/B Testing | 89', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 106}),\n",
       " Document(page_content='Y ou also need to pay attention to the test statistic  or metric you use to compare group\\nA to group B. Perhaps the most common metric in data science is a binary variable:\\nclick or no-click, buy or don’t buy, fraud or no fraud, and so on. Those results would\\nbe summed up in a 2×2 table. Table 3-1  is a 2×2 table for an actual price test (see\\n“Statistical Significance and p-Values” on page 103 for further discussion of these\\nresults).\\nTable 3-1. 2×2 table for ecommerce experiment results\\nOutcome Price A Price B\\nConversion 200 182\\nNo conversion 23,539 22,406\\nIf the metric is a continuous variable (purchase amount, profit, etc.) or a count (e.g.,\\ndays in hospital, pages visited), the result might be displayed differently. If one were\\ninterested not in conversion but in revenue per page view, the results of the price test\\nin Table 3-1  might look like this in typical default software output:\\nRevenue/page view with price A: mean = 3.87, SD = 51.10\\nRevenue/page view with price B: mean = 4.11, SD = 62.98\\n“SD” refers to the standard deviation of the values within each group.\\nJust because statistical software—including R and Python —gener‐\\nates output by default does not mean that all the output is useful or\\nrelevant. Y ou can see that the preceding standard deviations are not\\nthat useful; on their face they suggest that numerous values might\\nbe negative, when negative revenue is not feasible. This data con‐\\nsists of a small set of relatively high values (page views with conver‐\\nsions) and a huge number of 0-values (page views with no\\nconversion). It is difficult to sum up the variability of such data\\nwith a single number, though the mean absolute deviation from the\\nmean (7.68 for A and 8.15 for B) is more reasonable than the stan‐\\ndard deviation.\\nWhy Have a Control Group?\\nWhy not skip the control group and just run an experiment applying the treatment of\\ninterest to only one group, and compare the outcome to prior experience?\\nWithout a control group, there is no assurance that “all other things are equal” and\\nthat any difference is really due to the treatment (or to chance). When you have a\\ncontrol group, it is subject to the same conditions (except for the treatment of\\n90 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 107}),\n",
       " Document(page_content='interest ) as the treatment group. If you simply make a comparison to “baseline” or\\nprior experience, other factors, besides the treatment, might differ.\\nBlinding in studies\\nA blind study  is one in which the subjects are unaware of whether\\nthey are getting treatment A or treatment B.  Awareness of receiving\\na particular treatment can affect response. A double-blind  study is\\none in which the investigators and facilitators (e.g., doctors and\\nnurses in a medical study) also are unaware which subjects are get‐\\nting which treatment. Blinding is not possible when the nature of\\nthe treatment is transparent—for example, cognitive therapy from\\na computer versus a psychologist.\\nA/B testing in data science is typically used in a web context.  Treatments might be the\\ndesign of a web page, the price of a product, the wording of a headline, or some other\\nitem. Some thought is required to preserve the principles of randomization. Typically\\nthe subject in the experiment is the web visitor, and the outcomes we are interested in\\nmeasuring are clicks, purchases, visit duration, number of pages visited, whether a\\nparticular page is visited, and the like. In a standard A/B experiment, you need to\\ndecide on one metric ahead of time. Multiple behavior metrics might be collected and\\nbe of interest, but if the experiment is expected to lead to a decision between treat‐\\nment A and treatment B, a single metric, or test statistic , needs to be established\\nbeforehand. Selecting a test statistic after  the experiment is conducted opens the door\\nto researcher bias.\\nWhy Just A/B? Why Not C, D,…?\\nA/B tests are popular in the marketing and ecommerce worlds, but are far from the\\nonly type of statistical experiment. Additional treatments can be included. Subjects\\nmight have repeated measurements taken. Pharmaceutical trials where subjects are\\nscarce, expensive, and acquired over time are sometimes designed with multiple\\nopportunities to stop the experiment and reach a conclusion.\\nTraditional statistical experimental designs focus on answering a static question about\\nthe efficacy of specified treatments. Data scientists are less interested in the question:\\nIs the difference between price A and price B statistically significant?\\nthan in the question:\\nWhich, out of multiple possible prices, is best?\\nFor this, a relatively new type of experimental design is used: the multi-arm bandit\\n(see “Multi-Arm Bandit Algorithm” on page 131 ).\\nA/B Testing | 91', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 108}),\n",
       " Document(page_content='Getting Permission\\nIn scientific and medical research involving human subjects, it is\\ntypically necessary to get their permission, as well as obtain the\\napproval of an institutional review board. Experiments in business\\nthat are done as a part of ongoing operations almost never do this.\\nIn most cases (e.g., pricing experiments, or experiments about\\nwhich headline to show or which offer should be made), this prac‐\\ntice is widely accepted. Facebook, however, ran afoul of this general\\nacceptance in 2014 when it experimented with the emotional tone\\nin users’ newsfeeds. Facebook used sentiment analysis to classify\\nnewsfeed posts as positive or negative, and then altered the posi‐\\ntive/negative balance in what it showed users. Some randomly\\nselected users experienced more positive posts, while others experi‐\\nenced more negative posts. Facebook found that the users who\\nexperienced a more positive newsfeed were more likely to post pos‐\\nitively themselves, and vice versa. The magnitude of the effect was\\nsmall, however, and Facebook faced much criticism for conducting\\nthe experiment without users’ knowledge. Some users speculated\\nthat Facebook might have pushed some extremely depressed users\\nover the edge if they got the negative version of their feed.\\nKey Ideas\\n•Subjects are assigned to two (or more) groups that are treated exactly alike,\\nexcept that the treatment under study differs from one group to another.\\n•Ideally, subjects are assigned randomly to the groups.\\nFurther Reading\\n•Two-group comparisons (A/B tests) are a staple of traditional statistics, and just\\nabout any introductory statistics text will have extensive coverage of design prin‐\\nciples and inference procedures. For a discussion that places A/B tests in more of\\na data science context and uses resampling, see Introductory Statistics and Analyt‐\\nics: A Resampling Perspective  by Peter Bruce (Wiley, 2014).\\n•For web testing, the logistical aspects of testing can be just as challenging as the\\nstatistical ones. A good place to start is the Google Analytics help section on\\nexperiments .\\n•Beware advice found in the ubiquitous guides to A/B testing that you see on the\\nweb, such as these words in one such guide: “Wait for about 1,000 total visitors\\nand make sure you run the test for a week. ” Such general rules of thumb are not\\n92 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 109}),\n",
       " Document(page_content='statistically meaningful; see “Power and Sample Size” on page 135 for more\\ndetail.\\nHypothesis Tests\\nHypothesis tests, also called significance  tests , are ubiquitous in the traditional statisti‐\\ncal analysis of published research. Their purpose is to help you learn whether random\\nchance might be responsible for an observed effect.\\nKey Terms for Hypothesis Tests\\nNull hypothesis\\nThe hypothesis that chance is to blame.\\nAlternative hypothesis\\nCounterpoint to the null (what you hope to prove).\\nOne-way test\\nHypothesis test that counts chance results only in one direction.\\nTwo-way test\\nHypothesis test that counts chance results in two directions.\\nAn A/B test (see “ A/B Testing” on page 88) is typically constructed with a hypothesis\\nin mind. For example, the hypothesis might be that price B produces higher profit.\\nWhy do we need a hypothesis? Why not just look at the outcome of the experiment\\nand go with whichever treatment does better?\\nThe answer lies in the tendency of the human mind to underestimate the scope of\\nnatural random behavior. One manifestation of this is the failure to anticipate\\nextreme events, or so-called “black swans” (see “Long-Tailed Distributions” on page\\n73). Another manifestation is the tendency to misinterpret random events as having\\npatterns of some significance. Statistical hypothesis testing was invented as a way to\\nprotect researchers from being fooled by random chance.\\nMisinterpreting Randomness\\nY ou can observe the human tendency to underestimate randomness in this experi‐\\nment. Ask several friends to invent a series of 50 coin flips: have them write down a\\nseries of random Hs and Ts. Then ask them to actually flip a coin 50 times and write\\ndown the results. Have them put the real coin flip results in one pile, and the made-\\nup results in another. It is easy to tell which results are real: the real ones will have\\nlonger runs of Hs or Ts. In a set of 50 real coin flips, it is not at all unusual to see five\\nHypothesis Tests | 93', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 110}),\n",
       " Document(page_content='or six Hs or Ts in a row. However, when most of us are inventing random coin flips\\nand we have gotten three or four Hs in a row, we tell ourselves that, for the series to\\nlook random, we had better switch to T.\\nThe other side of this coin, so to speak, is that when we do see the real-world equiva‐\\nlent of six Hs in a row (e.g., when one headline outperforms another by 10%), we are\\ninclined to attribute it to something real, not just to chance.\\nIn a properly designed A/B test, you collect data on treatments A and B in such a way\\nthat any observed difference between A and B must be due to either:\\n•Random chance in assignment of subjects\\n•A true difference between A and B\\nA statistical hypothesis test is further analysis of an A/B test, or any randomized\\nexperiment, to assess whether random chance is a reasonable explanation for the\\nobserved difference between groups A and B.\\nThe Null Hypothesis\\nHypothesis tests use the following logic: “Given the human tendency to react to\\nunusual but random behavior and interpret it as something meaningful and real, in\\nour experiments we will require proof that the difference between groups is more\\nextreme than what chance might reasonably produce. ” This involves a baseline\\nassumption that the treatments are equivalent, and any difference between the groups\\nis due to chance. This baseline assumption is termed the null hypothesis . Our hope,\\nthen, is that we can in fact prove the null hypothesis wrong  and show that the out‐\\ncomes for groups A and B are more different than what chance might produce.\\nOne way to do this is via a resampling permutation procedure, in which we shuffle\\ntogether the results from groups A and B and then repeatedly deal out the data in\\ngroups of similar sizes, and then observe how often we get a difference as extreme as\\nthe observed difference. The combined shuffled results from groups A and B, and the\\nprocedure of resampling from them, embodies the null hypothesis of groups A and B\\nbeing equivalent and interchangeable and is termed the null model. See “Resampling”\\non page 96  for more detail.\\n94 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 111}),\n",
       " Document(page_content='Alternative Hypothesis\\nHypothesis tests by their nature involve not just a null hypothesis but also an offset‐\\nting alternative hypothesis. Here are some examples:\\n•Null = “no difference between the means of group A and group B”; alternative =\\n“ A is different from B” (could be bigger or smaller)\\n•Null = “ A ≤ B”; alternative = “ A > B”\\n•Null = “B is not X% greater than A ”; alternative = “B is X% greater than A ”\\nTaken together, the null and alternative hypotheses must account for all possibilities.\\nThe nature of the null hypothesis determines the structure of the hypothesis test.\\nOne-Way Versus Two-Way Hypothesis Tests\\nOften in an A/B test, you are testing a new option (say, B) against an established\\ndefault option (A), and the presumption is that you will stick with the default option\\nunless the new option proves itself definitively better. In such a case, you want a\\nhypothesis test to protect you from being fooled by chance in the direction favoring\\nB. Y ou don’t care about being fooled by chance in the other direction, because you\\nwould be sticking with A unless B proves definitively better. So you want a directional\\nalternative hypothesis (B is better than A). In such a case, you use a one-way  (or one-\\ntail) hypothesis test. This means that extreme chance results in only one direction\\ncount toward the p-value.\\nIf you want a hypothesis test to protect you from being fooled by chance in either\\ndirection, the alternative hypothesis is bidirectional  (A is different from B; could be\\nbigger or smaller). In such a case, you use a two-way  (or two-tail) hypothesis.  This\\nmeans that extreme chance results in either direction count toward the p-value.\\nA one-tail hypothesis test often fits the nature of A/B decision making, in which a\\ndecision is required and one option is typically assigned “default” status unless the\\nother proves better. Software, however, including R and scipy  in Python , typically\\nprovides a two-tail test in its default output, and many statisticians opt for the more\\nconservative two-tail test just to avoid argument. One-tail versus two-tail is a confus‐\\ning subject, and not that relevant for data science, where the precision of p-value cal‐\\nculations is not terribly important.\\nHypothesis Tests | 95', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 112}),\n",
       " Document(page_content='Key Ideas\\n•A null hypothesis is a logical construct embodying the notion that nothing spe‐\\ncial has happened, and any effect you observe is due to random chance.\\n•The hypothesis test assumes that the null hypothesis is true, creates a “null\\nmodel” (a probability model), and tests whether the effect you observe is a rea‐\\nsonable outcome of that model.\\nFurther Reading\\n•The Drunkard’s Walk  by Leonard Mlodinow (Pantheon, 2008) is a readable sur‐\\nvey of the ways in which “randomness rules our lives. ”\\n•David Freedman, Robert Pisani, and Roger Purves’s classic statistics text Statis‐\\ntics, 4th ed. (W . W . Norton, 2007), has excellent nonmathematical treatments of\\nmost statistics topics, including hypothesis testing.\\n•Introductory Statistics and Analytics: A Resampling Perspective  by Peter Bruce\\n(Wiley, 2014) develops hypothesis testing concepts using resampling.\\nResampling\\nResampling  in statistics means to repeatedly sample values from observed data, with a\\ngeneral goal of assessing random variability in a statistic. It can also  be used to assess\\nand improve the accuracy of some machine-learning models (e.g., the predictions\\nfrom decision tree models built on multiple bootstrapped data sets can be averaged in\\na process known as bagging —see “Bagging and the Random Forest” on page 259 ).\\nThere are two main types of resampling procedures: the bootstrap  and permutation\\ntests.  The bootstrap is used to assess the reliability of an estimate; it was discussed in\\nthe previous chapter (see “The Bootstrap” on page 61). Permutation tests are used to\\ntest hypotheses, typically involving two or more groups, and we discuss those in this\\nsection.\\n96 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 113}),\n",
       " Document(page_content='Key Terms for Resampling\\nPermutation test\\nThe procedure of combining two or more samples together and randomly (or\\nexhaustively) reallocating the observations to resamples.\\nSynonyms\\nRandomization test, random permutation test, exact test\\nResampling\\nDrawing additional samples (“resamples”) from an observed data set.\\nWith or without replacement\\nIn sampling, whether or not an item is returned to the sample before the next\\ndraw.\\nPermutation Test\\nIn a permutation  procedure, two or more samples are involved, typically the groups\\nin an A/B or other hypothesis test.  Permute  means to change the order of a set of val‐\\nues. The first step in a permutation test  of a hypothesis is to combine the results from\\ngroups A and B (and, if used, C, D,…). This is the logical embodiment of the null\\nhypothesis that the treatments to which the groups were exposed do not differ. We\\nthen test that hypothesis by randomly drawing groups from this combined set and\\nseeing how much they differ from one another. The permutation procedure is as\\nfollows:\\n1.Combine the results from the different groups into a single data set.\\n2.Shuffle the combined data and then randomly draw (without replacement) a\\nresample of the same size as group A (clearly it will contain some data from the\\nother groups).\\n3.From the remaining data, randomly draw (without replacement) a resample of\\nthe same size as group B.\\n4.Do the same for groups C, D, and so on. Y ou have now collected one set of\\nresamples that mirror the sizes of the original samples.\\n5.Whatever statistic or estimate was calculated for the original samples (e.g., differ‐\\nence in group proportions), calculate it now for the resamples, and record; this\\nconstitutes one permutation iteration.\\n6.Repeat the previous steps R times to yield a permutation distribution of the test\\nstatistic.\\nResampling | 97', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 114}),\n",
       " Document(page_content='Now go back to the observed difference between groups and compare it to the set of\\npermuted differences. If the observed difference lies well within the set of permuted\\ndifferences, then we have not proven anything—the observed difference is within the\\nrange of what chance might produce. However, if the observed difference  lies outside\\nmost of the permutation distribution, then we conclude that chance is not responsi‐\\nble. In technical terms, the difference is statistically significant . (See “Statistical Signif‐\\nicance and p-Values” on page 103 .)\\nExample: Web Stickiness\\nA company selling a relatively high-value service wants to test which of two web pre‐\\nsentations does a better selling job. Due to the high value of the service being sold,\\nsales are infrequent and the sales cycle is lengthy; it would take too long to accumu‐\\nlate enough sales to know which presentation is superior. So the company decides to\\nmeasure the results with a proxy variable, using the detailed interior page that\\ndescribes the service.\\nA proxy  variable is one that stands in for the true variable of inter‐\\nest, which may be unavailable, too costly, or too time-consuming to\\nmeasure. In climate research, for example, the oxygen content of\\nancient ice cores is used as a proxy for temperature. It is useful to\\nhave at least some  data on the true variable of interest, so the\\nstrength of its association with the proxy can be assessed.\\nOne potential proxy variable for our company is the number of clicks on the detailed\\nlanding page. A better one is how long people spend on the page. It is reasonable to\\nthink that a web presentation (page) that holds people’s attention longer will lead to\\nmore sales. Hence, our metric is average session time, comparing page A to page B.\\nDue to the fact that this is an interior, special-purpose page, it does not receive a huge\\nnumber of visitors. Also note that Google Analytics, which is how we measure session\\ntime, cannot measure session time for the last session a person visits.  Instead of delet‐\\ning that session from the data, though, Google Analytics records it as a zero, so the\\ndata requires additional processing to remove those sessions. The result is a total of\\n36 sessions for the two different presentations, 21 for page A and 15 for page B. Using\\nggplot , we can visually compare the session times using side-by-side boxplots:\\nggplot(session_times , aes(x=Page, y=Time)) +\\n  geom_boxplot ()\\n98 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 115}),\n",
       " Document(page_content=\"The pandas  boxplot  command uses the keyword argument by to create the figure:\\nax = session_times .boxplot(by='Page', column='Time')\\nax.set_xlabel ('')\\nax.set_ylabel ('Time (in seconds)' )\\nplt.suptitle ('')\\nThe boxplot, shown in Figure 3-3 , indicates that page B leads to longer sessions than\\npage A. The means for each group can be computed in R as follows:\\nmean_a <- mean(session_times [session_times ['Page'] == 'Page A' , 'Time'])\\nmean_b <- mean(session_times [session_times ['Page'] == 'Page B' , 'Time'])\\nmean_b - mean_a\\n[1] 35.66667\\nIn Python , we filter the pandas  data frame first by page and then determine the mean\\nof the Time  column:\\nmean_a = session_times [session_times .Page == 'Page A' ].Time.mean()\\nmean_b = session_times [session_times .Page == 'Page B' ].Time.mean()\\nmean_b - mean_a\\nPage B has session times that are greater than those of page A by 35.67 seconds, on\\naverage. The question is whether this difference is within the range of what random\\nchance might produce, i.e., is statistically significant. One way to answer this is to\\napply a permutation test—combine all the session times together and then repeatedly\\nshuffle and divide them into groups of 21 (recall that nA= 21  for page A) and 15\\n(nB= 15  for page B).\\nTo apply a permutation test, we need a function to randomly assign the 36 session\\ntimes to a group of 21 (page A) and a group of 15 (page B). The R version of this\\nfunction is:\\nperm_fun  <- function (x, nA, nB)\\n{\\n  n <- nA + nB\\n  idx_b <- sample(1:n, nB)\\n  idx_a <- setdiff(1:n, idx_b)\\n  mean_diff  <- mean(x[idx_b]) - mean(x[idx_a])\\n  return(mean_diff )\\n}\\nThe Python  version of this permutation test is the following:\\ndef perm_fun (x, nA, nB):\\n    n = nA + nB\\n    idx_B = set(random.sample(range(n), nB))\\n    idx_A = set(range(n)) - idx_B\\n    return x.loc[idx_B].mean() - x.loc[idx_A].mean()\\nResampling | 99\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 116}),\n",
       " Document(page_content=\"Figure 3-3. Session times for web pages A and B\\nThis function works by sampling (without replacement) nB indices and assigning\\nthem to the B group; the remaining nA indices are assigned to group A. The differ‐\\nence between the two means is returned. Calling this function R = 1,000 times and\\nspecifying nA= 21  and nB= 15  leads to a distribution of differences in the session\\ntimes that can be plotted as a histogram. In R this is done as follows using the hist\\nfunction:\\nperm_diffs  <- rep(0, 1000)\\nfor (i in 1:1000) {\\n  perm_diffs [i] = perm_fun (session_times [, 'Time'], 21, 15)\\n}\\nhist(perm_diffs , xlab='Session time differences (in seconds)' )\\nabline(v=mean_b - mean_a)\\nIn Python , we can create a similar graph using matplotlib :\\nperm_diffs  = [perm_fun (session_times .Time, nA, nB) for _ in range(1000)]\\nfig, ax = plt.subplots (figsize=(5, 5))\\nax.hist(perm_diffs , bins=11, rwidth=0.9)\\nax.axvline(x = mean_b - mean_a, color='black', lw=2)\\n100 | Chapter 3: Statistical Experiments and Significance  Testing\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 117}),\n",
       " Document(page_content=\"ax.text(50, 190, 'Observed \\\\ndifference' , bbox={'facecolor' :'white'})\\nax.set_xlabel ('Session time differences (in seconds)' )\\nax.set_ylabel ('Frequency' )\\nThe histogram, in Figure 3-4  shows that mean difference of random permutations\\noften exceeds the observed difference in session times (the vertical line). For our\\nresults, this happens in 12.6% of the cases:\\nmean(perm_diffs  > (mean_b - mean_a))\\n---\\n0.126\\nAs the simulation uses random numbers, the percentage will vary. For example, in the\\nPython  version, we got 12.1%:\\nnp.mean(perm_diffs  > mean_b - mean_a)\\n---\\n0.121\\nThis suggests that the observed difference in session time between page A and page B\\nis well within the range of chance variation and thus is not statistically significant.\\nFigure 3-4. Frequency distribution for session time differences  between pages A and B;\\nthe vertical line shows the observed difference\\nResampling | 101\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 118}),\n",
       " Document(page_content='Exhaustive and Bootstrap Permutation Tests\\nIn addition to the preceding random shuffling procedure, also called a random per‐\\nmutation test  or a randomization test , there are two variants of the permutation test:\\n•An exhaustive permutation test\\n•A bootstrap permutation test\\nIn an exhaustive permutation test, instead of just randomly shuffling and dividing the\\ndata, we actually figure out all the possible ways it could be divided. This is practical\\nonly for relatively small sample sizes. With a large number of repeated shufflings, the\\nrandom permutation test results approximate those of the exhaustive permutation\\ntest, and approach them in the limit.  Exhaustive permutation tests are also sometimes\\ncalled exact tests , due to their statistical property of guaranteeing that the null model\\nwill not test as “significant” more than the alpha level of the test (see “Statistical Sig‐\\nnificance and p-Values” on page 103 ).\\nIn a bootstrap permutation test, the draws outlined in steps 2 and 3 of the random\\npermutation test are made with replacement  instead of without replacement.  In this\\nway the resampling procedure models not just the random element in the assignment\\nof treatment to subject but also the random element in the selection of subjects from\\na population. Both procedures are encountered in statistics, and the distinction\\nbetween them is somewhat convoluted and not of consequence in the practice of data\\nscience.\\nPermutation Tests: The Bottom Line for Data Science\\nPermutation tests are useful heuristic procedures for exploring the role of random\\nvariation. They are relatively easy to code, interpret, and explain, and they offer a use‐\\nful detour around the formalism and “false determinism” of formula-based statistics,\\nin which the precision of formula “answers” tends to imply unwarranted certainty.\\nOne virtue of resampling, in contrast to formula approaches, is that it comes much\\ncloser to a one-size-fits-all approach to inference. Data can be numeric or binary.\\nSample sizes can be the same or different. Assumptions about normally distributed\\ndata are not needed.\\nKey Ideas\\n•In a permutation test, multiple samples are combined and then shuffled.\\n•The shuffled values are then divided into resamples, and the statistic of interest is\\ncalculated.\\n102 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 119}),\n",
       " Document(page_content='•This process is then repeated, and the resampled statistic is tabulated.\\n•Comparing the observed value of the statistic to the resampled distribution\\nallows you to judge whether an observed difference between samples might occur\\nby chance.\\nFurther Reading\\n•Randomization Tests , 4th ed., by Eugene Edgington and Patrick Onghena (Chap‐\\nman & Hall/CRC Press, 2007)—but don’t get too drawn into the thicket of\\nnonrandom sampling\\n•Introductory Statistics and Analytics: A Resampling Perspective  by Peter Bruce\\n(Wiley, 2014)\\nStatistical Significance  and p-Values\\nStatistical significance is how statisticians measure whether an experiment (or even a\\nstudy of existing data) yields a result more extreme than what chance might produce.\\nIf the result is beyond the realm of chance variation, it is said to be statistically\\nsignificant.\\nKey Terms for Statistical Significance  and p-Values\\np-value\\nGiven a chance model that embodies the null hypothesis, the p-value is the prob‐\\nability of obtaining results as unusual or extreme as the observed results.\\nAlpha\\nThe probability threshold of “unusualness” that chance results must surpass for\\nactual outcomes to be deemed statistically significant.\\nType 1 error\\nMistakenly concluding an effect is real (when it is due to chance).\\nType 2 error\\nMistakenly concluding an effect is due to chance (when it is real).\\nStatistical Significance  and p-Values | 103', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 120}),\n",
       " Document(page_content='Consider in Table 3-2  the results of the web test shown earlier.\\nTable 3-2. 2×2 table for ecommerce experiment results\\nOutcome Price A Price B\\nConversion 200 182\\nNo conversion 23,539 22,406\\nPrice A converts almost 5% better than price B (0.8425% = 200/(23539+200)*100,\\nversus 0.8057% = 182/(22406+182)*100—a difference of 0.0368 percentage points),\\nbig enough to be meaningful in a high-volume business. We have over 45,000 data\\npoints here, and it is tempting to consider this as “big data, ” not requiring tests of\\nstatistical significance (needed mainly to account for sampling variability in small\\nsamples). However, the conversion rates are so low (less than 1%) that the actual\\nmeaningful values—the conversions—are only in the 100s, and the sample size\\nneeded is really determined by these conversions. We can test whether the difference\\nin conversions between prices A and B is within the range of chance variation , using a\\nresampling procedure. By chance variation, we mean the random variation produced\\nby a probability model that embodies the null hypothesis that there is no difference\\nbetween the rates (see “The Null Hypothesis” on page 94 ). The following permutation\\nprocedure asks, “If the two prices share the same conversion rate, could chance varia‐\\ntion produce a difference as big as 5%?”\\n1.Put cards labeled 1 and 0 in a box: this represents the supposed shared conver‐\\nsion rate of 382 ones and 45,945 zeros = 0.008246 = 0.8246%.\\n2.Shuffle and draw out a resample of size 23,739 (same n as price A), and record\\nhow many 1s.\\n3.Record the number of 1s in the remaining 22,588 (same n as price B).\\n4.Record the difference in proportion of 1s.\\n5.Repeat steps 2–4.\\n6.How often was the difference >= 0.0368?\\n104 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 121}),\n",
       " Document(page_content=\"Reusing the function perm_fun  defined in “Example: Web Stickiness”  on page 98, we\\ncan create a histogram of randomly permuted differences in conversion rate in R:\\nobs_pct_diff  <- 100 * (200 / 23739 - 182 / 22588)\\nconversion  <- c(rep(0, 45945), rep(1, 382))\\nperm_diffs  <- rep(0, 1000)\\nfor (i in 1:1000) {\\n  perm_diffs [i] = 100 * perm_fun (conversion , 23739, 22588)\\n}\\nhist(perm_diffs , xlab='Conversion rate (percent)' , main='')\\nabline(v=obs_pct_diff )\\nThe corresponding Python  code is:\\nobs_pct_diff  = 100 * (200 / 23739 - 182 / 22588)\\nprint(f'Observed difference: {obs_pct_diff:.4f}%' )\\nconversion  = [0] * 45945\\nconversion .extend([1] * 382)\\nconversion  = pd.Series(conversion )\\nperm_diffs  = [100 * perm_fun (conversion , 23739, 22588)\\n              for _ in range(1000)]\\nfig, ax = plt.subplots (figsize=(5, 5))\\nax.hist(perm_diffs , bins=11, rwidth=0.9)\\nax.axvline(x=obs_pct_diff , color='black', lw=2)\\nax.text(0.06, 200, 'Observed \\\\ndifference' , bbox={'facecolor' :'white'})\\nax.set_xlabel ('Conversion rate (percent)' )\\nax.set_ylabel ('Frequency' )\\nSee the histogram of 1,000 resampled results in Figure 3-5 : as it happens, in this case\\nthe observed difference of 0.0368% is well within the range of chance variation.\\nStatistical Significance  and p-Values | 105\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 122}),\n",
       " Document(page_content='Figure 3-5. Frequency distribution for the difference  in conversion rates between prices A\\nand B\\np-Value\\nSimply looking at the graph is not a very precise way to measure statistical signifi‐\\ncance, so of more interest is the p-value . This is the frequency with which the chance\\nmodel produces a result more extreme than the observed result. We can estimate a p-\\nvalue from our permutation test by taking the proportion of times that the permuta‐\\ntion test produces a difference equal to or greater than the observed difference:\\nmean(perm_diffs  > obs_pct_diff )\\n[1] 0.308\\nnp.mean([diff > obs_pct_diff  for diff in perm_diffs ])\\nHere, both R and Python  use the fact that true is interpreted as 1 and false as 0.\\nThe p-value is 0.308, which means that we would expect to achieve a result as\\nextreme as this, or a more extreme result, by random chance over 30% of the time.\\n106 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 123}),\n",
       " Document(page_content=\"In this case, we didn’t need to use a permutation test to get a p-value. Since we have a\\nbinomial distribution, we can approximate the p-value. In R code, we do this using\\nthe function prop.test :\\n> prop.test (x=c(200, 182), n=c(23739, 22588), alternative ='greater' )\\n 2-sample test for equality  of proportions  with continuity  correction\\ndata:  c(200, 182) out of c(23739, 22588)\\nX-squared = 0.14893, df = 1, p-value = 0.3498\\nalternative  hypothesis : greater\\n95 percent confidence  interval :\\n -0.001057439   1.000000000\\nsample estimates :\\n     prop 1      prop 2\\n0.008424955  0.008057376\\nThe argument x is the number of successes for each group, and the argument n is the\\nnumber of trials.\\nThe method scipy.stats.chi2_contingency  takes the values as shown in Table 3-2 :\\nsurvivors  = np.array([[200, 23739 - 200], [182, 22588 - 182]])\\nchi2, p_value, df, _ = stats.chi2_contingency (survivors )\\nprint(f'p-value for single sided test: {p_value / 2:.4f}' )\\nThe normal approximation yields a p-value of 0.3498, which is close to the p-value\\nobtained from the permutation test.\\nAlpha\\nStatisticians frown on the practice of leaving it to the researcher’s discretion to deter‐\\nmine whether a result is “too unusual” to happen by chance. Rather, a threshold is\\nspecified in advance, as in “more extreme than 5% of the chance (null hypothesis)\\nresults”; this threshold is known as alpha . Typical alpha levels are 5% and 1%. Any\\nchosen level is an arbitrary decision—there is nothing about the process that will\\nguarantee correct decisions x% of the time. This is because the probability question\\nbeing answered is not “What is the probability that this happened by chance?” but\\nrather “Given a chance model, what is the probability of a result this extreme?” We\\nthen deduce backward about the appropriateness of the chance model, but that judg‐\\nment does not carry a probability. This point has been the subject of much confusion.\\np-value controversy\\nConsiderable controversy has surrounded the use of the p-value in recent years.  One\\npsychology journal has gone so far as to “ban” the use of p-values in submitted papers\\non the grounds that publication decisions based solely on the p-value were resulting\\nin the publication of poor research. Too many researchers, only dimly aware of what a\\nStatistical Significance  and p-Values | 107\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 124}),\n",
       " Document(page_content='p-value really means, root around in the data, and among different possible hypothe‐\\nses to test, until they find a combination that yields a significant p-value and, hence, a\\npaper suitable for publication.\\nThe real problem is that people want more meaning from the p-value than it con‐\\ntains. Here’s what we would like the p-value to convey:\\nThe probability that the result is due to chance.\\nWe hope for a low value, so we can conclude that we’ve proved something. This is\\nhow many journal editors were interpreting the p-value. But here’s what the p-value\\nactually  represents:\\nThe probability that, given a chance model , results as extreme as the observed results\\ncould occur.\\nThe difference is subtle but real. A significant p-value does not carry you quite as far\\nalong the road to “proof ” as it seems to promise. The logical foundation for the con‐\\nclusion “statistically significant” is somewhat weaker when the real meaning of the p-\\nvalue is understood.\\nIn March 2016, the American Statistical Association, after much internal deliberation,\\nrevealed the extent of misunderstanding about p-values when it issued a cautionary\\nstatement regarding their use. The ASA statement  stressed six principles for research‐\\ners and journal editors:\\n1.P-values can indicate how incompatible the data are with a specified statistical\\nmodel.\\n2.P-values do not measure the probability that the studied hypothesis is true, or the\\nprobability that the data were produced by random chance alone.\\n3.Scientific conclusions and business or policy decisions should not be based only\\non whether a p-value passes a specific threshold.\\n4.Proper inference requires full reporting and transparency.\\n5.A p-value, or statistical significance, does not measure the size of an effect or the\\nimportance of a result.\\n6.By itself, a p-value does not provide a good measure of evidence regarding a\\nmodel or hypothesis.\\n108 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 125}),\n",
       " Document(page_content='Practical significance\\nEven if a result is statistically significant, that does not mean it has practical signifi‐\\ncance. A small difference that has no practical meaning can be statistically significant\\nif it arose from large enough samples. Large samples ensure that small, non-\\nmeaningful effects can nonetheless be big enough to rule out chance as an explana‐\\ntion. Ruling out chance does not magically render important a result that is, in its\\nessence, unimportant.\\nType 1 and Type 2 Errors\\nIn assessing statistical significance, two types of error are possible:\\n•A Type 1 error, in which you mistakenly conclude an effect is real, when it is\\nreally just due to chance\\n•A Type 2 error, in which you mistakenly conclude that an effect is not real (i.e.,\\ndue to chance), when it actually is real\\nActually, a Type 2 error is not so much an error as a judgment that the sample size is\\ntoo small to detect the effect. When a p-value falls short of statistical significance\\n(e.g., it exceeds 5%), what we are really saying is “effect not proven. ” It could be that a\\nlarger sample would yield a smaller p-value.\\nThe basic function of significance tests (also called hypothesis tests ) is to protect\\nagainst being fooled by random chance; thus they are typically structured to mini‐\\nmize Type 1 errors.\\nData Science and p-Values\\nThe work that data scientists do is typically not destined for publication in scientific\\njournals, so the debate over the value of a p-value is somewhat academic.  For a data\\nscientist, a p-value is a useful metric in situations where you want to know whether a\\nmodel result that appears interesting and useful is within the range of normal chance\\nvariability. As a decision tool in an experiment, a p-value should not be considered\\ncontrolling, but merely another point of information bearing on a decision. For\\nexample, p-values are sometimes used as intermediate inputs in some statistical or\\nmachine learning models—a feature might be included in or excluded from a model\\ndepending on its p-value.\\nStatistical Significance  and p-Values | 109', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 126}),\n",
       " Document(page_content='Key Ideas\\n•Significance tests are used to determine whether an observed effect is within the\\nrange of chance variation for a null hypothesis model.\\n•The p-value is the probability that results as extreme as the observed results\\nmight occur, given a null hypothesis model.\\n•The alpha value is the threshold of “unusualness” in a null hypothesis chance\\nmodel.\\n•Significance testing has been much more relevant for formal reporting of\\nresearch than for data science (but has been fading recently, even for the former).\\nFurther Reading\\n•Stephen Stigler, “Fisher and the 5% Level, ” Chance  21, no. 4 (2008): 12. This arti‐\\ncle is a short commentary on Ronald Fisher’s 1925 book Statistical Methods for\\nResearch Workers  (Oliver & Boyd), and on Fisher’s emphasis on the 5% level of\\nsignificance.\\n•See also “Hypothesis Tests” on page 93  and the further reading mentioned there.\\nt-Tests\\nThere are numerous types of significance tests, depending on whether the data com‐\\nprises count data or measured data, how many samples there are, and what’s being\\nmeasured.  A very common one is the t-test , named after Student’s t-distribution,\\noriginally developed by W . S. Gosset to approximate the distribution of a single sam‐\\nple mean (see “Student’s t-Distribution” on page 75 ).\\nKey Terms for t-Tests\\nTest statistic\\nA metric for the difference or effect of interest.\\nt-statistic\\nA standardized version of common test statistics such as means.\\nt-distribution\\nA reference distribution (in this case derived from the null hypothesis), to which\\nthe observed t-statistic can be compared.\\n110 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 127}),\n",
       " Document(page_content=\"All significance tests require that you specify a test statistic  to measure the effect you\\nare interested in and help you determine whether that observed effect lies within the\\nrange of normal chance variation. In a resampling test (see the discussion of permu‐\\ntation in “Permutation Test” on page 97), the scale of the data does not matter. Y ou\\ncreate the reference (null hypothesis) distribution from the data itself and use the test\\nstatistic as is.\\nIn the 1920s and 1930s, when statistical hypothesis testing was being developed, it\\nwas not feasible to randomly shuffle data thousands of times to do a resampling test.\\nStatisticians found that a good approximation to the permutation (shuffled) distribu‐\\ntion was the t-test, based on Gosset’s t-distribution. It is used for the very common\\ntwo-sample comparison—A/B test—in which the data is numeric. But in order for\\nthe t-distribution to be used without regard to scale, a standardized form of the test\\nstatistic must be used.\\nA classic statistics text would at this stage show various formulas that incorporate\\nGosset’s distribution and demonstrate how to standardize your data to compare it to\\nthe standard t-distribution. These formulas are not shown here because all statistical\\nsoftware, as well as R and Python , includes commands that embody the formula. In R,\\nthe function is t.test :\\n> t.test(Time ~ Page, data=session_times , alternative ='less')\\n Welch Two Sample t-test\\ndata:  Time by Page\\nt = -1.0983, df = 27.693, p-value = 0.1408\\nalternative  hypothesis : true difference  in means is less than 0\\n95 percent confidence  interval :\\n     -Inf 19.59674\\nsample estimates :\\nmean in group Page A mean in group Page B\\n            126.3333              162.0000\\nThe function scipy.stats.ttest_ind  can be used in Python :\\nres = stats.ttest_ind (session_times [session_times .Page == 'Page A' ].Time,\\n                      session_times [session_times .Page == 'Page B' ].Time,\\n                      equal_var =False)\\nprint(f'p-value for single sided test: {res.pvalue / 2:.4f}' )\\nThe alternative hypothesis is that the session time mean for page A is less than that\\nfor page B. The p-value of 0.1408 is fairly close to the permutation test p-values of\\n0.121 and 0.126 (see “Example: Web Stickiness” on page 98 ).\\nIn a resampling mode, we structure the solution to reflect the observed data and the\\nhypothesis to be tested, not worrying about whether the data is numeric or binary,\\nwhether or not sample sizes are balanced, sample variances, or a variety of other fac‐\\ntors. In the formula world, many variations present themselves, and they can be\\nt-Tests | 111\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 128}),\n",
       " Document(page_content='bewildering. Statisticians need to navigate that world and learn its map, but data sci‐\\nentists do not—they are typically not in the business of sweating the details of\\nhypothesis tests and confidence intervals the way a researcher preparing a paper for\\npresentation might.\\nKey Ideas\\n•Before the advent of computers, resampling tests were not practical, and statisti‐\\ncians used standard reference distributions.\\n•A test statistic could then be standardized and compared to the reference\\ndistribution.\\n•One such widely used standardized statistic is the t-statistic.\\nFurther Reading\\n•Any introductory statistics text will have illustrations of the t-statistic and its\\nuses; two good ones are Statistics , 4th ed., by David Freedman, Robert Pisani, and\\nRoger Purves (W . W . Norton, 2007), and The Basic Practice of Statistics , 8th ed.,\\nby David S. Moore, William I. Notz, and Michael A. Fligner (W . H. Freeman,\\n2017).\\n•For a treatment of both the t-test and resampling procedures in parallel, see\\nIntroductory Statistics and Analytics: A Resampling Perspective  by Peter Bruce\\n(Wiley, 2014) or Statistics: Unlocking the Power of Data , 2nd ed., by Robin Lock\\nand four other Lock family members (Wiley, 2016).\\nMultiple Testing\\nAs we’ve mentioned previously, there is a saying in statistics: “Torture the data long\\nenough, and it will confess. ” This means that if you look at the data through enough\\ndifferent perspectives and ask enough questions, you almost invariably will find a\\nstatistically significant effect.\\nFor example, if you have 20 predictor variables and one outcome variable, all ran‐\\ndomly  generated, the odds are pretty good that at least one predictor will (falsely) turn\\nout to be statistically significant if you do a series of 20 significance tests at the alpha\\n= 0.05 level. As previously discussed, this is called a Type 1 error . Y ou can calculate\\nthis probability by first finding the probability that all will correctly  test nonsignificant\\nat the 0.05 level. The probability that one will correctly test nonsignificant is 0.95, so\\nthe probability that all 20 will correctly test nonsignificant is 0.95 × 0.95 × 0.95…, or\\n112 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 129}),\n",
       " Document(page_content='1The multiplication rule states that the probability of n independent events all happening is the product of the\\nindividual probabilities. For example, if you and I each flip a coin once, the probability that your coin and my\\ncoin will both land heads is 0.5 × 0.5 = 0.25.0.9520 = 0.36.1 The probability that at least one predictor will (falsely) test significant\\nis the flip side of this probability, or 1 – ( probability that all will be nonsignificant ) =\\n0.64. This is known as alpha inflation .\\nThis issue is related to the problem of overfitting in data mining, or “fitting the model\\nto the noise. ” The more variables you add, or the more models you run, the greater\\nthe probability that something will emerge as “significant” just by chance.\\nKey Terms for Multiple Testing\\nType 1 error\\nMistakenly concluding that an effect is statistically significant.\\nFalse discovery rate\\nAcross multiple tests, the rate of making a Type 1 error.\\nAlpha inflation\\nThe multiple testing phenomenon, in which alpha , the probability of making a\\nType 1 error, increases as you conduct more tests.\\nAdjustment of p-values\\nAccounting for doing multiple tests on the same data.\\nOverfitting\\nFitting the noise.\\nIn supervised learning tasks, a holdout set where models are assessed on data that the\\nmodel has not seen before mitigates this risk. In statistical and machine learning tasks\\nnot involving a labeled holdout set, the risk of reaching conclusions based on statisti‐\\ncal noise persists.\\nIn statistics, there are some procedures intended to deal with this problem in very\\nspecific circumstances. For example, if you are comparing results across multiple\\ntreatment groups, you might ask multiple questions. So, for treatments A–C, you\\nmight ask:\\n•Is A different from B?\\n•Is B different from C?\\n•Is A different from C?\\nMultiple Testing | 113', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 130}),\n",
       " Document(page_content='Or, in a clinical trial, you might want to look at results from a therapy at multiple\\nstages. In each case, you are asking multiple questions, and with each question, you\\nare increasing the chance of being fooled by chance. Adjustment procedures in statis‐\\ntics can compensate for this by setting the bar for statistical significance more strin‐\\ngently than it would be set for a single hypothesis test. These adjustment procedures\\ntypically involve “dividing up the alpha” according to the number of tests. This results\\nin a smaller alpha (i.e., a more stringent bar for statistical significance) for each test.\\nOne such procedure, the Bonferroni adjustment, simply divides the alpha by the\\nnumber of comparisons. Another, used in comparing multiple group means, is\\nTukey’s “honest significant difference, ” or Tukey’s HSD . This test applies to the maxi‐\\nmum difference among group means, comparing it to a benchmark based on the t-\\ndistribution  (roughly equivalent to shuffling all the values together, dealing out\\nresampled groups of the same sizes as the original groups, and finding the maximum\\ndifference among the resampled group means).\\nHowever, the problem of multiple comparisons goes beyond these highly structured\\ncases and is related to the phenomenon of repeated data “dredging” that gives rise to\\nthe saying about torturing the data. Put another way, given sufficiently complex data,\\nif you haven’t found something interesting, you simply haven’t looked long and hard\\nenough. More data is available now than ever before, and the number of journal arti‐\\ncles published nearly doubled between 2002 and 2010. This gives rise to lots of\\nopportunities to find something interesting in the data, including multiplicity issues\\nsuch as:\\n•Checking for multiple pairwise differences across groups\\n•Looking at multiple subgroup results (“we found no significant treatment effect\\noverall, but we did find an effect for unmarried women younger than 30”)\\n•Trying lots of statistical models\\n•Including lots of variables in models\\n•Asking a number of different questions (i.e., different possible outcomes)\\n114 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 131}),\n",
       " Document(page_content='False Discovery Rate\\nThe term false discovery rate  was originally used to describe the rate\\nat which a given set of hypothesis tests would falsely identify a sig‐\\nnificant effect. It became particularly useful with the advent of\\ngenomic research, in which massive numbers of statistical tests\\nmight be conducted as part of a gene sequencing project. In these\\ncases, the term applies to the testing protocol, and a single false\\n“discovery” refers to the outcome of a hypothesis test (e.g., between\\ntwo samples). Researchers sought to set the parameters of the test‐\\ning process to control the false discovery rate at a specified level.\\nThe term has also been used for classification in data mining; it is\\nthe misclassification rate within the class 1 predictions. Or, put\\nanother way, it is the probability that a “discovery” (labeling a\\nrecord as a “1”) is false. Here we typically are dealing with the case\\nwhere 0s are abundant and 1s are interesting and rare (see Chap‐\\nter 5  and “The Rare Class Problem” on page 223 ).\\nFor a variety of reasons, including especially this general issue of “multiplicity, ” more\\nresearch does not necessarily mean better research. For example, the pharmaceutical\\ncompany Bayer found in 2011 that when it tried to replicate 67 scientific studies, it\\ncould fully replicate only 14 of them. Nearly two-thirds could not be replicated at all.\\nIn any case, the adjustment procedures for highly defined and structured statistical\\ntests are too specific and inflexible to be of general use to data scientists.  The bottom\\nline for data scientists on multiplicity is:\\n•For predictive modeling, the risk of getting an illusory model whose apparent\\nefficacy is largely a product of random chance is mitigated by cross-validation\\n(see “Cross-Validation” on page 155 ) and use of a holdout sample.\\n•For other procedures without a labeled holdout set to check the model, you must\\nrely on:\\n—Awareness that the more you query and manipulate the data, the greater the\\nrole that chance might play.\\n—Resampling and simulation heuristics to provide random chance benchmarks\\nagainst which observed results can be compared.\\nMultiple Testing | 115', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 132}),\n",
       " Document(page_content='Key Ideas\\n•Multiplicity in a research study or data mining project (multiple comparisons,\\nmany variables, many models, etc.) increases the risk of concluding that some‐\\nthing is significant just by chance.\\n•For situations involving multiple statistical comparisons (i.e., multiple tests of\\nsignificance), there are statistical adjustment procedures.\\n•In a data mining situation, use of a holdout sample with labeled outcome vari‐\\nables can help avoid misleading results.\\nFurther Reading\\n•For a short exposition of one procedure (Dunnett’s test) to adjust for multiple\\ncomparisons, see David Lane’s online statistics text .\\n•Megan Goldman offers a slightly longer treatment of the Bonferroni adjustment\\nprocedure .\\n•For an in-depth treatment of more flexible statistical procedures for adjusting p-\\nvalues, see Resampling-Based Multiple Testing  by Peter Westfall and Stanley Y oung\\n(Wiley, 1993).\\n•For a discussion of data partitioning and the use of holdout samples in predictive\\nmodeling, see Chapter 2 of Data Mining for Business Analytics , by Galit Shmueli,\\nPeter Bruce, Nitin Patel, Peter Gedeck, Inbal Y ahav, and Kenneth Lichtendahl\\n(Wiley, 2007–2020, with editions for R, Python , Excel, and JMP).\\nDegrees of Freedom\\nIn the documentation and settings for many statistical tests and probability distribu‐\\ntions, you will see a reference to “degrees of freedom. ” The concept is applied to statis‐\\ntics calculated from sample data, and refers to the number of values free to vary. For\\nexample, if you know the mean for a sample of 10 values, there are 9 degrees of free‐\\ndom (once you know 9 of the sample values, the 10th can be calculated and is not free\\nto vary). The degrees of freedom parameter, as applied to many probability distribu‐\\ntions, affects the shape of the distribution.\\nThe number of degrees of freedom is an input to many statistical tests. For example,\\ndegrees of freedom is the name given to the n – 1 denominator seen in the calcula‐\\ntions for variance and standard deviation. Why does it matter? When you use a sam‐\\nple to estimate the variance for a population, you will end up with an estimate that is\\n116 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 133}),\n",
       " Document(page_content='slightly biased downward if you use n in the denominator. If you use n – 1 in the\\ndenominator, the estimate will be free of that bias.\\nKey Terms for Degrees of Freedom\\nn or sample size\\nThe number of observations (also called rows  or records ) in the data.\\nd.f.\\nDegrees of freedom.\\nA large share of a traditional statistics course or text is consumed by various standard\\ntests of hypotheses (t-test, F-test, etc.). When sample statistics are standardized for\\nuse in traditional statistical formulas, degrees of freedom is part of the standardiza‐\\ntion calculation to ensure that your standardized data matches the appropriate refer‐\\nence distribution (t-distribution, F-distribution, etc.).\\nIs it important for data science? Not really, at least in the context of significance test‐\\ning. For one thing, formal statistical tests are used only sparingly in data science. For\\nanother, the data size is usually large enough that it rarely makes a real difference for\\na data scientist whether, for example, the denominator has n or n – 1. (As n gets large,\\nthe bias that would come from using n in the denominator disappears.)\\nThere is one context, though, in which it is relevant: the use of factored variables in\\nregression (including logistic regression). Some regression algorithms choke if\\nexactly redundant predictor variables are present. This most commonly occurs when\\nfactoring categorical variables into binary indicators (dummies). Consider the vari‐\\nable “day of week. ” Although there are seven days of the week, there are only six\\ndegrees of freedom in specifying day of week. For example, once you know that day\\nof week is not Monday through Saturday, you know it must be Sunday. Inclusion of\\nthe Mon–Sat indicators thus means that also including Sunday would cause the\\nregression to fail, due to a multicollinearity  error.\\nKey Ideas\\n•The number of degrees of freedom (d.f.) forms part of the calculation to stand‐\\nardize test statistics so they can be compared to reference distributions (t-\\ndistribution, F-distribution, etc.).\\n•The concept of degrees of freedom lies behind the factoring of categorical vari‐\\nables into n – 1 indicator or dummy variables when doing a regression (to avoid\\nmulticollinearity).\\nDegrees of Freedom | 117', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 134}),\n",
       " Document(page_content='Further Reading\\nThere are several web tutorials on degrees of freedom .\\nANOVA\\nSuppose that, instead of an A/B test, we had a comparison of multiple groups, say\\nA/B/C/D, each with numeric data. The statistical procedure that tests for a statistically\\nsignificant difference among the groups is called analysis of variance , or ANOVA .\\nKey Terms for ANOVA\\nPairwise comparison\\nA hypothesis test (e.g., of means) between two groups among multiple groups.\\nOmnibus test\\nA single hypothesis test of the overall variance among multiple group means.\\nDecomposition of variance\\nSeparation of components contributing to an individual value (e.g., from the\\noverall average, from a treatment mean, and from a residual error).\\nF-statistic\\nA standardized statistic that measures the extent to which differences among\\ngroup means exceed what might be expected in a chance model.\\nSS\\n“Sum of squares, ” referring to deviations from some average value.\\nTable 3-3  shows the stickiness of four web pages, defined as the number of seconds a\\nvisitor spent on the page. The four pages are switched out so that each web visitor\\nreceives one at random. There are a total of five visitors for each page, and in\\nTable 3-3 , each column is an independent set of data. The first viewer for page 1 has\\nno connection to the first viewer for page 2. Note that in a web test like this, we can‐\\nnot fully implement the classic randomized sampling design in which each visitor is\\nselected at random from some huge population. We must take the visitors as they\\ncome. Visitors may systematically differ depending on time of day, time of week, sea‐\\nson of the year, conditions of their internet, what device they are using, and so on.\\nThese factors should be considered as potential bias when the experiment results are\\nreviewed.\\n118 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 135}),\n",
       " Document(page_content='Table 3-3. Stickiness (in seconds) of four web pages\\nPage 1 Page 2 Page 3 Page 4\\n164 178 175 155\\n172 191 193 166\\n177 182 171 164\\n156 185 163 170\\n195 177 176 168\\nAverage 172 185 176 162\\nGrand average 173.75\\nNow we have a conundrum (see Figure 3-6 ). When we were comparing just two\\ngroups, it was a simple matter; we merely looked at the difference between the means\\nof each group. With four means, there are six possible comparisons between groups:\\n•Page 1 compared to page 2\\n•Page 1 compared to page 3\\n•Page 1 compared to page 4\\n•Page 2 compared to page 3\\n•Page 2 compared to page 4\\n•Page 3 compared to page 4\\nThe more such pairwise  comparisons we make, the greater the potential for being\\nfooled by random chance (see “Multiple Testing”  on page 112). Instead of worrying\\nabout all the different comparisons between individual pages we could possibly make,\\nwe can do a single overall test that addresses the question, “Could all the pages have\\nthe same underlying stickiness, and the differences among them be due to the ran‐\\ndom way in which a common set of session times got allocated among the four\\npages?”\\nANOVA | 119', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 136}),\n",
       " Document(page_content='Figure 3-6. Boxplots of the four groups show considerable differences  among them\\nThe procedure used to test this is ANOV A. The basis for it can be seen in the follow‐\\ning resampling procedure (specified here for the A/B/C/D test of web page\\nstickiness):\\n1.Combine all the data together in a single box.\\n2.Shuffle and draw out four resamples of five values each.\\n3.Record the mean of each of the four groups.\\n4.Record the variance among the four group means.\\n5.Repeat steps 2–4 many (say, 1,000) times.\\nWhat proportion of the time did the resampled variance exceed the observed var‐\\niance? This is the p-value.\\nThis type of permutation test is a bit more involved than the type used in “Permuta‐\\ntion Test” on page 97 . Fortunately, the aovp  function in the lmPerm  package computes\\na permutation test for this case:\\n120 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 137}),\n",
       " Document(page_content='> library(lmPerm)\\n> summary(aovp(Time ~ Page, data=four_sessions ))\\n[1] \"Settings:  unique SS \"\\nComponent  1 :\\n            Df R Sum Sq R Mean Sq Iter Pr(Prob)\\nPage         3    831.4    277.13 3104  0.09278 .\\nResiduals    16   1618.4    101.15\\n---\\nSignif. codes:  0 \\'***\\' 0.001 \\'**\\' 0.01 \\'*\\' 0.05 \\'.\\' 0.1 \\' \\' 1\\nThe p-value, given by Pr(Prob) , is 0.09278. In other words, given the same underly‐\\ning stickiness, 9.3% of the time the response rate among four pages might differ as\\nmuch as was actually observed, just by chance. This degree of improbability falls\\nshort of the traditional statistical threshold of 5%, so we conclude that the difference\\namong the four pages could have arisen by chance.\\nThe column Iter  lists the number of iterations taken in the permutation test. The\\nother columns correspond to a traditional ANOV A table and are described next.\\nIn Python , we can compute the permutation test using the following code:\\nobserved_variance  = four_sessions .groupby(\\'Page\\').mean().var()[0]\\nprint(\\'Observed means:\\' , four_sessions .groupby(\\'Page\\').mean().values.ravel())\\nprint(\\'Variance:\\' , observed_variance )\\ndef perm_test (df):\\n    df = df.copy()\\n    df[\\'Time\\'] = np.random.permutation (df[\\'Time\\'].values)\\n    return df.groupby(\\'Page\\').mean().var()[0]\\nperm_variance  = [perm_test (four_sessions ) for _ in range(3000)]\\nprint(\\'Pr(Prob)\\' , np.mean([var > observed_variance  for var in perm_variance ]))\\nF-Statistic\\nJust like the t-test can be used instead of a permutation test for comparing the mean\\nof two groups, there is a statistical test for ANOV A based on the F-statistic . The F-\\nstatistic is based on the ratio of the variance across group means (i.e., the treatment\\neffect) to the variance due to residual error. The higher this ratio, the more statisti‐\\ncally significant the result. If the data follows a normal distribution, then statistical\\ntheory dictates that the statistic should have a certain distribution. Based on this, it is\\npossible to compute a p-value.\\nANOVA | 121', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 138}),\n",
       " Document(page_content=\"In R, we can compute an ANOVA table  using the aov function:\\n> summary(aov(Time ~ Page, data=four_sessions ))\\n            Df Sum Sq Mean Sq F value Pr(>F)\\nPage         3  831.4   277.1    2.74 0.0776 .\\nResiduals    16 1618.4   101.2\\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\\nThe statsmodels  package provides an ANOV A implementation in Python :\\nmodel = smf.ols('Time ~ Page' , data=four_sessions ).fit()\\naov_table  = sm.stats.anova_lm (model)\\naov_table\\nThe output from the Python  code is almost identical to that from R.\\nDf is “degrees of freedom, ” Sum Sq  is “sum of squares, ” Mean Sq  is “mean squares”\\n(short for mean-squared deviations), and F value  is the F-statistic.  For the grand\\naverage, sum of squares is the departure of the grand average from 0, squared, times\\n20 (the number of observations). The degrees of freedom for the grand average is 1,\\nby definition.\\nFor the treatment means, the degrees of freedom is 3 (once three values are set, and\\nthen the grand average is set, the other treatment mean cannot vary). Sum of squares\\nfor the treatment means is the sum of squared departures between the treatment\\nmeans and the grand average.\\nFor the residuals, degrees of freedom is 20 (all observations can vary), and SS is the\\nsum of squared difference between the individual observations and the treatment\\nmeans. Mean squares (MS) is the sum of squares divided by the degrees of freedom.\\nThe F-statistic is MS(treatment)/MS(error). The F value thus depends only on this\\nratio and can be compared to a standard F-distribution to determine whether the dif‐\\nferences among treatment means are greater than would be expected in random\\nchance variation.\\n122 | Chapter 3: Statistical Experiments and Significance  Testing\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 139}),\n",
       " Document(page_content='Decomposition of Variance\\nObserved values in a data set can be considered sums of different\\ncomponents. For any observed data value within a data set, we can\\nbreak it down into the grand average, the treatment effect, and the\\nresidual error. We call this a “decomposition of variance”:\\n1.Start with grand average (173.75 for web page stickiness data).\\n2.Add treatment effect, which might be negative (independent\\nvariable = web page).\\n3.Add residual error, which might be negative.\\nThus the decomposition of the variance for the top-left value in the\\nA/B/C/D test table is as follows:\\n1.Start with grand average: 173.75.\\n2.Add treatment (group) effect: –1.75 (172 – 173.75).\\n3.Add residual: –8 (164 – 172).\\n4.Equals: 164.\\nTwo-Way ANOVA\\nThe A/B/C/D test just described is a “one-way” ANOV A, in which we have one factor\\n(group) that is varying. We could have a second factor involved—say, “weekend ver‐\\nsus weekday”—with data collected on each combination (group A weekend, group A\\nweekday, group B weekend, etc.). This would be a “two-way ANOV A, ” and we would\\nhandle it in similar fashion to the one-way ANOV A by identifying the “interaction\\neffect. ” After identifying the grand average effect and the treatment effect, we then\\nseparate the weekend and weekday observations for each group and find the differ‐\\nence between the averages for those subsets and the treatment average.\\nY ou can see that ANOV A and then two-way ANOV A are the first steps on the road\\ntoward a full statistical model, such as regression and logistic regression, in which\\nmultiple factors and their effects can be modeled (see Chapter 4 ).\\nKey Ideas\\n•ANOV A is a statistical procedure for analyzing the results of an experiment with\\nmultiple groups.\\n•It is the extension of similar procedures for the A/B test, used to assess whether\\nthe overall variation among groups is within the range of chance variation.\\nANOVA | 123', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 140}),\n",
       " Document(page_content='•A useful outcome of ANOV A is the identification of variance components associ‐\\nated with group treatments, interaction effects, and errors.\\nFurther Reading\\n•Introductory Statistics and Analytics: A Resampling Perspective  by Peter Bruce\\n(Wiley, 2014) has a chapter on ANOV A.\\n•Introduction to Design and Analysis of Experiments  by George Cobb (Wiley, 2008)\\nis a comprehensive and readable treatment of its subject.\\nChi-Square Test\\nWeb testing often goes beyond A/B testing and tests multiple treatments at once.  The\\nchi-square test is used with count data to test how well it fits some expected distribu‐\\ntion. The most common use of the chi-square  statistic in statistical practice is with\\nr×c contingency tables, to assess whether the null hypothesis of independence\\namong variables is reasonable (see also “Chi-Square Distribution” on page 80 ).\\nThe chi-square test was originally developed by Karl Pearson in 1900 . The term chi\\ncomes from the Greek letter Χ used by Pearson in the article.\\nKey Terms for Chi-Square Test\\nChi-square statistic\\nA measure of the extent to which some observed data departs from expectation.\\nExpectation or expected\\nHow we would expect the data to turn out under some assumption, typically the\\nnull hypothesis.\\nr×c means “rows by columns”—a 2 × 3 table has two rows and\\nthree columns.\\nChi-Square Test: A Resampling Approach\\nSuppose you are testing three different headlines—A, B, and C—and you run them\\neach on 1,000 visitors, with the results shown in Table 3-4 .\\n124 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 141}),\n",
       " Document(page_content='Table 3-4. Web testing results for three different  headlines\\nHeadline A Headline B Headline C\\nClick 14 8 12\\nNo-click 986 992 988\\nThe headlines certainly appear to differ. Headline A returns nearly twice the click rate\\nof B. The actual numbers are small, though. A resampling procedure can test whether\\nthe click rates differ to an extent greater than chance might cause. For this test, we\\nneed to have the “expected” distribution of clicks, and in this case, that would be\\nunder the null hypothesis assumption that all three headlines share the same click\\nrate, for an overall click rate of 34/3,000. Under this assumption, our contingency\\ntable would look like Table 3-5 .\\nTable 3-5. Expected if all three headlines have the same click rate (null hypothesis)\\nHeadline A Headline B Headline C\\nClick 11.33 11.33 11.33\\nNo-click 988.67 988.67 988.67\\nThe Pearson residual  is defined as:\\nR=Observed−Expected\\nExpected\\nR measures the extent to which the actual counts differ from these expected counts\\n(see Table 3-6 ).\\nTable 3-6. Pearson residuals\\nHeadline A Headline B Headline C\\nClick 0.792 –0.990 0.198\\nNo-click –0.085 0.106 –0.021\\nThe chi-square statistic is defined as the sum of the squared Pearson residuals:\\nΧ=∑\\nir\\n∑\\njc\\nR2\\nwhere r and c are the number of rows and columns, respectively. The chi-square sta‐\\ntistic for this example is 1.666. Is that more than could reasonably occur in a chance\\nmodel?\\nChi-Square Test | 125', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 142}),\n",
       " Document(page_content=\"We can test with this resampling algorithm:\\n1.Constitute a box with 34 ones (clicks) and 2,966 zeros (no clicks).\\n2.Shuffle, take three separate samples of 1,000, and count the clicks in each.\\n3.Find the squared differences between the shuffled counts and the expected\\ncounts and sum them.\\n4.Repeat steps 2 and 3, say, 1,000 times.\\n5.How often does the resampled sum of squared deviations exceed the observed?\\nThat’s the p-value.\\nThe function chisq.test  can be used to compute a resampled chi-square statistic in\\nR. For the click data, the chi-square test is:\\n> chisq.test (clicks, simulate.p.value =TRUE)\\n Pearson's Chi-squared test with simulated p-value (based on 2000 replicates)\\ndata:  clicks\\nX-squared = 1.6659, df = NA, p-value = 0.4853\\nThe test shows that this result could easily have been obtained by randomness.\\nTo run a permutation test in Python , use the following implementation:\\nbox = [1] * 34\\nbox.extend([0] * 2966)\\nrandom.shuffle(box)\\ndef chi2(observed , expected ):\\n    pearson_residuals  = []\\n    for row, expect in zip(observed , expected ):\\n        pearson_residuals .append([(observe - expect) ** 2 / expect\\n                                  for observe in row])\\n    # return sum of squares\\n    return np.sum(pearson_residuals )\\nexpected_clicks  = 34 / 3\\nexpected_noclicks  = 1000 - expected_clicks\\nexpected  = [34 / 3, 1000 - 34 / 3]\\nchi2observed  = chi2(clicks.values, expected )\\ndef perm_fun (box):\\n    sample_clicks  = [sum(random.sample(box, 1000)),\\n                     sum(random.sample(box, 1000)),\\n                     sum(random.sample(box, 1000))]\\n    sample_noclicks  = [1000 - n for n in sample_clicks ]\\n    return chi2([sample_clicks , sample_noclicks ], expected )\\nperm_chi2  = [perm_fun (box) for _ in range(2000)]\\n126 | Chapter 3: Statistical Experiments and Significance  Testing\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 143}),\n",
       " Document(page_content=\"resampled_p_value  = sum(perm_chi2  > chi2observed ) / len(perm_chi2 )\\nprint(f'Observed chi2: {chi2observed:.4f}' )\\nprint(f'Resampled p-value: {resampled_p_value:.4f}' )\\nChi-Square Test: Statistical Theory\\nAsymptotic statistical theory shows that the distribution of the chi-square statistic\\ncan be approximated by a chi-square distribution  (see “Chi-Square Distribution” on\\npage 80). The appropriate standard chi-square distribution is determined by the\\ndegrees of freedom  (see “Degrees of Freedom”  on page 116). For a contingency table,\\nthe degrees of freedom are related to the number of rows ( r) and columns ( c) as\\nfollows:\\ndegrees of freedom = r− 1 ×c− 1\\nThe chi-square distribution is typically skewed, with a long tail to the right; see\\nFigure 3-7  for the distribution with 1, 2, 5, and 20 degrees of freedom. The further\\nout on the chi-square distribution the observed statistic is, the lower the p-value.\\nThe function chisq.test  can be used to compute the p-value using the chi-square\\ndistribution as a reference:\\n> chisq.test (clicks, simulate.p.value =FALSE)\\n Pearson's Chi-squared test\\ndata:  clicks\\nX-squared = 1.6659, df = 2, p-value = 0.4348\\nIn Python , use the function scipy.stats.chi2_contingency :\\nchisq, pvalue, df, expected  = stats.chi2_contingency (clicks)\\nprint(f'Observed chi2: {chi2observed:.4f}' )\\nprint(f'p-value: {pvalue:.4f}' )\\nThe p-value is a little less than the resampling p-value; this is because the chi-square\\ndistribution is only an approximation of the actual distribution of the statistic.\\nChi-Square Test | 127\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 144}),\n",
       " Document(page_content=\"Figure 3-7. Chi-square distribution with various degrees of freedom\\nFisher’s Exact Test\\nThe chi-square distribution is a good approximation of the shuffled resampling test\\njust described, except when counts are extremely low (single digits, especially five or\\nfewer).  In such cases, the resampling procedure will yield more accurate p-values. In\\nfact, most statistical software has a procedure to actually enumerate all the possible\\nrearrangements (permutations) that can occur, tabulate their frequencies, and deter‐\\nmine exactly how extreme the observed result is. This is called Fisher’s exact test  after\\nthe great statistician R. A. Fisher. R code for Fisher’s exact test is simple in its basic\\nform:\\n> fisher.test (clicks)\\n Fisher's Exact Test for Count Data\\ndata:  clicks\\np-value = 0.4824\\nalternative  hypothesis : two.sided\\nThe p-value is very close to the p-value of 0.4853 obtained using the resampling\\nmethod.\\nWhere some counts are very low but others are quite high (e.g., the denominator in a\\nconversion rate), it may be necessary to do a shuffled permutation test instead of a\\nfull exact test, due to the difficulty of calculating all possible permutations. The\\npreceding  R function has several arguments that control whether to use this\\n128 | Chapter 3: Statistical Experiments and Significance  Testing\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 145}),\n",
       " Document(page_content='approximation  (simulate.p.value=TRUE or FALSE ), how many iterations should be\\nused (B=... ), and a computational constraint ( workspace=... ) that limits how far\\ncalculations for the exact  result should go.\\nThere is no implementation of Fisher’s exact test easily available in Python .\\nDetecting Scientific  Fraud\\nAn interesting example is provided by the case of Tufts University researcher Thereza\\nImanishi-Kari, who was accused in 1991 of fabricating data in her research. Congress‐\\nman John Dingell became involved, and the case eventually led to the resignation of\\nher colleague, David Baltimore, from the presidency of Rockefeller University.\\nOne element in the case rested on statistical evidence regarding the expected distribu‐\\ntion of digits in her laboratory data, where each observation had many digits. Investi‐\\ngators focused on the interior  digits (ignoring the first digit and last digit of a\\nnumber), which would be expected to follow a uniform random  distribution.  That is,\\nthey would occur randomly, with each digit having equal probability of occurring (the\\nlead digit might be predominantly one value, and the final digits might be affected by\\nrounding). Table 3-7  lists the frequencies of interior digits from the actual data in the\\ncase.\\nTable 3-7. Frequency of interior digits in laboratory data\\nDigit Frequency\\n0 14\\n1 71\\n2 7\\n3 65\\n4 23\\n5 19\\n6 12\\n7 45\\n8 53\\n9 6\\nThe distribution of the 315 digits, shown in Figure 3-8 , certainly looks nonrandom.\\nInvestigators calculated the departure from expectation (31.5—that’s how often each\\ndigit would occur in a strictly uniform distribution) and used a chi-square test (a\\nresampling procedure could equally have been used) to show that the actual distribu‐\\ntion was well beyond the range of normal chance variation, indicating the data might\\nChi-Square Test | 129', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 146}),\n",
       " Document(page_content='have been fabricated. (Note: Imanishi-Kari was ultimately exonerated after a lengthy\\nproceeding.)\\nFigure 3-8. Frequency histogram for Imanishi-Kari lab data\\nRelevance for Data Science\\nThe chi-square test, or Fisher’s exact test, is used when you want to know whether an\\neffect is for real or might be the product of chance. In most classical statistical appli‐\\ncations of the chi-square test, its role is to establish statistical significance, which is\\ntypically needed before a study or an experiment can be published. This is not so\\nimportant for data scientists. In most data science experiments, whether A/B or\\nA/B/C…, the goal is not simply to establish statistical significance but rather to arrive\\nat the best treatment. For this purpose, multi-armed bandits (see “Multi-Arm Bandit\\nAlgorithm” on page 131 ) offer a more complete solution.\\nOne data science application of the chi-square test, especially Fisher’s exact version, is\\nin determining appropriate sample sizes for web experiments. These experiments\\noften have very low click rates, and despite thousands of exposures, count rates might\\nbe too small to yield definitive conclusions in an experiment. In such cases, Fisher’s\\n130 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 147}),\n",
       " Document(page_content='exact test, the chi-square test, and other tests can be useful as a component of power\\nand sample size calculations (see “Power and Sample Size” on page 135 ).\\nChi-square tests are used widely in research by investigators in search of the elusive\\nstatistically significant p-value that will allow publication. Chi-square tests, or similar\\nresampling simulations, are used in data science applications more as a filter to deter‐\\nmine whether an effect or a feature is worthy of further consideration than as a for‐\\nmal test of significance. For example, they are used in spatial statistics and mapping\\nto determine whether spatial data conforms to a specified null distribution (e.g., are\\ncrimes concentrated in a certain area to a greater degree than random chance would\\nallow?). They can also be used in automated feature selection in machine learning, to\\nassess class prevalence across features and identify features where the prevalence of a\\ncertain class is unusually high or low, in a way that is not compatible with random\\nvariation.\\nKey Ideas\\n•A common procedure in statistics is to test whether observed data counts are\\nconsistent with an assumption of independence (e.g., propensity to buy a partic‐\\nular item is independent of gender).\\n•The chi-square distribution is the reference distribution (which embodies the\\nassumption of independence) to which the observed calculated chi-square statis‐\\ntic must be compared.\\nFurther Reading\\n•R. A. Fisher’s famous “Lady Tasting Tea” example from the beginning of the 20th\\ncentury remains a simple and effective illustration of his exact test. Google “Lady\\nTasting Tea, ” and you will find a number of good writeups.\\n•Stat Trek offers a good tutorial on the chi-square test .\\nMulti-Arm Bandit Algorithm\\nMulti-arm bandits offer an approach to testing, especially web testing, that allows\\nexplicit optimization and more rapid decision making than the traditional statistical\\napproach to designing experiments.\\nMulti-Arm Bandit Algorithm | 131', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 148}),\n",
       " Document(page_content='Key Terms for Multi-Arm Bandits\\nMulti-arm bandit\\nAn imaginary slot machine with multiple arms for the customer to choose from,\\neach with different payoffs, here taken to be an analogy for a multitreatment\\nexperiment.\\nArm\\nA treatment in an experiment (e.g., “headline A in a web test”).\\nWin\\nThe experimental analog of a win at the slot machine (e.g., “customer clicks on\\nthe link”).\\nA traditional A/B test involves data collected in an experiment, according to a speci‐\\nfied design, to answer a specific question such as, “Which is better, treatment A or\\ntreatment B?” The presumption is that once we get an answer to that question, the\\nexperimenting is over and we proceed to act on the results.\\nY ou can probably perceive several difficulties with that approach. First, our answer\\nmay be inconclusive: “effect not proven. ” In other words, the results from the experi‐\\nment may suggest an effect, but if there is an effect, we don’t have a big enough sam‐\\nple to prove it (to the satisfaction of the traditional statistical standards). What\\ndecision do we take? Second, we might want to begin taking advantage of results that\\ncome in prior to the conclusion of the experiment. Third, we might want the right to\\nchange our minds or to try something different based on additional data that comes\\nin after the experiment is over. The traditional approach to experiments and hypothe‐\\nsis tests dates from the 1920s and is rather inflexible. The advent of computer power\\nand software has enabled more powerful flexible approaches. Moreover, data science\\n(and business in general) is not so worried about statistical significance, but con‐\\ncerned more with optimizing overall effort and results.\\nBandit algorithms, which are very popular in web testing, allow you to test multiple\\ntreatments at once and reach conclusions faster than traditional statistical designs.\\nThey take their name from slot machines used in gambling, also termed one-armed\\nbandits (since they are configured in such a way that they extract money from the\\ngambler in a steady flow). If you imagine a slot machine with more than one arm,\\neach arm paying out at a different rate, you would have a multi-armed bandit, which\\nis the full name for this algorithm.\\nY our goal is to win as much money as possible and, more specifically, to identify and\\nsettle on the winning arm sooner rather than later. The challenge is that you don’t\\nknow at what overall rate the arms pay out—you only know the results of individual\\npulls on the arms. Suppose each “win” is for the same amount, no matter which arm.\\n132 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 149}),\n",
       " Document(page_content='What differs is the probability of a win. Suppose further that you initially try each\\narm 50 times and get the following results:\\nArm A: 10 wins out of 50\\nArm B: 2 win out of 50\\nArm C: 4 wins out of 50\\nOne extreme approach is to say, “Looks like arm A is a winner—let’s quit trying the\\nother arms and stick with A. ” This takes full advantage of the information from the\\ninitial trial. If A is truly superior, we get the benefit of that early on. On the other\\nhand, if B or C is truly better, we lose any opportunity to discover that. Another\\nextreme approach is to say, “This all looks to be within the realm of chance—let’s\\nkeep pulling them all equally. ” This gives maximum opportunity for alternates to A to\\nshow themselves. However, in the process, we are deploying what seem to be inferior\\ntreatments. How long do we permit that? Bandit algorithms take a hybrid approach:\\nwe start pulling A more often, to take advantage of its apparent superiority, but we\\ndon’t abandon B and C. We just pull them less often. If A continues to outperform,\\nwe continue to shift resources (pulls) away from B and C and pull A more often. If,\\non the other hand, C starts to do better, and A starts to do worse, we can shift pulls\\nfrom A back to C. If one of them turns out to be superior to A and this was hidden in\\nthe initial trial due to chance, it now has an opportunity to emerge with further\\ntesting.\\nNow think of applying this to web testing. Instead of multiple slot machine arms, you\\nmight have multiple offers, headlines, colors, and so on being tested on a website.\\nCustomers either click (a “win” for the merchant) or don’t click. Initially, the offers\\nare shown randomly and equally. If, however, one offer starts to outperform the oth‐\\ners, it can be shown (“pulled”) more often. But what should the parameters of the\\nalgorithm that modifies the pull rates be? What “pull rates” should we change to, and\\nwhen should we change?\\nHere is one simple algorithm, the epsilon-greedy algorithm for an A/B test:\\n1.Generate a uniformly distributed random number between 0 and 1.\\n2.If the number lies between 0 and epsilon (where epsilon is a number between 0\\nand 1, typically fairly small), flip a fair coin (50/50 probability), and:\\na.If the coin is heads, show offer A.\\nb.If the coin is tails, show offer B.\\n3.If the number is ≥ epsilon, show whichever offer has had the highest response\\nrate to date.\\nEpsilon is the single parameter that governs this algorithm. If epsilon is 1, we end up\\nwith a standard simple A/B experiment (random allocation between A and B for each\\nMulti-Arm Bandit Algorithm | 133', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 150}),\n",
       " Document(page_content='subject). If epsilon is 0, we end up with a purely greedy  algorithm—one that chooses\\nthe best available immediate option (a local optimum).  It seeks no further experimen‐\\ntation, simply assigning subjects (web visitors) to the best-performing treatment.\\nA more sophisticated algorithm uses “Thompson’s sampling. ” This procedure “sam‐\\nples” (pulls a bandit arm) at each stage to maximize the probability of choosing\\nthe best arm. Of course you don’t know which is the best arm—that’s the whole\\nproblem!— but as you observe the payoff with each successive draw, you gain more\\ninformation. Thompson’s sampling uses a Bayesian approach: some prior distribution\\nof rewards is assumed initially, using what is called a beta distribution  (this is a com‐\\nmon mechanism for specifying prior information in a Bayesian problem). As infor‐\\nmation accumulates from each draw, this information can be updated, allowing the\\nselection of the next draw to be better optimized as far as choosing the right arm.\\nBandit algorithms can efficiently handle 3+ treatments and move toward optimal\\nselection of the “best. ” For traditional statistical testing procedures, the complexity of\\ndecision making for 3+ treatments far outstrips that of the traditional A/B test, and\\nthe advantage of bandit algorithms is much greater.\\nKey Ideas\\n•Traditional A/B tests envision a random sampling process, which can lead to\\nexcessive exposure to the inferior treatment.\\n•Multi-arm bandits, in contrast, alter the sampling process to incorporate infor‐\\nmation learned during the experiment and reduce the frequency of the inferior\\ntreatment.\\n•They also facilitate efficient treatment of more than two treatments.\\n•There are different algorithms for shifting sampling probability away from the\\ninferior treatment(s) and to the (presumed) superior one.\\nFurther Reading\\n•An excellent short treatment of multi-arm bandit algorithms is found in Bandit\\nAlgorithms for Website Optimization , by John Myles White (O’Reilly, 2012).\\nWhite includes Python  code, as well as the results of simulations to assess the\\nperformance of bandits.\\n•For more (somewhat technical) information about Thompson sampling, see\\n“ Analysis of Thompson Sampling for the Multi-armed Bandit Problem”  by Shipra\\nAgrawal and Navin Goyal.\\n134 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 151}),\n",
       " Document(page_content='Power and Sample Size\\nIf you run a web test, how do you decide how long it should run (i.e., how many\\nimpressions per treatment are needed)? Despite what you may read in many guides to\\nweb testing, there is no good general guidance—it depends, mainly, on the frequency\\nwith which the desired goal is attained.\\nKey Terms for Power and Sample Size\\nEffect  size\\nThe minimum size of the effect that you hope to be able to detect in a statistical\\ntest, such as “a 20% improvement in click rates. ”\\nPower\\nThe probability of detecting a given effect size with a given sample size.\\nSignificance  level\\nThe statistical significance level at which the test will be conducted.\\nOne step in statistical calculations for sample size is to ask “Will a hypothesis test\\nactually reveal a difference between treatments A and B?” The outcome of a hypothe‐\\nsis test—the p-value—depends on what the real difference is between treatment A\\nand treatment B. It also depends on the luck of the draw—who gets selected for the\\ngroups in the experiment. But it makes sense that the bigger the actual difference\\nbetween treatments A and B, the greater the probability that our experiment will\\nreveal it; and the smaller the difference, the more data will be needed to detect it. To\\ndistinguish between a .350 hitter and a .200 hitter in baseball, not that many at-bats\\nare needed. To distinguish between a .300 hitter and a .280 hitter, a good many more\\nat-bats will be needed.\\nPower  is the probability of detecting a specified effect  size with specified sample char‐\\nacteristics (size and variability). For example, we might say (hypothetically) that the\\nprobability of distinguishing between a .330 hitter and a .200 hitter in 25 at-bats is\\n0.75. The effect size here is a difference of .130. And “detecting” means that a hypoth‐\\nesis test will reject the null hypothesis of “no difference” and conclude there is a real\\neffect. So the experiment of 25 at-bats ( n = 25) for two hitters, with an effect size of\\n0.130, has (hypothetical) power of 0.75, or 75%.\\nY ou can see that there are several moving parts here, and it is easy to get tangled up in\\nthe numerous statistical assumptions and formulas that will be needed (to specify\\nsample variability, effect size, sample size, alpha-level for the hypothesis test, etc., and\\nto calculate power). Indeed, there is special-purpose statistical software to calculate\\npower. Most data scientists will not need to go through all the formal steps needed to\\nreport power, for example, in a published paper. However, they may face occasions\\nPower and Sample Size | 135', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 152}),\n",
       " Document(page_content='where they want to collect some data for an A/B test, and collecting or processing the\\ndata involves some cost. In that case, knowing approximately how much data to col‐\\nlect can help avoid the situation where you collect data at some effort, and the result\\nends up being inconclusive. Here’s a fairly intuitive alternative approach:\\n1.Start with some hypothetical data that represents your best guess about the data\\nthat will result (perhaps based on prior data)—for example, a box with 20 ones\\nand 80 zeros to represent a .200 hitter, or a box with some observations of “time\\nspent on website. ”\\n2.Create a second sample simply by adding the desired effect size to the first sam‐\\nple—for example, a second box with 33 ones and 67 zeros, or a second box with\\n25 seconds added to each initial “time spent on website. ”\\n3.Draw a bootstrap sample of size n from each box.\\n4.Conduct a permutation (or formula-based) hypothesis test on the two bootstrap\\nsamples and record whether the difference between them is statistically\\nsignificant.\\n5.Repeat the preceding two steps many times and determine how often the differ‐\\nence was significant—that’s the estimated power.\\nSample Size\\nThe most common use of power calculations is to estimate how big a sample you will\\nneed.\\nFor example, suppose you are looking at click-through rates (clicks as a percentage of\\nexposures), and testing a new ad against an existing ad. How many clicks do you need\\nto accumulate in the study? If you are interested only in results that show a huge dif‐\\nference (say, a 50% difference), a relatively small sample might do the trick. If, on the\\nother hand, even a minor difference would be of interest, then a much larger sample\\nis needed. A standard approach is to establish a policy that a new ad must do better\\nthan an existing ad by some percentage, say, 10%; otherwise, the existing ad will\\nremain in place. This goal, the “effect size, ” then drives the sample size.\\nFor example, suppose current click-through rates are about 1.1%, and you are seeking\\na 10% boost to 1.21%. So we have two boxes: box A with 1.1% ones (say, 110 ones and\\n9,890 zeros), and box B with 1.21% ones (say, 121 ones and 9,879 zeros). For starters,\\nlet’s try 300 draws from each box (this would be like 300 “impressions” for each ad).\\nSuppose our first draw yields the following:\\nBox A: 3 ones\\nBox B: 5 ones\\n136 | Chapter 3: Statistical Experiments and Significance  Testing', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 153}),\n",
       " Document(page_content=\"Right away we can see that any hypothesis test would reveal this difference (5 versus\\n3) to be well within the range of chance variation. This combination of sample size\\n(n = 300 in each group) and effect size (10% difference) is too small for any hypothe‐\\nsis test to reliably show a difference.\\nSo we can try increasing the sample size (let’s try 2,000 impressions), and require a\\nlarger improvement (50% instead of 10%).\\nFor example, suppose current click-through rates are still 1.1%, but we are now seek‐\\ning a 50% boost to 1.65%. So we have two boxes: box A still with 1.1% ones (say, 110\\nones and 9,890 zeros), and box B with 1.65% ones (say, 165 ones and 9,868 zeros).\\nNow we’ll try 2,000 draws from each box. Suppose our first draw yields the following:\\nBox A: 19 ones\\nBox B: 34 ones\\nA significance test on this difference (34–19) shows it still registers as “not signifi‐\\ncant” (though much closer to significance than the earlier difference of 5–3). To cal‐\\nculate power, we would need to repeat the previous procedure many times, or use\\nstatistical software that can calculate power, but our initial draw suggests to us that\\neven detecting a 50% improvement will require several thousand ad impressions.\\nIn summary, for calculating power or required sample size, there are four moving\\nparts:\\n•Sample size\\n•Effect size you want to detect\\n•Significance level (alpha) at which the test will be conducted\\n•Power\\nSpecify any three of them, and the fourth can be calculated. Most commonly, you\\nwould want to calculate sample size, so you must specify the other three. With R and\\nPython , you also have to specify the alternative hypothesis as “greater” or “larger” to\\nget a one-sided test; see “One-Way Versus Two-Way Hypothesis Tests”  on page 95 for\\nmore discussion of one-way versus two-way tests. Here is R code for a test involving\\ntwo proportions, where both samples are the same size (this uses the pwr package):\\neffect_size  = ES.h(p1=0.0121, p2=0.011)\\npwr.2p.test (h=effect_size , sig.level =0.05, power=0.8, alternative ='greater’)\\n--\\n     Difference  of proportion  power calculation  for binomial  distribution\\n                                                       (arcsine transformation )\\n              h = 0.01029785\\n              n = 116601.7\\n      sig.level  = 0.05\\nPower and Sample Size | 137\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 154}),\n",
       " Document(page_content=\"power = 0.8\\n    alternative  = greater\\nNOTE: same sample sizes\\nThe function ES.h  calculates the effect size. We see that if we want a power of 80%, we\\nrequire a sample size of almost 120,000 impressions. If we are seeking a 50% boost\\n(p1=0.0165 ), the sample size is reduced to 5,500 impressions.\\nThe statsmodels  package contains several methods for power calculation. Here, we\\nuse proportion_effectsize  to calculate the effect size and TTestIndPower  to solve\\nfor the sample size:\\neffect_size  = sm.stats.proportion_effectsize (0.0121, 0.011)\\nanalysis  = sm.stats.TTestIndPower ()\\nresult = analysis .solve_power (effect_size =effect_size ,\\n                              alpha=0.05, power=0.8, alternative ='larger' )\\nprint('Sample Size: %.3f' % result)\\n--\\nSample Size: 116602.393\\nKey Ideas\\n•Finding out how big a sample size you need requires thinking ahead to the statis‐\\ntical test you plan to conduct.\\n•Y ou must specify the minimum size of the effect that you want to detect.\\n•Y ou must also specify the required probability of detecting that effect size\\n(power).\\n•Finally, you must specify the significance level (alpha) at which the test will be\\nconducted.\\nFurther Reading\\n•Sample Size Determination and Power  by Thomas Ryan (Wiley, 2013) is a com‐\\nprehensive and readable review of this subject.\\n•Steve Simon, a statistical consultant, has written a very engaging narrative-style\\npost on the subject .\\n138 | Chapter 3: Statistical Experiments and Significance  Testing\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 155}),\n",
       " Document(page_content='Summary\\nThe principles of experimental design—randomization of subjects into two or more\\ngroups receiving different treatments—allow us to draw valid conclusions about how\\nwell the treatments work. It is best to include a control treatment of “making no\\nchange. ” The subject of formal statistical inference—hypothesis testing, p-values,\\nt-tests , and much more along these lines—occupies much time and space in a tradi‐\\ntional statistics course or text, and the formality is mostly unneeded from a data sci‐\\nence perspective. However, it remains important to recognize the role that random\\nvariation can play in fooling the human brain. Intuitive resampling procedures (per‐\\nmutation and bootstrap) allow data scientists to gauge the extent to which chance\\nvariation can play a role in their data analysis.\\nSummary | 139', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 156}),\n",
       " Document(page_content='1This and subsequent sections in this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter\\nGedeck; used by permission.CHAPTER 4\\nRegression and Prediction\\nPerhaps the most common goal in statistics is to answer the question “Is the variable\\nX (or more likely, X1, ...,Xp) associated with a variable Y, and if so, what is the rela‐\\ntionship and can we use it to predict Y?”\\nNowhere is the nexus between statistics and data science stronger than in the realm of\\nprediction—specifically, the prediction of an outcome (target) variable based on the\\nvalues of other “predictor” variables. This process of training a model on data where\\nthe outcome is known, for subsequent application to data where the  outcome is not\\nknown, is termed supervised learning . Another important connection between data\\nscience and statistics is in the area of anomaly detection , where regression diagnostics\\noriginally intended for data analysis and improving the regression model can be used\\nto detect unusual records.\\nSimple Linear Regression\\nSimple linear regression provides a model of the relationship between the magnitude\\nof one variable and that of a second—for example, as X increases, Y also increases. Or\\nas X increases, Y decreases.1 Correlation is another way to measure how two variables\\nare related—see the section “Correlation” on page 30 . The difference is that while cor‐\\nrelation measures the strength  of an association between two variables, regression\\nquantifies the nature  of the relationship.\\nRegression and Prediction | 141', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 158}),\n",
       " Document(page_content='Key Terms for Simple Linear Regression\\nResponse\\nThe variable we are trying to predict.\\nSynonyms\\ndependent variable, Y variable, target, outcome\\nIndependent variable\\nThe variable used to predict the response.\\nSynonyms\\nX variable, feature, attribute, predictor\\nRecord\\nThe vector of predictor and outcome values for a specific individual or case.\\nSynonyms\\nrow, case, instance, example\\nIntercept\\nThe intercept of the regression line—that is, the predicted value when X= 0.\\nSynonyms\\nb0, β0\\nRegression coefficient\\nThe slope of the regression line.\\nSynonyms\\nslope, b1, β1, parameter estimates, weights\\nFitted values\\nThe estimates Yi obtained from the regression line.\\nSynonym\\npredicted values\\nResiduals\\nThe difference between the observed values and the fitted values.\\nSynonym\\nerrors\\n142 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 159}),\n",
       " Document(page_content='Least squares\\nThe method of fitting a regression by minimizing the sum of squared residuals.\\nSynonyms\\nordinary least squares, OLS\\nThe Regression Equation\\nSimple linear regression estimates how much Y will change when X changes by a\\ncertain amount . With the correlation coefficient, the variables X and Y are inter‐\\nchangeable. With regression, we are trying to predict the Y variable from X using a\\nlinear relationship (i.e., a line):\\nY=b0+b1X\\nWe read this as “Y equals b1 times X, plus a constant b0. ” The symbol b0 is known as\\nthe intercept  (or constant), and the symbol b1 as the slope  for X. Both appear in R\\noutput as coefficients , though in general use the term coefficient  is often reserved for\\nb1. The Y variable is known as the response  or dependent  variable since it depends on\\nX. The X variable is known as the predictor  or independent  variable. The machine\\nlearning community tends to use other terms, calling Y the target  and X a feature\\nvector. Throughout this book, we will use the terms predictor  and feature  interchange‐\\nably.\\nConsider the scatterplot in Figure 4-1  displaying the number of years a worker was\\nexposed to cotton dust ( Exposure ) versus a measure of lung capacity ( PEFR  or “peak\\nexpiratory flow rate”). How is PEFR  related to Exposure ? It’s hard to tell based just on\\nthe picture.\\n143', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 160}),\n",
       " Document(page_content='Figure 4-1. Cotton exposure versus lung capacity\\nSimple linear regression tries to find the “best” line to predict the response PEFR  as a\\nfunction of the predictor variable Exposure :\\nPEFR = b0+b1Exposure\\nThe lm function in R can be used to fit a linear regression:\\nmodel <- lm(PEFR ~ Exposure , data=lung)\\nlm stands for linear model , and the ~ symbol denotes that PEFR  is predicted by Expo\\nsure . With this model definition, the intercept is automatically included and fitted. If\\nyou want to exclude the intercept from the model, you need to write the model defi‐\\nnition as follows:\\nPEFR ~ Exposure  - 1\\n144 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 161}),\n",
       " Document(page_content=\"Printing the model  object produces the following output:\\nCall:\\nlm(formula = PEFR ~ Exposure , data = lung)\\nCoefficients :\\n(Intercept )     Exposure\\n    424.583       -4.185\\nThe intercept, or b0, is 424.583 and can be interpreted as the predicted PEFR  for a\\nworker with zero years exposure. The regression coefficient, or b1, can be interpreted\\nas follows: for each additional year that a worker is exposed to cotton dust, the work‐\\ner’s PEFR  measurement is reduced by –4.185.\\nIn Python , we can use LinearRegression  from the scikit-learn  package. (the stats\\nmodels  package has a linear regression implementation that is more similar to R\\n(sm.OLS ); we will use it later in this chapter):\\npredictors  = ['Exposure' ]\\noutcome = 'PEFR'\\nmodel = LinearRegression ()\\nmodel.fit(lung[predictors ], lung[outcome])\\nprint(f'Intercept: {model.intercept_:.3f}' )\\nprint(f'Coefficient Exposure: {model.coef_[0]:.3f}' )\\nThe regression line from this model is displayed in Figure 4-2 .\\nSimple Linear Regression | 145\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 162}),\n",
       " Document(page_content='Figure 4-2. Slope and intercept for the regression fit to the lung data\\nFitted Values and Residuals\\nImportant concepts in regression analysis are the fitted  values  (the predictions) and\\nresiduals  (prediction errors). In general, the data doesn’t fall exactly on a line, so the\\nregression equation should include an explicit error term ei:\\nYi=b0+b1Xi+ei\\nThe fitted values, also referred to as the predicted values , are typically denoted by Yi\\n(Y-hat). These are given by:\\nYi=b0+b1Xi\\nThe notation b0 and b1 indicates that the coefficients are estimated versus known.\\n146 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 163}),\n",
       " Document(page_content='2In Bayesian statistics, the true value is assumed to be a random variable with a specified distribution. In the\\nBayesian context, instead of estimates of unknown parameters, there are posterior and prior distributions.\\nHat Notation: Estimates Versus Known Values\\nThe “hat” notation is used to differentiate between estimates and\\nknown values. So the symbol b (“b-hat”) is an estimate of the\\nunknown parameter b. Why do statisticians differentiate between\\nthe estimate and the true value? The estimate has uncertainty,\\nwhereas the true value is fixed.2\\nWe compute the residuals ei by subtracting the predicted  values from the original\\ndata:\\nei=Yi−Yi\\nIn R, we can obtain the fitted values and residuals using the functions predict  and\\nresiduals :\\nfitted <- predict(model)\\nresid <- residuals (model)\\nWith scikit-learn ’s LinearRegression  model, we use the predict  method on the\\ntraining data to get the fitted  values and subsequently the residuals . As we will see,\\nthis is a general pattern that all models in scikit-learn  follow:\\nfitted = model.predict(lung[predictors ])\\nresiduals  = lung[outcome] - fitted\\nFigure 4-3  illustrates the residuals from the regression line fit to the lung data. The\\nresiduals are the length of the vertical dashed lines from the data to the line.\\nSimple Linear Regression | 147', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 164}),\n",
       " Document(page_content='Figure 4-3. Residuals from a regression line (to accommodate all the data, the y-axis\\nscale differs  from Figure 4-2 , hence the apparently different  slope)\\nLeast Squares\\nHow is the model fit to the data? When there is a clear relationship, you could imag‐\\nine fitting the line by hand. In practice, the regression line is the estimate that mini‐\\nmizes the sum of squared residual values, also called the residual sum of squares  or\\nRSS:\\nRSS =∑\\ni= 1n\\nYi−Yi2\\n=∑\\ni= 1n\\nYi−b0−b1Xi2\\nThe estimates b0 and b1 are the values that minimize RSS.\\nThe method of minimizing the sum of the squared residuals is termed least squares\\nregression, or ordinary least squares  (OLS) regression. It is often attributed to Carl\\nFriedrich Gauss, the German mathematician, but was first published by the French\\n148 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 165}),\n",
       " Document(page_content='mathematician Adrien-Marie Legendre in 1805.  Least squares regression can be com‐\\nputed quickly and easily with any standard statistical software.\\nHistorically, computational convenience is one reason for the widespread use of least\\nsquares in regression. With the advent of big data, computational speed is still an\\nimportant factor. Least squares, like the mean (see “Median and Robust Estimates” on\\npage 10 ), are sensitive to outliers, although this tends to be a significant problem only\\nin small or moderate-sized data sets. See “Outliers”  on page 177 for a discussion of\\noutliers in regression.\\nRegression Terminology\\nWhen analysts and researchers use the term regression  by itself,\\nthey are typically referring to linear regression; the focus is usually\\non developing a linear model to explain the relationship between\\npredictor variables and a numeric outcome variable. In its formal\\nstatistical sense, regression also includes nonlinear models that\\nyield a functional relationship between predictors and outcome\\nvariables. In the machine learning community, the term is also\\noccasionally used loosely to refer to the use of any predictive model\\nthat produces a predicted numeric outcome (as opposed to classifi‐\\ncation methods that predict a binary or categorical outcome).\\nPrediction Versus Explanation (Profiling)\\nHistorically, a primary use of regression was to illuminate a supposed linear relation‐\\nship between predictor variables and an outcome variable. The goal has been to\\nunderstand a relationship and explain it using the data that the regression was fit to.\\nIn this case, the primary focus is on the estimated slope of the regression equation, b.\\nEconomists want to know the relationship between consumer spending and GDP\\ngrowth. Public health officials might want to understand whether a public informa‐\\ntion campaign is effective in promoting safe sex practices. In such cases, the focus is\\nnot on predicting individual cases but rather on understanding the overall relation‐\\nship among variables.\\nWith the advent of big data, regression is widely used to form a model to predict indi‐\\nvidual outcomes for new data (i.e., a predictive model) rather than explain data in\\nhand. In this instance, the main items of interest are the fitted values Y. In marketing,\\nregression can be used to predict the change in revenue in response to the size of an\\nad campaign. Universities use regression to predict students’ GPA based on their SAT\\nscores.\\nA regression model that fits the data well is set up such that changes in X lead to\\nchanges in Y. However, by itself, the regression equation does not prove the direction\\nof causation. Conclusions about causation must come from a broader understanding\\nSimple Linear Regression | 149', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 166}),\n",
       " Document(page_content='about the relationship. For example, a regression equation might show a definite rela‐\\ntionship between number of clicks on a web ad and number of conversions. It is our\\nknowledge of the marketing process, not the regression equation, that leads us to the\\nconclusion that clicks on the ad lead to sales, and not vice versa.\\nKey Ideas\\n•The regression equation models the relationship between a response variable Y\\nand a predictor variable X as a line.\\n•A regression model yields fitted values and residuals—predictions of the\\nresponse and the errors of the predictions.\\n•Regression models are typically fit by the method of least squares.\\n•Regression is used both for prediction and explanation.\\nFurther Reading\\nFor an in-depth treatment of prediction versus explanation, see Galit Shmueli’s article\\n“To Explain or to Predict?” .\\nMultiple Linear Regression\\nWhen there are multiple predictors, the equation  is simply extended to accommodate\\nthem:\\nY=b0+b1X1+b2X2+ ... + bpXp+e\\nInstead of a line, we now have a linear model—the relationship between each coeffi‐\\ncient and its variable (feature) is linear.\\nKey Terms for Multiple Linear Regression\\nRoot mean squared error\\nThe square root of the average squared error of the regression (this is the most\\nwidely used metric to compare regression models).\\nSynonym\\nRMSE\\nResidual standard error\\nThe same as the root mean squared error, but adjusted for degrees of freedom.\\n150 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 167}),\n",
       " Document(page_content=\"Synonym\\nRSE\\nR-squared\\nThe proportion of variance explained by the model, from 0 to 1.\\nSynonyms\\ncoefficient of determination, R2\\nt-statistic\\nThe coefficient for a predictor, divided by the standard error of the coefficient,\\ngiving a metric to compare the importance of variables in the model. See “t-Tests”\\non page 110 .\\nWeighted regression\\nRegression with the records having different weights.\\nAll of the other concepts in simple linear regression, such as fitting by least squares\\nand the definition of fitted values and residuals, extend to the multiple linear regres‐\\nsion setting. For example, the fitted values are given by:\\nYi=b0+b1X1,i+b2X2,i+ ... + bpXp,i\\nExample: King County Housing Data\\nAn example of using multiple linear regression is in estimating the value of houses.\\nCounty assessors must estimate the value of a house for the purposes of assessing\\ntaxes. Real estate professionals and home buyers consult popular websites such as Zil‐\\nlow to ascertain a fair price. Here are a few rows of housing data from King County\\n(Seattle), Washington, from the house data.frame :\\nhead(house[, c('AdjSalePrice' , 'SqFtTotLiving' , 'SqFtLot' , 'Bathrooms' ,\\n               'Bedrooms' , 'BldgGrade' )])\\nSource: local data frame [6 x 6]\\n  AdjSalePrice  SqFtTotLiving  SqFtLot Bathrooms  Bedrooms  BldgGrade\\n         (dbl)         (int)   (int)     (dbl)    (int)     (int)\\n1       300805          2400    9373      3.00        6         7\\n2      1076162          3764   20156      3.75        4        10\\n3       761805          2060   26036      1.75        4         8\\n4       442065          3200    8618      3.75        5         7\\n5       297065          1720    8620      1.75        4         7\\n6       411781           930    1012      1.50        2         8\\nThe head  method of pandas  data frame lists the top rows:\\nMultiple Linear Regression | 151\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 168}),\n",
       " Document(page_content=\"subset = ['AdjSalePrice' , 'SqFtTotLiving' , 'SqFtLot' , 'Bathrooms' ,\\n          'Bedrooms' , 'BldgGrade' ]\\nhouse[subset].head()\\nThe goal is to predict the sales price from the other variables. The lm function handles\\nthe multiple regression case simply by including more terms on the righthand side of\\nthe equation; the argument na.action=na.omit  causes the model to drop records\\nthat have missing values:\\nhouse_lm  <- lm(AdjSalePrice  ~ SqFtTotLiving  + SqFtLot + Bathrooms  +\\n               Bedrooms  + BldgGrade ,\\n               data=house, na.action =na.omit)\\nscikit-learn ’s LinearRegression  can be used for multiple linear regression as well:\\npredictors  = ['SqFtTotLiving' , 'SqFtLot' , 'Bathrooms' , 'Bedrooms' , 'BldgGrade' ]\\noutcome = 'AdjSalePrice'\\nhouse_lm  = LinearRegression ()\\nhouse_lm .fit(house[predictors ], house[outcome])\\nPrinting house_lm  object produces the following output:\\nhouse_lm\\nCall:\\nlm(formula = AdjSalePrice  ~ SqFtTotLiving  + SqFtLot + Bathrooms  +\\n    Bedrooms  + BldgGrade , data = house, na.action  = na.omit)\\nCoefficients :\\n  (Intercept )  SqFtTotLiving         SqFtLot      Bathrooms\\n   -5.219e+05       2.288e+02      -6.047e-02      -1.944e+04\\n     Bedrooms       BldgGrade\\n   -4.777e+04       1.061e+05\\nFor a LinearRegression  model, intercept and coefficients are the fields intercept_\\nand coef_  of the fitted model:\\nprint(f'Intercept: {house_lm.intercept_:.3f}' )\\nprint('Coefficients:' )\\nfor name, coef in zip(predictors , house_lm .coef_):\\n    print(f' {name}: {coef}' )\\nThe interpretation of  the coefficients is as with simple linear regression: the predicted\\nvalue Y changes by the coefficient bj for each unit change in Xj assuming all the other\\nvariables, Xk for k≠j, remain the same. For example, adding an extra finished square\\nfoot to a house increases the estimated value by roughly $229; adding 1,000 finished\\nsquare feet implies the value will increase by $228,800.\\n152 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 169}),\n",
       " Document(page_content='Assessing the Model\\nThe most important performance metric from a data science perspective is root mean\\nsquared error , or RMSE . RMSE is the square root of the average squared error in the\\npredicted yi values:\\nRMSE =∑i= 1nyi−yi2\\nn\\nThis measures the overall accuracy of the model and is a basis for comparing it to\\nother models (including models fit using machine learning techniques). Similar to\\nRMSE is the residual standard error , or RSE. In this case we have p predictors, and the\\nRSE is given by:\\nRSE =∑i= 1nyi−yi2\\nn−p− 1\\nThe only difference is that the denominator is the degrees of freedom, as opposed to\\nnumber of records (see “Degrees of Freedom” on page 116). In practice, for linear\\nregression, the difference between RMSE and RSE is very small, particularly for big\\ndata applications.\\nThe summary  function in R computes RSE as well as other metrics for a regression\\nmodel:\\nsummary(house_lm )\\nCall:\\nlm(formula = AdjSalePrice  ~ SqFtTotLiving  + SqFtLot + Bathrooms  +\\n    Bedrooms  + BldgGrade , data = house, na.action  = na.omit)\\nResiduals :\\n     Min       1Q   Median       3Q      Max\\n-1199479   -118908   -20977    87435  9473035\\nCoefficients :\\n                Estimate  Std. Error t value Pr(>|t|)\\n(Intercept )   -5.219e+05   1.565e+04  -33.342  < 2e-16 ***\\nSqFtTotLiving   2.288e+02   3.899e+00   58.694  < 2e-16 ***\\nSqFtLot       -6.047e-02   6.118e-02   -0.988    0.323\\nBathrooms      -1.944e+04   3.625e+03   -5.363 8.27e-08  ***\\nBedrooms       -4.777e+04   2.490e+03  -19.187  < 2e-16 ***\\nBldgGrade       1.061e+05   2.396e+03   44.277  < 2e-16 ***\\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\\nMultiple Linear Regression | 153', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 170}),\n",
       " Document(page_content=\"Residual  standard  error: 261300 on 22681 degrees of freedom\\nMultiple  R-squared:  0.5406, Adjusted  R-squared:  0.5405\\nF-statistic :  5338 on 5 and 22681 DF,  p-value: < 2.2e-16\\nscikit-learn  provides a number of metrics for regression and classification. Here,\\nwe use mean_squared_error  to get RMSE and r2_score  for the coefficient of\\ndetermination:\\nfitted = house_lm .predict(house[predictors ])\\nRMSE = np.sqrt(mean_squared_error (house[outcome], fitted))\\nr2 = r2_score (house[outcome], fitted)\\nprint(f'RMSE: {RMSE:.0f}' )\\nprint(f'r2: {r2:.4f}' )\\nUse statsmodels  to get a more detailed analysis of the regression model in Python :\\nmodel = sm.OLS(house[outcome], house[predictors ].assign(const=1))\\nresults = model.fit()\\nresults.summary()\\nThe pandas  method assign , as used here, adds a constant column with value 1 to the\\npredictors. This is required to model the intercept.\\nAnother useful metric that you will see in software output is the coefficient  of determi‐\\nnation , also called the R-squared  statistic or R2. R-squared ranges from 0 to 1 and\\nmeasures the proportion of variation in the data that is accounted for in the model. It\\nis useful mainly in explanatory uses of regression where you want to assess how well\\nthe model fits the data. The formula for R2 is:\\nR2= 1 −∑i= 1nyi−yi2\\n∑i= 1nyi−y2\\nThe denominator is proportional to the variance of Y. The output from R also reports\\nan adjusted R-squared , which adjusts for the degrees of freedom, effectively penalizing\\nthe addition of more predictors to a model. Seldom is this significantly different from\\nR-squared  in multiple regression with large data sets.\\nAlong with the estimated coefficients, R and statsmodels  report the standard error \\nof the coefficients (SE) and a t-statistic :\\ntb=b\\nSEb\\nThe t-statistic—and its mirror image, the p-value—measures the extent to which a\\ncoefficient is “statistically significant”—that is, outside the range of what a random\\nchance arrangement of predictor and target variable might produce. The higher the\\n154 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 171}),\n",
       " Document(page_content='t-statistic  (and the lower the p-value), the more significant the predictor. Since parsi‐\\nmony is a valuable model feature, it is useful to have a tool like this to guide choice of\\nvariables to include as predictors (see “Model Selection and Stepwise Regression”  on\\npage 156 ).\\nIn addition to the t-statistic, R and other packages will often report\\na p-value  (Pr(>|t|)  in the R output) and F-statistic . Data scientists\\ndo not generally get too involved with the interpretation of these\\nstatistics, nor with the issue of statistical significance. Data scien‐\\ntists primarily focus on the t-statistic as a useful guide for whether\\nto include a predictor in a model or not.  High t-statistics (which go\\nwith p-values near 0) indicate a predictor should be retained in a\\nmodel, while very low t-statistics indicate a predictor could be\\ndropped. See “p-Value” on page 106  for more discussion.\\nCross-Validation\\nClassic statistical regression metrics ( R2, F-statistics, and p-values) are all “in-sample”\\nmetrics—they are applied to the same data that was used to fit the model. Intuitively,\\nyou can see that it would make a lot of sense to set aside some of the original data, not\\nuse it to fit the model, and then apply the model to the set-aside (holdout) data to see\\nhow well it does. Normally, you would use a majority of the data to fit the model and\\nuse a smaller portion to test the model.\\nThis idea of “out-of-sample” validation is not new, but it did not really take hold until\\nlarger data sets became more prevalent; with a small data set, analysts typically want\\nto use all the data and fit the best possible model.\\nUsing a holdout sample, though, leaves you subject to some uncertainty that arises\\nsimply from variability in the small holdout sample. How different would the assess‐\\nment be if you selected a different holdout sample?\\nCross-validation extends the idea of a holdout sample to multiple sequential holdout\\nsamples. The algorithm for basic k-fold cross-validation  is as follows:\\n1.Set aside 1/k of the data as a holdout sample.\\n2.Train the model on the remaining data.\\n3.Apply (score) the model to the 1/k holdout, and record needed model assessment\\nmetrics.\\n4.Restore the first 1/k of the data, and set aside the next 1/k (excluding any records\\nthat got picked the first time).\\n5.Repeat steps 2 and 3.\\nMultiple Linear Regression | 155', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 172}),\n",
       " Document(page_content=\"6.Repeat until each record has been used in the holdout portion.\\n7.Average or otherwise combine the model assessment metrics.\\nThe division of the data into the training sample and the holdout sample is also called \\na fold.\\nModel Selection and Stepwise Regression\\nIn some problems, many variables could be used as predictors in a regression. For\\nexample, to predict house value, additional variables such as the basement size or\\nyear built could be used. In R, these are easy to add to the regression equation:\\nhouse_full  <- lm(AdjSalePrice  ~ SqFtTotLiving  + SqFtLot + Bathrooms  +\\n                 Bedrooms  + BldgGrade  + PropertyType  + NbrLivingUnits  +\\n                 SqFtFinBasement  + YrBuilt + YrRenovated  +\\n                 NewConstruction ,\\n               data=house, na.action =na.omit)\\nIn Python , we need to convert the categorical and boolean variables into numbers:\\npredictors  = ['SqFtTotLiving' , 'SqFtLot' , 'Bathrooms' , 'Bedrooms' , 'BldgGrade' ,\\n              'PropertyType' , 'NbrLivingUnits' , 'SqFtFinBasement' , 'YrBuilt' ,\\n              'YrRenovated' , 'NewConstruction' ]\\nX = pd.get_dummies (house[predictors ], drop_first =True)\\nX['NewConstruction' ] = [1 if nc else 0 for nc in X['NewConstruction' ]]\\nhouse_full  = sm.OLS(house[outcome], X.assign(const=1))\\nresults = house_full .fit()\\nresults.summary()\\nAdding more variables, however, does not necessarily mean we have a better model.\\nStatisticians use the principle of Occam’s razor  to guide the choice of a model: all\\nthings being equal, a simpler model should be used in preference to a more compli‐\\ncated model.\\nIncluding additional variables always reduces RMSE and increases R2 for the training\\ndata. Hence, these are not appropriate to help guide the model choice. One approach\\nto including model complexity is to use the adjusted R2:\\nRad j2= 1 − (1 − R2)n− 1\\nn−P− 1\\nHere, n is the number of records and P is the number of variables in the model.\\n156 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 173}),\n",
       " Document(page_content='In the 1970s, Hirotugu Akaike, the eminent Japanese statistician, developed a metric\\ncalled AIC (Akaike’s Information Criteria) that penalizes adding terms to a model.  In\\nthe case of regression, AIC has the form:\\nAIC = 2 P + n log(RSS/n)\\nwhere P is the number of variables and n is the number of records. The goal is to find\\nthe model that minimizes AIC; models with k more extra variables are penalized by\\n2k.\\nAIC, BIC, and Mallows Cp\\nThe formula for AIC may seem a bit mysterious, but in fact it is\\nbased on asymptotic results in information theory. There are sev‐\\neral variants to AIC:\\nAICc\\nA version of AIC corrected for small sample sizes.\\nBIC or Bayesian information criteria\\nSimilar to AIC, with a stronger penalty for including addi‐\\ntional variables to the model.\\nMallows Cp\\nA variant of AIC developed by Colin Mallows.\\nThese are typically reported as in-sample metrics (i.e., on the train‐\\ning data), and data scientists using holdout data for model assess‐\\nment do not need to worry about the differences among them or\\nthe underlying theory behind them.\\nHow do we find the model that minimizes AIC or maximizes adjusted R2? One way is\\nto search through all possible models, an approach called all subset regression . This is\\ncomputationally expensive and is not feasible for problems with large data and many\\nvariables. An attractive alternative is to use stepwise regression . It could start with a\\nfull model and successively drop variables that don’t contribute meaningfully. This is\\ncalled backward elimination . Alternatively one could start with a constant model and\\nsuccessively add variables ( forward selection ). As a third option we can also succes‐\\nsively add and drop predictors to find a model that lowers AIC or adjusted R2. The\\nMASS  in R package by Venebles and Ripley offers a stepwise regression function called\\nstepAIC :\\nlibrary(MASS)\\nstep <- stepAIC(house_full , direction =\"both\")\\nstep\\nCall:\\nMultiple Linear Regression | 157', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 174}),\n",
       " Document(page_content=\"lm(formula = AdjSalePrice  ~ SqFtTotLiving  + Bathrooms  + Bedrooms  +\\n    BldgGrade  + PropertyType  + SqFtFinBasement  + YrBuilt, data = house,\\n    na.action  = na.omit)\\nCoefficients :\\n              (Intercept )              SqFtTotLiving\\n                6.179e+06                   1.993e+02\\n                Bathrooms                    Bedrooms\\n                4.240e+04                  -5.195e+04\\n                BldgGrade   PropertyTypeSingle  Family\\n                1.372e+05                   2.291e+04\\n    PropertyTypeTownhouse             SqFtFinBasement\\n                8.448e+04                   7.047e+00\\n                  YrBuilt\\n               -3.565e+03\\nscikit-learn  has no implementation for stepwise regression. We implemented func‐\\ntions stepwise_selection , forward_selection , and backward_elimination  in our\\ndmba  package:\\ny = house[outcome]\\ndef train_model (variables ): \\n    if len(variables ) == 0:\\n        return None\\n    model = LinearRegression ()\\n    model.fit(X[variables ], y)\\n    return model\\ndef score_model (model, variables ): \\n    if len(variables ) == 0:\\n        return AIC_score (y, [y.mean()] * len(y), model, df=1)\\n    return AIC_score (y, model.predict(X[variables ]), model)\\nbest_model , best_variables  = stepwise_selection (X.columns, train_model ,\\n                                                score_model , verbose=True)\\nprint(f'Intercept: {best_model.intercept_:.3f}' )\\nprint('Coefficients:' )\\nfor name, coef in zip(best_variables , best_model .coef_):\\n    print(f' {name}: {coef}' )\\nDefine a function that returns a fitted model for a given set of variables.\\nDefine a function that returns a score for a given model and set of variables. In\\nthis case, we use the AIC_score  implemented in the dmba  package.\\nThe function chose a model in which several variables were dropped from\\nhouse_full : SqFtLot , NbrLivingUnits , YrRenovated , and NewConstruction .\\n158 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 175}),\n",
       " Document(page_content='Simpler yet are forward selection  and backward selection . In forward selection, you\\nstart with no predictors and add them one by one, at each step adding the predictor\\nthat has the largest contribution to R2, and stopping when the contribution is no\\nlonger statistically significant. In backward selection, or backward elimination , you\\nstart with the full model and take away predictors that are not statistically significant\\nuntil you are left with a model in which all predictors are statistically significant.\\nPenalized regression  is similar in spirit to AIC.  Instead of explicitly searching through\\na discrete set of models, the model-fitting equation incorporates a constraint that\\npenalizes the model for too many variables (parameters). Rather than eliminating\\npredictor variables entirely—as with stepwise, forward, and backward selection—\\npenalized regression applies the penalty by reducing coefficients, in some cases to\\nnear zero. Common penalized regression methods are ridge regression  and lasso\\nregression .\\nStepwise regression and all subset regression are in-sample  methods to assess and\\ntune models. This means the model selection is possibly subject to overfitting (fitting\\nthe noise in the data) and may not perform as well when applied to new data. One\\ncommon approach to avoid this is to use cross-validation to validate the models. In\\nlinear regression, overfitting is typically not a major issue, due to the simple (linear)\\nglobal structure imposed on the data. For more sophisticated types of models, partic‐\\nularly iterative procedures that respond to local data structure, cross-validation is a\\nvery important tool; see “Cross-Validation” on page 155  for details.\\nWeighted Regression\\nWeighted regression is used by statisticians for a variety of purposes; in particular, it\\nis important for analysis of complex surveys. Data scientists may find weighted\\nregression useful in two cases:\\n•Inverse-variance weighting when different observations have been measured\\nwith different precision; the higher variance ones receiving lower weights.\\n•Analysis of data where rows represent multiple cases; the weight variable encodes\\nhow many original observations each row represents.\\nFor example, with the housing data, older sales are less reliable than more recent\\nsales. Using the DocumentDate  to determine the year of the sale, we can compute a\\nWeight  as the number of years since 2005 (the beginning of the data):\\nR\\nlibrary(lubridate )\\nhouse$Year = year(house$DocumentDate )\\nhouse$Weight = house$Year - 2005\\nMultiple Linear Regression | 159', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 176}),\n",
       " Document(page_content=\"Python\\nhouse['Year'] = [int(date.split('-')[0]) for date in house.DocumentDate ]\\nhouse['Weight' ] = house.Year - 2005\\nWe can compute a weighted regression with the lm function using the weight\\nargument:\\nhouse_wt  <- lm(AdjSalePrice  ~ SqFtTotLiving  + SqFtLot + Bathrooms  +\\n                 Bedrooms  + BldgGrade ,\\n               data=house, weight=Weight)\\nround(cbind(house_lm =house_lm $coefficients ,\\n            house_wt =house_wt $coefficients ), digits=3)\\n                 house_lm     house_wt\\n(Intercept )   -521871.368  -584189.329\\nSqFtTotLiving      228.831     245.024\\nSqFtLot            -0.060      -0.292\\nBathrooms       -19442.840   -26085.970\\nBedrooms        -47769.955   -53608.876\\nBldgGrade       106106.963   115242.435\\nThe coefficients in the weighted regression are slightly different from the original\\nregression.\\nMost models in scikit-learn  accept weights as the keyword argument sam\\nple_weight  in the call of the fit method:\\npredictors  = ['SqFtTotLiving' , 'SqFtLot' , 'Bathrooms' , 'Bedrooms' , 'BldgGrade' ]\\noutcome = 'AdjSalePrice'\\nhouse_wt  = LinearRegression ()\\nhouse_wt .fit(house[predictors ], house[outcome], sample_weight =house.Weight)\\nKey Ideas\\n•Multiple linear regression models the relationship between a response variable Y\\nand multiple predictor variables X1, ...,Xp.\\n•The most important metrics to evaluate a model are root mean squared error\\n(RMSE) and R-squared ( R2).\\n•The standard error of the coefficients can be used to measure the reliability of a\\nvariable’s contribution to a model.\\n•Stepwise regression is a way to automatically determine which variables should\\nbe included in the model.\\n•Weighted regression is used to give certain records more or less weight in fitting\\nthe equation.\\n160 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 177}),\n",
       " Document(page_content='Further Reading\\nAn excellent treatment of cross-validation and resampling can be found in An Intro‐\\nduction to Statistical Learning  by Gareth James, Daniela Witten, Trevor Hastie, and\\nRobert Tibshirani (Springer, 2013).\\nPrediction Using Regression\\nThe primary purpose of regression in data science is prediction.  This is useful to keep\\nin mind, since regression, being an old and established statistical method, comes with\\nbaggage that is more relevant to its traditional role as a tool for explanatory modeling\\nthan to prediction.\\nKey Terms for Prediction Using Regression\\nPrediction interval\\nAn uncertainty interval around an individual predicted value.\\nExtrapolation\\nExtension of a model beyond the range of the data used to fit it.\\nThe Dangers of Extrapolation\\nRegression models should not be used to extrapolate beyond the range of the data\\n(leaving aside the use of regression for time series forecasting.).  The model is valid\\nonly for predictor values for which the data has sufficient values (even in the case that\\nsufficient data is available, there could be other problems—see “Regression Diagnos‐\\ntics” on page 176). As an extreme case, suppose model_lm  is used to predict the value\\nof a 5,000-square-foot empty lot. In such a case, all the predictors related to the build‐\\ning would have a value of 0, and the regression equation would yield an absurd pre‐\\ndiction of –521,900 + 5,000 × –.0605 = –$522,202. Why did this happen? The data\\ncontains only parcels with buildings—there are no records corresponding to vacant\\nland. Consequently, the model has no information to tell it how to predict the sales\\nprice for vacant land.\\nConfidence  and Prediction Intervals\\nMuch of statistics involves understanding and measuring variability (uncertainty).\\nThe t-statistics and p-values reported in regression output deal with this in a formal\\nway, which is sometimes useful for variable selection (see “ Assessing the Model”  on\\npage 153). More useful metrics are confidence intervals, which are uncertainty inter‐\\nvals placed around regression coefficients and predictions.  An easy way to under‐\\nstand this is via the bootstrap (see “The Bootstrap” on page 61 for more details about\\nPrediction Using Regression | 161', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 178}),\n",
       " Document(page_content='the general bootstrap procedure). The most common regression confidence intervals\\nencountered in software output are those for regression parameters (coefficients).\\nHere is a bootstrap algorithm for generating confidence intervals for regression\\nparameters (coefficients) for a data set with P predictors and n records (rows):\\n1.Consider each row (including outcome variable) as a single “ticket” and place all\\nthe n tickets in a box.\\n2.Draw a ticket at random, record the values, and replace it in the box.\\n3.Repeat step 2 n times; you now have one bootstrap resample.\\n4.Fit a regression to the bootstrap sample, and record the estimated coefficients.\\n5.Repeat steps 2 through 4, say, 1,000 times.\\n6.Y ou now have 1,000 bootstrap values for each coefficient; find the appropriate\\npercentiles for each one (e.g., 5th and 95th for a 90% confidence interval).\\nY ou can use the Boot  function in R to generate actual bootstrap confidence intervals\\nfor the coefficients, or you can simply use the formula-based intervals that are a rou‐\\ntine R output. The conceptual meaning and interpretation are the same, and not of\\ncentral importance to data scientists, because they concern the regression coefficients.\\nOf greater interest to data scientists are intervals around predicted y values ( Yi). The\\nuncertainty around Yi comes from two sources:\\n•Uncertainty about what the relevant predictor variables and their coefficients are\\n(see the preceding bootstrap algorithm)\\n•Additional error inherent in individual data points\\nThe individual data point error can be thought of as follows: even if we knew for cer‐\\ntain what the regression equation was (e.g., if we had a huge number of records to fit\\nit), the actual  outcome values for a given set of predictor values will vary. For exam‐\\nple, several houses—each with 8 rooms, a 6,500-square-foot lot, 3 bathrooms, and a\\nbasement—might have different values. We can model this individual error with the\\nresiduals from the fitted values. The bootstrap algorithm for modeling both the\\nregression model error and the individual data point error would look as follows:\\n1.Take a bootstrap sample from the data (spelled out in greater detail earlier).\\n2.Fit the regression, and predict the new value.\\n3.Take a single residual at random from the original regression fit, add it to the\\npredicted value, and record the result.\\n4.Repeat steps 1 through 3, say, 1,000 times.\\n5.Find the 2.5th and the 97.5th percentiles of the results.\\n162 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 179}),\n",
       " Document(page_content='Key Ideas\\n•Extrapolation beyond the range of the data can lead to error.\\n•Confidence intervals quantify uncertainty around regression coefficients.\\n•Prediction intervals quantify uncertainty in individual predictions.\\n•Most software, R included, will produce prediction and confidence intervals in\\ndefault or specified output, using formulas.\\n•The bootstrap can also be used to produce prediction and confidence intervals;\\nthe interpretation and idea are the same.\\nPrediction Interval or Confidence  Interval?\\nA prediction interval pertains to uncertainty around a single value,\\nwhile a confidence interval pertains to a mean or other statistic cal‐\\nculated from multiple values. Thus, a prediction interval will typi‐\\ncally be much wider than a confidence interval for the same value.\\nWe model this individual value error in the bootstrap model by\\nselecting an individual residual to tack on to the predicted value.\\nWhich should you use? That depends on the context and the pur‐\\npose of the analysis, but, in general, data scientists are interested in\\nspecific individual predictions, so a prediction interval would be\\nmore appropriate. Using a confidence interval when you should be\\nusing a prediction interval will greatly underestimate the uncer‐\\ntainty in a given predicted value.\\nFactor Variables in Regression\\nFactor  variables, also termed categorical  variables, take on a limited number of dis‐\\ncrete values. For example, a loan purpose can be “debt consolidation, ” “wedding, ”\\n“car, ” and so on. The binary (yes/no) variable, also called an indicator  variable, is a\\nspecial case of a factor variable. Regression requires numerical inputs, so factor vari‐\\nables need to be recoded to use in the model. The most common approach is to con‐\\nvert a variable into a set of binary dummy  variables.\\nFactor Variables in Regression | 163', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 180}),\n",
       " Document(page_content=\"Key Terms for Factor Variables\\nDummy variables\\nBinary 0–1 variables derived by recoding factor data for use in regression and\\nother models.\\nReference coding\\nThe most common type of coding used by statisticians, in which one level of a\\nfactor is used as a reference and other factors are compared to that level.\\nSynonym\\ntreatment coding\\nOne hot encoder\\nA common type of coding used in the machine learning community in which all\\nfactor levels are retained. While useful for certain machine learning algorithms,\\nthis approach is not appropriate for multiple linear regression.\\nDeviation coding\\nA type of coding that compares each level against the overall mean as opposed to\\nthe reference level.\\nSynonym\\nsum contrasts\\nDummy Variables Representation\\nIn the King County housing data, there is a factor variable for the property type; a\\nsmall subset of six records is shown below:\\nR:\\nhead(house[, 'PropertyType' ])\\nSource: local data frame [6 x 1]\\n   PropertyType\\n         (fctr)\\n1     Multiplex\\n2 Single Family\\n3 Single Family\\n4 Single Family\\n5 Single Family\\n6     Townhouse\\nPython :\\nhouse.PropertyType .head()\\n164 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 181}),\n",
       " Document(page_content=\"3The -1 argument in the model.matrix  produces one hot encoding representation (by removing the intercept,\\nhence the “-”). Otherwise, the default in R is to produce a matrix with P – 1 columns with the first factor level\\nas a reference.There are three possible values: Multiplex , Single Family , and Townhouse . To use\\nthis factor variable, we need to convert it to a set of binary variables. We do this by\\ncreating a binary variable for each possible value of the factor variable. To do this in\\nR, we use the model.matrix  function:3\\nprop_type_dummies  <- model.matrix (~PropertyType  -1, data=house)\\nhead(prop_type_dummies )\\n  PropertyTypeMultiplex  PropertyTypeSingle  Family PropertyTypeTownhouse\\n1                     1                         0                     0\\n2                     0                         1                     0\\n3                     0                         1                     0\\n4                     0                         1                     0\\n5                     0                         1                     0\\n6                     0                         0                     1\\nThe function model.matrix  converts a data frame into a matrix suitable to a linear\\nmodel. The factor variable PropertyType , which has three distinct levels, is repre‐\\nsented as a matrix with three columns. In the machine learning community, this rep‐\\nresentation is referred to as one hot encoding  (see “One Hot Encoder” on page 242 ).\\nIn Python , we can convert categorical variables to dummies using the pandas  method\\nget_dummies :\\npd.get_dummies (house['PropertyType' ]).head() \\npd.get_dummies (house['PropertyType' ], drop_first =True).head() \\nBy default, returns one hot encoding of the categorical variable.\\nThe keyword argument drop_first  will return P – 1 columns. Use this to avoid\\nthe problem of multicollinearity.\\nIn certain machine learning algorithms, such as nearest neighbors and tree models,\\none hot encoding is the standard way to represent factor variables (for example, see\\n“Tree Models” on page 249 ).\\nIn the regression setting, a factor variable with P distinct levels is usually represented\\nby a matrix with only P – 1 columns. This is because a regression model typically\\nincludes an intercept term. With an intercept, once you have defined the values for\\nP – 1 binaries, the value for the Pth is known and could be considered redundant.\\nAdding the Pth column will cause a multicollinearity error (see “Multicollinearity” on\\npage 172 ).\\nFactor Variables in Regression | 165\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 182}),\n",
       " Document(page_content=\"4This is unintuitive, but can be explained by the impact of location as a confounding variable; see “Confound‐\\ning Variables” on page 172 .The default representation in R is to use the first factor level as a reference  and inter‐\\npret the remaining levels relative to that factor:\\nlm(AdjSalePrice  ~ SqFtTotLiving  + SqFtLot + Bathrooms  +\\n       Bedrooms  + BldgGrade  + PropertyType , data=house)\\nCall:\\nlm(formula = AdjSalePrice  ~ SqFtTotLiving  + SqFtLot + Bathrooms  +\\n    Bedrooms  + BldgGrade  + PropertyType , data = house)\\nCoefficients :\\n              (Intercept )              SqFtTotLiving\\n               -4.468e+05                   2.234e+02\\n                  SqFtLot                  Bathrooms\\n               -7.037e-02                  -1.598e+04\\n                 Bedrooms                   BldgGrade\\n               -5.089e+04                   1.094e+05\\nPropertyTypeSingle  Family      PropertyTypeTownhouse\\n               -8.468e+04                  -1.151e+05\\nThe method get_dummies  takes the optional keyword argument drop_first  to\\nexclude the first factor as reference :\\npredictors  = ['SqFtTotLiving' , 'SqFtLot' , 'Bathrooms' , 'Bedrooms' ,\\n              'BldgGrade' , 'PropertyType' ]\\nX = pd.get_dummies (house[predictors ], drop_first =True)\\nhouse_lm_factor  = LinearRegression ()\\nhouse_lm_factor .fit(X, house[outcome])\\nprint(f'Intercept: {house_lm_factor.intercept_:.3f}' )\\nprint('Coefficients:' )\\nfor name, coef in zip(X.columns, house_lm_factor .coef_):\\n    print(f' {name}: {coef}' )\\nThe output from the R regression shows two coefficients corresponding to Property\\nType : PropertyTypeSingle Family  and PropertyTypeTownhouse . There is no coeffi‐\\ncient of Multiplex  since it is implicitly defined when PropertyTypeSingle Family\\n== 0  and PropertyTypeTownhouse == 0 . The coefficients are interpreted as relative\\nto Multiplex , so a home that is Single Family  is worth almost $85,000 less, and a\\nhome that is Townhouse  is worth over $150,000 less.4\\n166 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 183}),\n",
       " Document(page_content=\"Different  Factor Codings\\nThere are several different ways to encode factor variables, known\\nas contrast coding  systems.  For example, deviation coding , also\\nknown as sum contrasts , compares each level against the overall\\nmean.  Another contrast is polynomial coding , which is appropriate\\nfor ordered factors; see the section “Ordered Factor Variables” on\\npage 169 . With the exception of ordered factors, data scientists will\\ngenerally not encounter any type of coding besides reference cod‐\\ning or one hot encoder.\\nFactor Variables with Many Levels\\nSome factor variables can produce a huge number of binary dummies—zip codes are\\na factor variable, and there are 43,000 zip codes in the US.  In such cases, it is useful to\\nexplore the data, and the relationships between predictor variables and the outcome,\\nto determine whether useful information is contained in the categories. If so, you\\nmust further decide whether it is useful to retain all factors, or whether the levels\\nshould be consolidated.\\nIn King County, there are 80 zip codes with a house sale:\\ntable(house$ZipCode)\\n98001 98002 98003 98004 98005 98006 98007 98008 98010 98011 98014 98019\\n  358   180   241   293   133   460   112   291    56   163    85   242\\n98022 98023 98024 98027 98028 98029 98030 98031 98032 98033 98034 98038\\n  188   455    31   366   252   475   263   308   121   517   575   788\\n98039 98040 98042 98043 98045 98047 98050 98051 98052 98053 98055 98056\\n   47   244   641     1   222    48     7    32   614   499   332   402\\n98057 98058 98059 98065 98068 98070 98072 98074 98075 98077 98092 98102\\n    4   420   513   430     1    89   245   502   388   204   289   106\\n98103 98105 98106 98107 98108 98109 98112 98113 98115 98116 98117 98118\\n  671   313   361   296   155   149   357     1   620   364   619   492\\n98119 98122 98125 98126 98133 98136 98144 98146 98148 98155 98166 98168\\n  260   380   409   473   465   310   332   287    40   358   193   332\\n98177 98178 98188 98198 98199 98224 98288 98354\\n  216   266   101   225   393     3     4     9\\nThe value_counts  method of pandas  data frames returns the same information:\\npd.DataFrame (house['ZipCode' ].value_counts ()).transpose ()\\nZipCode  is an important variable, since it is a proxy for the effect of location on the\\nvalue of a house. Including all levels requires 79 coefficients corresponding to 79\\ndegrees of freedom. The original model house_lm  has only 5 degrees of freedom; see\\n“ Assessing the Model” on page 153 . Moreover, several zip codes have only one sale. In\\nsome problems, you can consolidate a zip code using the first two or three digits,\\nFactor Variables in Regression | 167\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 184}),\n",
       " Document(page_content=\"corresponding  to a submetropolitan geographic region. For King County, almost all\\nof the sales occur in 980xx or 981xx, so this doesn’t help.\\nAn alternative approach is to group the zip codes according to another variable, such\\nas sale price. Even better is to form zip code groups using the residuals from an initial\\nmodel. The following dplyr  code in R consolidates the 80 zip codes into five groups\\nbased on the median of the residual from the house_lm  regression:\\nzip_groups  <- house %>%\\n  mutate(resid = residuals (house_lm )) %>%\\n  group_by (ZipCode) %>%\\n  summarize (med_resid  = median(resid),\\n            cnt = n()) %>%\\n  arrange(med_resid ) %>%\\n  mutate(cum_cnt = cumsum(cnt),\\n         ZipGroup  = ntile(cum_cnt, 5))\\nhouse <- house %>%\\n  left_join (select(zip_groups , ZipCode, ZipGroup ), by='ZipCode' )\\nThe median residual is computed for each zip, and the ntile  function is used to split\\nthe zip codes, sorted by the median, into five groups. See “Confounding Variables” on\\npage 172  for an example of how this is used as a term in a regression improving upon\\nthe original fit.\\nIn Python  we can calculate this information as follows:\\nzip_groups  = pd.DataFrame ([\\n    *pd.DataFrame ({\\n        'ZipCode' : house['ZipCode' ],\\n        'residual'  : house[outcome] - house_lm .predict(house[predictors ]),\\n    })\\n    .groupby(['ZipCode' ])\\n    .apply(lambda x: {\\n        'ZipCode' : x.iloc[0,0],\\n        'count': len(x),\\n        'median_residual' : x.residual .median()\\n    })\\n]).sort_values ('median_residual' )\\nzip_groups ['cum_count' ] = np.cumsum(zip_groups ['count'])\\nzip_groups ['ZipGroup' ] = pd.qcut(zip_groups ['cum_count' ], 5, labels=False,\\n                                 retbins=False)\\nto_join = zip_groups [['ZipCode' , 'ZipGroup' ]].set_index ('ZipCode' )\\nhouse = house.join(to_join, on='ZipCode' )\\nhouse['ZipGroup' ] = house['ZipGroup' ].astype('category' )\\nThe concept of using the residuals to help guide the regression fitting is a fundamen‐\\ntal step in the modeling process; see “Regression Diagnostics” on page 176 .\\n168 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 185}),\n",
       " Document(page_content='Ordered Factor Variables\\nSome factor variables reflect levels of a factor; these are termed ordered factor vari‐\\nables  or ordered categorical variables . For example, the loan grade could be A, B, C,\\nand so on—each grade carries more risk than the prior grade. Often, ordered factor\\nvariables can be converted to numerical values and used as is.  For example, the vari‐\\nable BldgGrade  is an ordered factor variable. Several of the types of grades are shown\\nin Table 4-1 . While the grades have specific meaning, the numeric value is ordered\\nfrom low to high, corresponding to higher-grade homes. With the regression model\\nhouse_lm , fit in “Multiple Linear Regression” on page 150 , BldgGrade  was treated as a\\nnumeric variable.\\nTable 4-1. Building grades and numeric equivalents\\nValue Description\\n1 Cabin\\n2 Substandard\\n5 Fair\\n10 Very good\\n12 Luxury\\n13 Mansion\\nTreating ordered factors as a numeric variable preserves the information contained in\\nthe ordering that would be lost if it were converted to a factor.\\nKey Ideas\\n•Factor variables need to be converted into numeric variables for use in a\\nregression.\\n•The most common method to encode a factor variable with P distinct values is to\\nrepresent them using P – 1 dummy variables.\\n•A factor variable with many levels, even in very big data sets, may need to be con‐\\nsolidated into a variable with fewer levels.\\n•Some factors have levels that are ordered and can be represented as a single\\nnumeric variable.\\nInterpreting the Regression Equation\\nIn data science, the most important use of regression is to predict some dependent\\n(outcome) variable. In some cases, however, gaining insight from the equation itself\\nInterpreting the Regression Equation | 169', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 186}),\n",
       " Document(page_content='to understand the nature of the relationship between the predictors and the outcome\\ncan be of value. This section provides guidance on examining the regression equation\\nand interpreting it.\\nKey Terms for Interpreting the Regression Equation\\nCorrelated variables\\nWhen the predictor variables are highly correlated, it is difficult to interpret the\\nindividual coefficients.\\nMulticollinearity\\nWhen the predictor variables have perfect, or near-perfect, correlation, the\\nregression can be unstable or impossible to compute.\\nSynonym\\ncollinearity\\nConfounding variables\\nAn important predictor that, when omitted, leads to spurious relationships in a\\nregression equation.\\nMain effects\\nThe relationship between a predictor and the outcome variable, independent of\\nother variables.\\nInteractions\\nAn interdependent relationship between two or more predictors and the\\nresponse.\\nCorrelated Predictors\\nIn multiple regression, the predictor variables are often correlated with each other.  As\\nan example, examine the regression coefficients for the model step_lm , fit in “Model\\nSelection and Stepwise Regression” on page 156 .\\nR:\\nstep_lm$coefficients\\n              (Intercept )             SqFtTotLiving                  Bathrooms\\n             6.178645e+06               1.992776e+02               4.239616e+04\\n                 Bedrooms                  BldgGrade  PropertyTypeSingle  Family\\n            -5.194738e+04               1.371596e+05               2.291206e+04\\n    PropertyTypeTownhouse            SqFtFinBasement                    YrBuilt\\n             8.447916e+04               7.046975e+00              -3.565425e+03\\n170 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 187}),\n",
       " Document(page_content=\"Python :\\nprint(f'Intercept: {best_model.intercept_:.3f}' )\\nprint('Coefficients:' )\\nfor name, coef in zip(best_variables , best_model .coef_):\\n    print(f' {name}: {coef}' )\\nThe coefficient for Bedrooms  is negative! This implies that adding a bedroom to a\\nhouse will reduce its value. How can this be? This is because the predictor variables\\nare correlated: larger houses tend to have more bedrooms, and it is the size that drives\\nhouse value, not the number of bedrooms. Consider two homes of the exact same\\nsize: it is reasonable to expect that a home with more but smaller bedrooms would be\\nconsidered less desirable.\\nHaving correlated predictors can make it difficult to interpret the sign and value of\\nregression coefficients (and can inflate the standard error of the estimates).  The vari‐\\nables for bedrooms, house size, and number of bathrooms are all correlated. This is\\nillustrated by the following example in R, which fits another regression removing the\\nvariables SqFtTotLiving , SqFtFinBasement , and Bathrooms  from the equation:\\nupdate(step_lm, . ~ . - SqFtTotLiving  - SqFtFinBasement  - Bathrooms )\\nCall:\\nlm(formula = AdjSalePrice  ~ Bedrooms  + BldgGrade  + PropertyType  +\\n    YrBuilt, data = house, na.action  = na.omit)\\nCoefficients :\\n              (Intercept )                   Bedrooms\\n                  4913973                      27151\\n                BldgGrade   PropertyTypeSingle  Family\\n                   248998                     -19898\\n    PropertyTypeTownhouse                     YrBuilt\\n                   -47355                      -3212\\nThe update  function can be used to add or remove variables from a model. Now the\\ncoefficient for bedrooms is positive—in line with what we would expect (though it is\\nreally acting as a proxy for house size, now that those variables have been removed).\\nIn Python , there is no equivalent to R’s update  function. We need to refit the model\\nwith the modified predictor list:\\npredictors  = ['Bedrooms' , 'BldgGrade' , 'PropertyType' , 'YrBuilt' ]\\noutcome = 'AdjSalePrice'\\nX = pd.get_dummies (house[predictors ], drop_first =True)\\nreduced_lm  = LinearRegression ()\\nreduced_lm .fit(X, house[outcome])\\nCorrelated variables are only one issue with interpreting regression coefficients. In\\nhouse_lm , there is no variable to account for the location of the home, and the model\\nInterpreting the Regression Equation | 171\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 188}),\n",
       " Document(page_content='is mixing together very different types of regions. Location may be a confounding\\nvariable; see “Confounding Variables” on page 172  for further discussion.\\nMulticollinearity\\nAn extreme case of correlated variables produces multicollinearity—a condition in\\nwhich there is redundance among the predictor variables. Perfect multicollinearity\\noccurs when one predictor variable can be expressed as a linear combination of oth‐\\ners. Multicollinearity occurs when:\\n•A variable is included multiple times by error.\\n•P dummies, instead of P – 1 dummies, are created from a factor variable (see\\n“Factor Variables in Regression” on page 163 ).\\n•Two variables are nearly perfectly correlated with one another.\\nMulticollinearity in regression must be addressed—variables should be removed until\\nthe multicollinearity is gone. A regression does not have a well-defined solution in\\nthe presence of perfect multicollinearity. Many software packages, including R and\\nPython , automatically handle certain types of multicollinearity. For example, if\\nSqFtTotLiving  is included twice in the regression of the house  data, the results are\\nthe same as for the house_lm  model. In the case of nonperfect multicollinearity, the\\nsoftware may obtain a solution, but the results may be unstable.\\nMulticollinearity is not such a problem for nonlinear regression\\nmethods like trees, clustering, and nearest-neighbors, and in such\\nmethods it may be advisable to retain P dummies (instead of P – 1).\\nThat said, even in those methods, nonredundancy in predictor\\nvariables is still a virtue.\\nConfounding Variables\\nWith correlated variables, the problem is one of commission: including different vari‐\\nables that have a similar predictive relationship with the response.  With confounding\\nvariables , the problem is one of omission: an important variable is not included in the\\nregression equation. Naive interpretation of the equation coefficients can lead to inva‐\\nlid conclusions.\\nTake, for example, the King County regression equation house_lm  from “Example:\\nKing County Housing Data” on page 151. The regression coefficients of SqFtLot ,\\nBathrooms , and Bedrooms  are all negative. The original regression model does not\\ncontain a variable to represent location—a very important predictor of house price.\\n172 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 189}),\n",
       " Document(page_content=\"5There are 80 zip codes in King County, several with just a handful of sales. An alternative to directly using zip\\ncode as a factor variable, ZipGroup  clusters similar zip codes into a single group. See “Factor Variables with\\nMany Levels” on page 167  for details.To model location, include a variable ZipGroup  that categorizes the zip code into one\\nof five groups, from least expensive (1) to most expensive (5):5\\nlm(formula = AdjSalePrice  ~ SqFtTotLiving  + SqFtLot + Bathrooms  +\\n    Bedrooms  + BldgGrade  + PropertyType  + ZipGroup , data = house,\\n    na.action  = na.omit)\\nCoefficients :\\n              (Intercept )              SqFtTotLiving\\n               -6.666e+05                   2.106e+02\\n                  SqFtLot                  Bathrooms\\n                4.550e-01                   5.928e+03\\n                 Bedrooms                   BldgGrade\\n               -4.168e+04                   9.854e+04\\nPropertyTypeSingle  Family      PropertyTypeTownhouse\\n                1.932e+04                  -7.820e+04\\n                ZipGroup2                   ZipGroup3\\n                5.332e+04                   1.163e+05\\n                ZipGroup4                   ZipGroup5\\n                1.784e+05                   3.384e+05\\nThe same model in Python :\\npredictors  = ['SqFtTotLiving' , 'SqFtLot' , 'Bathrooms' , 'Bedrooms' ,\\n              'BldgGrade' , 'PropertyType' , 'ZipGroup' ]\\noutcome = 'AdjSalePrice'\\nX = pd.get_dummies (house[predictors ], drop_first =True)\\nconfounding_lm  = LinearRegression ()\\nconfounding_lm .fit(X, house[outcome])\\nprint(f'Intercept: {confounding_lm.intercept_:.3f}' )\\nprint('Coefficients:' )\\nfor name, coef in zip(X.columns, confounding_lm .coef_):\\n    print(f' {name}: {coef}' )\\nZipGroup  is clearly an important variable: a home in the most expensive zip code\\ngroup is estimated to have a higher sales price by almost $340,000. The coefficients of\\nSqFtLot  and Bathrooms  are now positive, and adding a bathroom increases the sale\\nprice by $5,928.\\nThe coefficient for Bedrooms  is still negative. While this is unintuitive, this is a well-\\nknown phenomenon in real estate. For homes of the same livable area and number of\\nbathrooms, having more and therefore smaller bedrooms is associated with less val‐\\nuable homes.\\nInterpreting the Regression Equation | 173\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 190}),\n",
       " Document(page_content=\"Interactions and Main Effects\\nStatisticians like to distinguish between main effects , or independent variables, and\\nthe interactions  between the main effects. Main effects are what are often referred to\\nas the predictor variables  in the regression equation. An implicit assumption when\\nonly main effects are used in a model is that the relationship between a predictor\\nvariable and the response is independent of the other predictor variables. This is\\noften not the case.\\nFor example, the model fit to the King County Housing Data in “Confounding Vari‐\\nables”  on page 172 includes several variables as main effects, including ZipCode .\\nLocation in real estate is everything, and it is natural to presume that the relationship\\nbetween, say, house size and the sale price depends on location. A big house built in a\\nlow-rent district is not going to retain the same value as a big house built in an expen‐\\nsive area. Y ou include interactions between variables in R using the * operator. For\\nthe King County data, the following fits an interaction between SqFtTotLiving  and\\nZipGroup :\\nlm(formula = AdjSalePrice  ~ SqFtTotLiving  * ZipGroup  + SqFtLot +\\n    Bathrooms  + Bedrooms  + BldgGrade  + PropertyType , data = house,\\n    na.action  = na.omit)\\nCoefficients :\\n              (Intercept )              SqFtTotLiving\\n               -4.853e+05                   1.148e+02\\n                ZipGroup2                   ZipGroup3\\n               -1.113e+04                   2.032e+04\\n                ZipGroup4                   ZipGroup5\\n                2.050e+04                  -1.499e+05\\n                  SqFtLot                  Bathrooms\\n                6.869e-01                  -3.619e+03\\n                 Bedrooms                   BldgGrade\\n               -4.180e+04                   1.047e+05\\nPropertyTypeSingle  Family      PropertyTypeTownhouse\\n                1.357e+04                  -5.884e+04\\n  SqFtTotLiving :ZipGroup2     SqFtTotLiving :ZipGroup3\\n                3.260e+01                   4.178e+01\\n  SqFtTotLiving :ZipGroup4     SqFtTotLiving :ZipGroup5\\n                6.934e+01                   2.267e+02\\nThe resulting model has four new terms: SqFtTotLiving:ZipGroup2 , SqFtTotLiv\\ning:ZipGroup3 , and so on.\\nIn Python , we need to use the statsmodels  package to train linear regression models\\nwith interactions. This package was designed similar to R and allows defining models\\nusing a formula interface:\\nmodel = smf.ols(formula='AdjSalePrice ~ SqFtTotLiving*ZipGroup + SqFtLot + '  +\\n     'Bathrooms + Bedrooms + BldgGrade + PropertyType' , data=house)\\n174 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 191}),\n",
       " Document(page_content='results = model.fit()\\nresults.summary()\\nThe statsmodels  package takes care of categorical variables (e.g., ZipGroup[T.1] ,\\nPropertyType[T.Single Family] ) and interaction terms (e.g., SqFtTotLiv\\ning:ZipGroup[T.1] ).\\nLocation and house size appear to have a strong interaction. For a home in the lowest\\nZipGroup , the slope is the same as the slope for the main effect SqFtTotLiving , which\\nis $118 per square foot (this is because R uses reference  coding for factor variables; see\\n“Factor Variables in Regression” on page 163). For a home in the highest ZipGroup ,\\nthe slope is the sum of the main effect plus SqFtTotLiving:ZipGroup5 , or $115 +\\n$227 = $342 per square foot. In other words, adding a square foot in the most expen‐\\nsive zip code group boosts the predicted sale price by a factor of almost three, com‐\\npared to the average boost from adding a square foot.\\nModel Selection with Interaction Terms\\nIn problems involving many variables, it can be challenging to\\ndecide which interaction terms should be included in the model.\\nSeveral different approaches are commonly taken:\\n•In some problems, prior knowledge and intuition can guide\\nthe choice of which interaction terms to include in the model.\\n•Stepwise selection (see “Model Selection and Stepwise Regres‐\\nsion”  on page 156) can be used to sift through the various\\nmodels.\\n•Penalized regression can automatically fit to a large set of pos‐\\nsible interaction terms.\\n•Perhaps the most common approach is to use tree models , as\\nwell as their descendants, random forest  and gradient boosted\\ntrees . This class of models automatically searches for optimal\\ninteraction terms; see “Tree Models” on page 249 .\\nKey Ideas\\n•Because of correlation between predictors, care must be taken in the interpreta‐\\ntion of the coefficients in multiple linear regression.\\n•Multicollinearity can cause numerical instability in fitting the regression\\nequation.\\n•A confounding variable is an important predictor that is omitted from a model\\nand can lead to a regression equation with spurious relationships.\\nInterpreting the Regression Equation | 175', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 192}),\n",
       " Document(page_content='•An interaction term between two variables is needed if the relationship between\\nthe variables and the response is interdependent.\\nRegression Diagnostics\\nIn explanatory modeling (i.e., in a research context), various steps, in addition to the\\nmetrics mentioned previously (see “ Assessing the Model” on page 153), are taken to\\nassess how well the model fits the data; most are based on analysis of the residuals.\\nThese steps do not directly address predictive accuracy, but they can provide useful\\ninsight in a predictive setting.\\nKey Terms for Regression Diagnostics\\nStandardized residuals\\nResiduals divided by the standard error of the residuals.\\nOutliers\\nRecords (or outcome values) that are distant from the rest of the data (or the pre‐\\ndicted outcome).\\nInfluential  value\\nA value or record whose presence or absence makes a big difference in the\\nregression equation.\\nLeverage\\nThe degree of influence that a single record has on a regression equation.\\nSynonym\\nhat-value\\nNon-normal residuals\\nNon-normally distributed residuals can invalidate some technical requirements\\nof regression but are usually not a concern in data science.\\nHeteroskedasticity\\nWhen some ranges of the outcome experience residuals with higher variance\\n(may indicate a predictor missing from the equation).\\nPartial residual plots\\nA diagnostic plot to illuminate the relationship between the outcome variable\\nand a single predictor.\\nSynonym\\nadded variables plot\\n176 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 193}),\n",
       " Document(page_content=\"Outliers\\nGenerally speaking, an extreme value, also called an outlier , is one that is distant from\\nmost of the other observations. Just as outliers need to be handled for estimates of\\nlocation and variability (see “Estimates of Location” on page 7  and “Estimates of Vari‐\\nability”  on page 13), outliers can cause problems with regression models. In regres‐\\nsion, an outlier is a record whose actual y value is distant from the predicted value.\\nY ou can detect outliers by examining the standardized residual , which is the residual\\ndivided by the standard error of the residuals.\\nThere is no statistical theory that separates outliers from nonoutliers. Rather, there\\nare (arbitrary) rules of thumb for how distant from the bulk of the data an observa‐\\ntion needs to be in order to be called an outlier. For example, with the boxplot, outli‐\\ners are those data points that are too far above or below the box boundaries (see\\n“Percentiles and Boxplots” on page 20), where “too far” = “more than 1.5 times the\\ninterquartile range. ” In regression, the standardized residual is the metric that is typi‐\\ncally used to determine whether a record is classified as an outlier. Standardized\\nresiduals can be interpreted as “the number of standard errors away from the regres‐\\nsion line. ”\\nLet’s fit a regression to the King County house sales data for all sales in zip code 98105\\nin R:\\nhouse_98105  <- house[house$ZipCode == 98105,]\\nlm_98105  <- lm(AdjSalePrice  ~ SqFtTotLiving  + SqFtLot + Bathrooms  +\\n                 Bedrooms  + BldgGrade , data=house_98105 )\\nIn Python :\\nhouse_98105  = house.loc[house['ZipCode' ] == 98105, ]\\npredictors  = ['SqFtTotLiving' , 'SqFtLot' , 'Bathrooms' , 'Bedrooms' , 'BldgGrade' ]\\noutcome = 'AdjSalePrice'\\nhouse_outlier  = sm.OLS(house_98105 [outcome],\\n                       house_98105 [predictors ].assign(const=1))\\nresult_98105  = house_outlier .fit()\\nWe extract the standardized residuals in R using the rstandard  function and obtain\\nthe index of the smallest residual using the order  function:\\nsresid <- rstandard (lm_98105 )\\nidx <- order(sresid)\\nsresid[idx[1]]\\n    20429\\n-4.326732\\nRegression Diagnostics | 177\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 194}),\n",
       " Document(page_content=\"In statsmodels , use OLSInfluence  to analyze the residuals:\\ninfluence  = OLSInfluence (result_98105 )\\nsresiduals  = influence .resid_studentized_internal\\nsresiduals .idxmin(), sresiduals .min()\\nThe biggest overestimate from the model is more than four standard errors above the\\nregression line, corresponding to an overestimate of $757,754. The original data\\nrecord corresponding to this outlier is as follows in R:\\nhouse_98105 [idx[1], c('AdjSalePrice' , 'SqFtTotLiving' , 'SqFtLot' ,\\n              'Bathrooms' , 'Bedrooms' , 'BldgGrade' )]\\nAdjSalePrice  SqFtTotLiving  SqFtLot Bathrooms  Bedrooms  BldgGrade\\n         (dbl)         (int)   (int)     (dbl)    (int)     (int)\\n20429   119748          2900    7276         3        6         7\\nIn Python :\\noutlier = house_98105 .loc[sresiduals .idxmin(), :]\\nprint('AdjSalePrice' , outlier[outcome])\\nprint(outlier[predictors ])\\nIn this case, it appears that there is something wrong with the record: a house of that\\nsize typically sells for much more than $119,748 in that zip code. Figure 4-4  shows an\\nexcerpt from the statutory deed from this sale: it is clear that the sale involved only\\npartial interest in the property. In this case, the outlier corresponds to a sale that is\\nanomalous and should not be included in the regression. Outliers could also be the\\nresult of other problems, such as a “fat-finger” data entry or a mismatch of units (e.g.,\\nreporting a sale in thousands of dollars rather than simply in dollars).\\nFigure 4-4. Statutory warrany deed for the largest negative residual\\nFor big data problems, outliers are generally not a problem in fitting the regression to\\nbe used in predicting new data. However, outliers are central to anomaly detection,\\nwhere finding outliers is the whole point. The outlier could also correspond to a case\\nof fraud or an accidental action. In any case, detecting outliers can be a critical busi‐\\nness need.\\n178 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 195}),\n",
       " Document(page_content='6The term hat-value  comes from the notion of the hat matrix in regression. Multiple linear regression can be\\nexpressed by the formula Y=HY where H is the hat matrix. The hat-values correspond to the diagonal of H.Influential  Values\\nA value whose absence would significantly change the regression equation is termed\\nan influential  observation . In regression, such a value need not be associated with a\\nlarge residual. As an example, consider the regression lines in Figure 4-5 . The solid\\nline corresponds to the regression with all the data, while the dashed line corresponds\\nto the regression with the point in the upper-right corner removed. Clearly, that data\\nvalue has a huge influence on the regression even though it is not associated with a\\nlarge outlier (from the full regression). This data value is considered to have high lev‐\\nerage  on the regression.\\nIn addition to standardized residuals (see “Outliers”  on page 177), statisticians have\\ndeveloped several metrics to determine the influence of a single record on a regres‐\\nsion.  A common measure of leverage is the hat-value ; values above 2P+ 1/n indi‐\\ncate a high-leverage data value.6\\nFigure 4-5. An example of an influential  data point in regression\\nRegression Diagnostics | 179', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 196}),\n",
       " Document(page_content=\"Another metric is Cook’s distance , which defines influence as a combination of lever‐\\nage and residual size. A rule of thumb is that an observation has high influence if\\nCook’s distance exceeds 4/n−P− 1.\\nAn influence  plot  or bubble plot  combines standardized residuals, the hat-value, and\\nCook’s distance in a single plot. Figure 4-6  shows the influence plot for the King\\nCounty house data and can be created by the following R code:\\nstd_resid  <- rstandard (lm_98105 )\\ncooks_D <- cooks.distance (lm_98105 )\\nhat_values  <- hatvalues (lm_98105 )\\nplot(subset(hat_values , cooks_D > 0.08), subset(std_resid , cooks_D > 0.08),\\n     xlab='hat_values' , ylab='std_resid' ,\\n     cex=10*sqrt(subset(cooks_D, cooks_D > 0.08)), pch=16, col='lightgrey' )\\npoints(hat_values , std_resid , cex=10*sqrt(cooks_D))\\nabline(h=c(-2.5, 2.5), lty=2)\\nHere is the Python  code to create a similar figure:\\ninfluence  = OLSInfluence (result_98105 )\\nfig, ax = plt.subplots (figsize=(5, 5))\\nax.axhline(-2.5, linestyle ='--', color='C1')\\nax.axhline(2.5, linestyle ='--', color='C1')\\nax.scatter(influence .hat_matrix_diag , influence .resid_studentized_internal ,\\n           s=1000 * np.sqrt(influence .cooks_distance [0]),\\n           alpha=0.5)\\nax.set_xlabel ('hat values' )\\nax.set_ylabel ('studentized residuals' )\\nThere are apparently several data points that exhibit large influence in the regression.\\nCook’s distance can be computed using the function cooks.distance , and you can\\nuse hatvalues  to compute the diagnostics. The hat values are plotted on the x-axis,\\nthe residuals are plotted on the y-axis, and the size of the points is related to the value\\nof Cook’s distance.\\n180 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 197}),\n",
       " Document(page_content='7The coefficient for Bathrooms  becomes negative, which is unintuitive. Location has not been taken into\\naccount, and the zip code 98105 contains areas of disparate types of homes. See “Confounding Variables” on\\npage 172  for a discussion of confounding variables.\\nFigure 4-6. A plot to determine which observations have high influence;  points with\\nCook’s distance greater than 0.08 are highlighted in grey\\nTable 4-2  compares the regression with the full data set and with highly influential\\ndata points removed (Cook’s distance > 0.08).\\nThe regression coefficient for Bathrooms  changes quite dramatically.7\\nTable 4-2. Comparison of regression coefficients  with the full data and with influential  data\\nremoved\\nOriginal Influential  removed\\n(Intercept) –772,550 –647,137\\nSqFtTotLiving 210 230\\nSqFtLot 39 33\\nBathrooms 2282 –16,132\\nBedrooms –26,320 –22,888\\nBldgGrade 130,000 114,871\\nRegression Diagnostics | 181', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 198}),\n",
       " Document(page_content=\"For purposes of fitting a regression that reliably predicts future data, identifying\\ninfluential observations is useful only in smaller data sets. For regressions involving\\nmany records, it is unlikely that any one observation will carry sufficient weight to\\ncause extreme influence on the fitted equation (although the regression may still have\\nbig outliers). For purposes of anomaly detection, though, identifying influential\\nobservations can be very useful.\\nHeteroskedasticity, Non-Normality, and Correlated Errors\\nStatisticians pay considerable attention to the distribution of the residuals.  It turns\\nout that ordinary least squares (see “Least Squares” on page 148) are unbiased, and in\\nsome cases are the “optimal” estimator, under a wide range of distributional assump‐\\ntions. This means that in most problems, data scientists do not need to be too con‐\\ncerned with the distribution of the residuals.\\nThe distribution of the residuals is relevant mainly for the validity of formal statistical\\ninference (hypothesis tests and p-values), which is of minimal importance to data sci‐\\nentists concerned mainly with predictive accuracy. Normally distributed errors are a\\nsign that the model is complete; errors that are not normally distributed indicate the\\nmodel may be missing something. For formal inference to be fully valid, the residuals\\nare assumed to be normally distributed, have the same variance, and be independent.\\nOne area where this may be of concern to data scientists is the standard calculation of\\nconfidence intervals for predicted values, which are based upon the assumptions\\nabout the residuals (see “Confidence and Prediction Intervals” on page 161 ).\\nHeteroskedasticity  is the lack of constant residual variance across the range of the pre‐\\ndicted values. In other words, errors are greater for some portions of the range than\\nfor others. Visualizing the data is a convenient way to analyze residuals.\\nThe following code in R plots the absolute residuals versus the predicted values for\\nthe lm_98105  regression fit in “Outliers” on page 177 :\\ndf <- data.frame (resid = residuals (lm_98105 ), pred = predict(lm_98105 ))\\nggplot(df, aes(pred, abs(resid))) + geom_point () + geom_smooth ()\\nFigure 4-7  shows the resulting plot. Using geom_smooth , it is easy to superpose a\\nsmooth of the absolute residuals. The function calls the loess  method (locally esti‐\\nmated scatterplot smoothing) to produce a smoothed estimate of the relationship\\nbetween the variables on the x-axis and y-axis in a scatterplot (see “Scatterplot\\nSmoothers” on page 185 ).\\nIn Python , the seaborn  package has the regplot  function to create a similar figure:\\nfig, ax = plt.subplots (figsize=(5, 5))\\nsns.regplot(result_98105 .fittedvalues , np.abs(result_98105 .resid),\\n            scatter_kws ={'alpha': 0.25}, line_kws ={'color': 'C1'},\\n            lowess=True, ax=ax)\\n182 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 199}),\n",
       " Document(page_content=\"ax.set_xlabel ('predicted' )\\nax.set_ylabel ('abs(residual)' )\\nFigure 4-7. A plot of the absolute value of the residuals versus the predicted values\\nEvidently, the variance of the residuals tends to increase for higher-valued homes but\\nis also large for lower-valued homes. This plot indicates that lm_98105  has heteroske‐\\ndastic  errors.\\nWhy Would a Data Scientist Care About Heteroskedasticity?\\nHeteroskedasticity indicates that prediction errors differ for differ‐\\nent ranges of the predicted value, and may suggest an incomplete\\nmodel.  For example, the heteroskedasticity in lm_98105  may indi‐\\ncate that the regression has left something unaccounted for in high-\\nand low-range homes.\\nFigure 4-8  is a histogram of the standardized residuals for the lm_98105  regression.\\nThe distribution has decidedly longer tails than the normal distribution and exhibits\\nmild skewness toward larger residuals.\\nRegression Diagnostics | 183\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 200}),\n",
       " Document(page_content='Figure 4-8. A histogram of the residuals from the regression of the housing data\\nStatisticians may also check the assumption that the errors are independent. This is\\nparticularly true for data that is collected over time or space. The Durbin-Watson  sta‐\\ntistic can be used to detect if there is significant autocorrelation in a regression\\ninvolving time series data. If the errors from a regression model are correlated, then\\nthis information can be useful in making short-term forecasts and should be built\\ninto the model. See Practical Time Series Forecasting with R , 2nd ed., by Galit Shmueli\\nand Kenneth Lichtendahl (Axelrod Schnall, 2018) to learn more about how to build\\nautocorrelation information into regression models for time series data. If longer-\\nterm forecasts or explanatory models are the goal, excess autocorrelated data at the\\nmicrolevel may distract. In that case, smoothing, or less granular collection of data in\\nthe first place, may be in order.\\nEven though a regression may violate one of the distributional assumptions, should\\nwe care? Most often in data science, the interest is primarily in predictive accuracy, so\\nsome review of heteroskedasticity may be in order. Y ou may discover that there is\\nsome signal in the data that your model has not captured. However, satisfying distri‐\\nbutional assumptions simply for the sake of validating formal statistical inference (p-\\nvalues, F-statistics, etc.) is not that important for the data scientist.\\n184 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 201}),\n",
       " Document(page_content=\"Scatterplot Smoothers\\nRegression is about modeling the relationship between the\\nresponse and predictor variables. In evaluating a regression model,\\nit is useful to use a scatterplot smoother  to visually highlight rela‐\\ntionships between two variables.\\nFor example, in Figure 4-7 , a smooth of the relationship between\\nthe absolute residuals and the predicted value shows that the var‐\\niance of the residuals depends on the value of the residual. In this\\ncase, the loess  function was used; loess  works by repeatedly fit‐\\nting a series of local regressions to contiguous subsets to come up\\nwith a smooth. While loess  is probably the most commonly used\\nsmoother, other scatterplot smoothers are available in R, such as\\nsuper smooth ( supsmu ) and kernel smoothing ( ksmooth ). In\\nPython , we can find additional smoothers in scipy  (wiener  or sav)\\nand statsmodels  (kernel_regression ). For the purposes of evalu‐\\nating a regression model, there is typically no need to worry about\\nthe details of these scatterplot smooths.\\nPartial Residual Plots and Nonlinearity\\nPartial residual plots  are a way to visualize how well the estimated fit explains the rela‐\\ntionship between a predictor and the outcome. The basic idea of a partial residual\\nplot is to isolate the relationship between a predictor variable and the response, taking\\ninto account all of the other predictor variables . A partial residual might be thought of\\nas a “synthetic outcome” value, combining the prediction based on a single predictor\\nwith the actual residual from the full regression equation. A partial residual for pre‐\\ndictor Xi is the ordinary residual plus the regression term associated with Xi:\\nPartial residual = Residual + biXi\\nwhere bi is the estimated regression coefficient. The predict  function in R has an\\noption to return the individual regression terms biXi:\\nterms <- predict(lm_98105 , type='terms')\\npartial_resid  <- resid(lm_98105 ) + terms\\nThe partial residual plot displays the Xi predictor on the x-axis and the partial residu‐\\nals on the y-axis. Using ggplot2  makes it easy to superpose a smooth of the partial\\nresiduals:\\ndf <- data.frame (SqFtTotLiving  = house_98105 [, 'SqFtTotLiving' ],\\n                 Terms = terms[, 'SqFtTotLiving' ],\\n                 PartialResid  = partial_resid [, 'SqFtTotLiving' ])\\nggplot(df, aes(SqFtTotLiving , PartialResid )) +\\nRegression Diagnostics | 185\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 202}),\n",
       " Document(page_content=\"geom_point (shape=1) + scale_shape (solid = FALSE) +\\n  geom_smooth (linetype =2) +\\n  geom_line (aes(SqFtTotLiving , Terms))\\nThe statsmodels  package has the method sm.graphics.plot_ccpr  that creates a\\nsimilar partial residual plot:\\nsm.graphics .plot_ccpr (result_98105 , 'SqFtTotLiving' )\\nThe R and Python  graphs differ by a constant shift. In R, a constant is added so that\\nthe mean of the terms is zero.\\nThe resulting plot is shown in Figure 4-9 . The partial residual is an estimate of the\\ncontribution that SqFtTotLiving  adds to the sales price. The relationship between\\nSqFtTotLiving  and the sales price is evidently nonlinear (dashed line). The regres‐\\nsion line (solid line) underestimates the sales price for homes less than 1,000 square\\nfeet and overestimates the price for homes between 2,000 and 3,000 square feet.\\nThere are too few data points above 4,000 square feet to draw conclusions for those\\nhomes.\\nFigure 4-9. A partial residual plot for the variable SqFtTotLiving\\n186 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 203}),\n",
       " Document(page_content='This nonlinearity makes sense in this case: adding 500 feet in a small home makes a\\nmuch bigger difference than adding 500 feet in a large home. This suggests that,\\ninstead of a simple linear term for SqFtTotLiving , a nonlinear term should be con‐\\nsidered (see “Polynomial and Spline Regression” on page 187 ).\\nKey Ideas\\n•While outliers can cause problems for small data sets, the primary interest with\\noutliers is to identify problems with the data, or locate anomalies.\\n•Single records (including regression outliers) can have a big influence on a\\nregression equation with small data, but this effect washes out in big data.\\n•If the regression model is used for formal inference (p-values and the like), then\\ncertain assumptions about the distribution of the residuals should be checked. In\\ngeneral, however, the distribution of residuals is not critical in data science.\\n•The partial residuals plot can be used to qualitatively assess the fit for each\\nregression term, possibly leading to alternative model specification.\\nPolynomial and Spline Regression\\nThe relationship between the response and a predictor variable isn’t necessarily linear.\\nThe response to the dose of a drug is often nonlinear: doubling the dosage generally\\ndoesn’t lead to a doubled response. The demand for a product isn’t a linear function\\nof marketing dollars spent; at some point, demand is likely to be saturated. There are\\nmany ways that regression can be extended to capture these nonlinear effects.\\nKey Terms for Nonlinear Regression\\nPolynomial regression\\nAdds polynomial terms (squares, cubes, etc.) to a regression.\\nSpline regression\\nFitting a smooth curve with a series of polynomial segments.\\nKnots\\nValues that separate spline segments.\\nGeneralized additive models\\nSpline models with automated selection of knots.\\nSynonym\\nGAM\\nPolynomial and Spline Regression | 187', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 204}),\n",
       " Document(page_content=\"Nonlinear Regression\\nWhen statisticians talk about nonlinear regression , they are refer‐\\nring to models that can’t be fit using least squares. What kind of\\nmodels are nonlinear? Essentially all models where the response\\ncannot be expressed as a linear combination of the predictors or\\nsome transform of the predictors. Nonlinear regression models are\\nharder and computationally more intensive to fit, since they\\nrequire numerical optimization. For this reason, it is generally pre‐\\nferred to use a linear model if possible.\\nPolynomial\\nPolynomial regression  involves including polynomial terms in a regression equation.\\nThe use of polynomial regression dates back almost to the development of regression\\nitself with a paper by Gergonne in 1815. For example, a quadratic regression between\\nthe response Y and the predictor X would take the form:\\nY=b0+b1X+b2X2+e\\nPolynomial regression can be fit in R through the poly  function. For example, the fol‐\\nlowing fits a quadratic polynomial for SqFtTotLiving  with the King County housing\\ndata:\\nlm(AdjSalePrice  ~  poly(SqFtTotLiving , 2) + SqFtLot +\\n                BldgGrade  + Bathrooms  + Bedrooms ,\\n                    data=house_98105 )\\nCall:\\nlm(formula = AdjSalePrice  ~ poly(SqFtTotLiving , 2) + SqFtLot +\\n   BldgGrade  + Bathrooms  + Bedrooms , data = house_98105 )\\nCoefficients :\\n           (Intercept )  poly(SqFtTotLiving , 2)1  poly(SqFtTotLiving , 2)2\\n            -402530.47                3271519.49                 776934.02\\n               SqFtLot                BldgGrade                 Bathrooms\\n                 32.56                135717.06                  -1435.12\\n              Bedrooms\\n              -9191.94\\nIn statsmodels , we add the squared term to the model definition using I(SqFtTot\\nLiving**2) :\\nmodel_poly  = smf.ols(formula='AdjSalePrice ~  SqFtTotLiving + '  +\\n                '+ I(SqFtTotLiving**2) + '  +\\n                'SqFtLot + Bathrooms + Bedrooms + BldgGrade' , data=house_98105 )\\nresult_poly  = model_poly .fit()\\nresult_poly .summary()  \\n188 | Chapter 4: Regression and Prediction\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 205}),\n",
       " Document(page_content='The intercept and the polynomial coefficients are different compared to R. This is\\ndue to different implementations. The remaining coefficients and the predictions\\nare equivalent.\\nThere are now two coefficients associated with SqFtTotLiving : one for the linear\\nterm and one for the quadratic term.\\nThe partial residual plot (see “Partial Residual Plots and Nonlinearity” on page 185)\\nindicates some curvature in the regression equation associated with SqFtTotLiving .\\nThe fitted line more closely matches the smooth (see “Splines”  on page 189) of the\\npartial residuals as compared to a linear fit (see Figure 4-10 ).\\nThe statsmodels  implementation works only for linear terms. The accompanying\\nsource code gives an implementation that will work for polynomial regression as well.\\nFigure 4-10. A polynomial regression fit for the variable SqFtTotLiving  (solid line) ver‐\\nsus a smooth (dashed line; see the following section about splines)\\nSplines\\nPolynomial regression captures only a certain amount of curvature in a nonlinear\\nrelationship. Adding in higher-order terms, such as a cubic quartic polynomial, often\\nPolynomial and Spline Regression | 189', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 206}),\n",
       " Document(page_content='leads to undesirable “wiggliness” in the regression equation. An alternative, and often\\nsuperior, approach to modeling nonlinear relationships is to use splines . Splines  pro‐\\nvide a way to smoothly interpolate between fixed points. Splines were originally used\\nby draftsmen to draw a smooth curve, particularly in ship and aircraft building.\\nThe splines were created by bending a thin piece of wood using weights, referred to as\\n“ducks”; see Figure 4-11 .\\nFigure 4-11. Splines were originally created using bendable wood and “ducks” and were\\nused as a draftsman’s  tool to fit curves (photo courtesy of Bob Perry)\\nThe technical definition of a spline is a series of piecewise continuous polynomials.\\nThey were first developed during World War II at the US Aberdeen Proving Grounds\\nby I. J. Schoenberg, a Romanian mathematician. The polynomial pieces are smoothly\\nconnected at a series of fixed points in a predictor variable, referred to as knots . For‐\\nmulation of splines is much more complicated than polynomial regression; statistical\\nsoftware usually handles the details of fitting a spline. The R package splines\\nincludes the function bs to create a b-spline  (basis spline) term in a regression model.\\nFor example, the following adds a b-spline term to the house regression model:\\nlibrary(splines)\\nknots <- quantile (house_98105 $SqFtTotLiving , p=c(.25, .5, .75))\\nlm_spline  <- lm(AdjSalePrice  ~ bs(SqFtTotLiving , knots=knots, degree=3) +\\n  SqFtLot + Bathrooms  + Bedrooms  + BldgGrade ,  data=house_98105 )\\nTwo parameters need to be specified: the degree of the polynomial and the location of\\nthe knots. In this case, the predictor SqFtTotLiving  is included in the model using a\\ncubic spline ( degree=3 ). By default, bs places knots at the boundaries; in addition,\\nknots were also placed at the lower quartile, the median quartile, and the upper\\nquartile.\\nThe statsmodels  formula interface supports the use of splines in a similar way to R.\\nHere, we specify the b-spline  using df, the degrees of freedom. This will create df –\\ndegree  = 6 – 3 = 3 internal knots with positions calculated in the same way as in the R\\ncode above:\\n190 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 207}),\n",
       " Document(page_content=\"formula = 'AdjSalePrice ~ bs(SqFtTotLiving, df=6, degree=3) + '  +\\n          'SqFtLot + Bathrooms + Bedrooms + BldgGrade'\\nmodel_spline  = smf.ols(formula=formula, data=house_98105 )\\nresult_spline  = model_spline .fit()\\nIn contrast to a linear term, for which the coefficient has a direct meaning, the coeffi‐\\ncients for a spline term are not interpretable. Instead, it is more useful to use the vis‐\\nual display to reveal the nature of the spline fit. Figure 4-12  displays the partial\\nresidual plot from the regression. In contrast to the polynomial model, the spline\\nmodel more closely matches the smooth, demonstrating the greater flexibility of\\nsplines. In this case, the line more closely fits the data. Does this mean the spline\\nregression is a better model? Not necessarily: it doesn’t make economic sense that\\nvery small homes (less than 1,000 square feet) would have higher value than slightly\\nlarger homes. This is possibly an artifact of a confounding variable; see “Confounding\\nVariables” on page 172 .\\nFigure 4-12. A spline regression fit for the variable SqFtTotLiving  (solid line) compared\\nto a smooth (dashed line)\\nPolynomial and Spline Regression | 191\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 208}),\n",
       " Document(page_content='Generalized Additive Models\\nSuppose you suspect a nonlinear relationship between the response and a predictor\\nvariable, either by a priori knowledge or by examining the regression diagnostics.\\nPolynomial terms may not be flexible enough to capture the relationship, and spline\\nterms require specifying the knots. Generalized additive models , or GAM , are a flexi‐\\nble modeling technique that can be used to automatically fit a spline regression. The\\nmgcv  package in R can be used to fit a GAM model to the housing data:\\nlibrary(mgcv)\\nlm_gam <- gam(AdjSalePrice  ~ s(SqFtTotLiving ) + SqFtLot +\\n                    Bathrooms  +  Bedrooms  + BldgGrade ,\\n                    data=house_98105 )\\nThe term s(SqFtTotLiving)  tells the gam function to find the “best” knots for a\\nspline term (see Figure 4-13 ).\\nFigure 4-13. A GAM regression fit for the variable SqFtTotLiving  (solid line) compared\\nto a smooth (dashed line)\\nIn Python , we can use the pyGAM  package. It provides methods for regression and clas‐\\nsification. Here, we use LinearGAM  to create a regression model:\\n192 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 209}),\n",
       " Document(page_content=\"predictors  = ['SqFtTotLiving' , 'SqFtLot' , 'Bathrooms' ,  'Bedrooms' , 'BldgGrade' ]\\noutcome = 'AdjSalePrice'\\nX = house_98105 [predictors ].values\\ny = house_98105 [outcome]\\ngam = LinearGAM (s(0, n_splines =12) + l(1) + l(2) + l(3) + l(4))  \\ngam.gridsearch (X, y)\\nThe default value for n_splines  is 20. This leads to overfitting for larger SqFtTot\\nLiving  values. A value of 12 leads to a more reasonable fit.\\nKey Ideas\\n•Outliers in a regression are records with a large residual.\\n•Multicollinearity can cause numerical instability in fitting the regression\\nequation.\\n•A confounding variable is an important predictor that is omitted from a model\\nand can lead to a regression equation with spurious relationships.\\n•An interaction term between two variables is needed if the effect of one variable\\ndepends on the level or magnitude of the other.\\n•Polynomial regression can fit nonlinear relationships between predictors and the\\noutcome variable.\\n•Splines are series of polynomial segments strung together, joining at knots.\\n•We can automate the process of specifying the knots in splines using generalized\\nadditive models (GAM).\\nFurther Reading\\n•For more on spline models and GAMs, see The Elements of Statistical Learning ,\\n2nd ed., by Trevor Hastie, Robert Tibshirani, and Jerome Friedman (2009), and\\nits shorter cousin based on R, An Introduction to Statistical Learning  by Gareth\\nJames, Daniela Witten, Trevor Hastie, and Robert Tibshirani (2013); both are\\nSpringer books.\\n•To learn more about using regression models for time series forecasting, see\\nPractical Time Series Forecasting with R  by Galit Shmueli and Kenneth Lichten‐\\ndahl (Axelrod Schnall, 2018).\\nPolynomial and Spline Regression | 193\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 210}),\n",
       " Document(page_content='Summary\\nPerhaps no other statistical method has seen greater use over the years than regres‐\\nsion—the process of establishing a relationship between multiple predictor variables\\nand an outcome variable. The fundamental form is linear: each predictor variable has\\na coefficient that describes a linear relationship between the predictor and the out‐\\ncome. More advanced forms of regression, such as polynomial and spline regression,\\npermit the relationship to be nonlinear. In classical statistics, the emphasis is on find‐\\ning a good fit to the observed data to explain or describe some phenomenon, and the\\nstrength of this fit is how traditional in-sample  metrics are used to assess the model.\\nIn data science, by contrast, the goal is typically to predict values for new data, so\\nmetrics based on predictive accuracy for out-of-sample data are used. Variable selec‐\\ntion methods are used to reduce dimensionality and create more compact models.\\n194 | Chapter 4: Regression and Prediction', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 211}),\n",
       " Document(page_content='CHAPTER 5\\nClassification\\nData scientists are often tasked with automating decisions for business problems. Is\\nan email an attempt at phishing? Is a customer likely to churn? Is the web user likely\\nto click on an advertisement? These are all classification  problems, a form of super‐\\nvised learning  in which we first train a model on data where the outcome is known\\nand then apply the model to data where the outcome is not known. Classification is\\nperhaps the most important form of prediction: the goal is to predict whether a\\nrecord is a 1 or a 0 (phishing/not-phishing, click/don’t click, churn/don’t churn), or in\\nsome cases, one of several categories (for example, Gmail’s filtering of your inbox into\\n“primary, ” “social, ” “promotional, ” or “forums”).\\nOften, we need more than a simple binary classification: we want to know the predic‐\\nted probability that a case belongs to a class. Rather than having a model simply\\nassign a binary classification, most algorithms can return a probability score (propen‐\\nsity) of belonging to the class of interest. In fact, with logistic regression, the default\\noutput from R is on the log-odds scale, and this must be transformed to a propensity.\\nIn Python ’s scikit-learn , logistic regression, like most classification methods, pro‐\\nvides two prediction methods: predict  (which returns the class) and predict_proba\\n(which returns probabilities for each class). A sliding cutoff can then be used to con‐\\nvert the propensity score to a decision. The general approach is as follows:\\n1.Establish a cutoff probability for the class of interest, above which we consider a\\nrecord as belonging to that class.\\n2.Estimate (with any model) the probability that a record belongs to the class of\\ninterest.\\n3.If that probability is above the cutoff probability, assign the new record to the\\nclass of interest.\\n195', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 212}),\n",
       " Document(page_content='1This and subsequent sections in this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter\\nGedeck; used with permission.The higher the cutoff, the fewer the records predicted as 1—that is, as belonging to\\nthe class of interest. The lower the cutoff, the more the records predicted as 1.\\nThis chapter covers several key techniques for classification and estimating propensi‐\\nties; additional methods that can be used both for classification and for numerical\\nprediction are described in the next chapter.\\nMore Than Two Categories?\\nThe vast majority of problems involve a binary response. Some classification prob‐\\nlems, however, involve a response with more than two possible outcomes.  For exam‐\\nple, at the anniversary of a customer’s subscription contract, there might be three\\noutcomes: the customer leaves or “churns” ( Y = 2), goes on a month-to-month con‐\\ntract ( Y = 1), or signs a new long-term contract ( Y = 0). The goal is to predict Y = j  for\\nj = 0, 1, or 2. Most of the classification methods in this chapter can be applied, either\\ndirectly or with modest adaptations, to responses that have more than two outcomes.\\nEven in the case of more than two outcomes, the problem can often be recast into a\\nseries of binary problems using conditional probabilities. For example, to predict the\\noutcome of the contract, you can solve two binary prediction problems:\\n•Predict whether Y = 0 or Y > 0.\\n•Given that Y > 0, predict whether Y = 1 or Y = 2.\\nIn this case, it makes sense to break up the problem into two cases: (1) whether the\\ncustomer churns; and (2) if they don’t churn, what type of contract they will choose.\\nFrom a model-fitting viewpoint, it is often advantageous to convert the multiclass\\nproblem to a series of binary problems. This is particularly true when one category is\\nmuch more common than the other categories.\\nNaive Bayes\\nThe naive Bayes algorithm uses the probability of observing predictor values, given\\nan outcome, to estimate what is really of interest: the probability of observing out‐\\ncome Y = i , given a set of predictor values.1\\n196 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 213}),\n",
       " Document(page_content='Key Terms for Naive Bayes\\nConditional probability\\nThe probability of observing some event (say, X = i ) given some other event (say,\\nY = i ), written as PXiYi.\\nPosterior probability\\nThe probability of an outcome after the predictor information has been incorpo‐\\nrated (in contrast to the prior probability  of outcomes, not taking predictor infor‐\\nmation into account).\\nTo understand naive Bayesian classification, we can start out by imagining complete\\nor exact Bayesian classification. For each record to be classified:\\n1.Find all the other records with the same predictor profile (i.e., where the predic‐\\ntor values are the same).\\n2.Determine what classes those records belong to and which class is most prevalent\\n(i.e., probable).\\n3.Assign that class to the new record.\\nThe preceding approach amounts to finding all the records in the sample that are\\nexactly like the new record to be classified in the sense that all the predictor values are\\nidentical.\\nPredictor variables must be categorical (factor) variables in the\\nstandard naive Bayes algorithm.  See “Numeric Predictor Variables”\\non page 200  for two workarounds for using continuous variables.\\nWhy Exact Bayesian Classification  Is Impractical\\nWhen the number of predictor variables exceeds a handful, many of the records to be\\nclassified will be without exact matches. Consider a model to predict voting on the\\nbasis of demographic variables. Even a sizable sample may not contain even a single\\nmatch for a new record who is a male Hispanic with high income from the US Mid‐\\nwest who voted in the last election, did not vote in the prior election, has three\\ndaughters and one son, and is divorced. And this is with just eight variables, a small\\nnumber for most classification problems. The addition of just a single new variable\\nwith five equally frequent categories reduces the probability of a match by a factor\\nof 5.\\nNaive Bayes | 197', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 214}),\n",
       " Document(page_content='The Naive Solution\\nIn the naive Bayes solution, we no longer restrict the probability calculation to those\\nrecords that match the record to be classified. Instead, we use the entire data set. The\\nnaive Bayes modification is as follows:\\n1.For a binary response Y = i  (i = 0 or 1), estimate the individual conditional prob‐\\nabilities for each predictor PXjY=i; these are the probabilities that the pre‐\\ndictor value is in the record when we observe Y = i . This probability is estimated\\nby the proportion of Xj values among the Y = i  records in the training set.\\n2.Multiply these probabilities by each other, and then by the proportion of records\\nbelonging to Y = i .\\n3.Repeat steps 1 and 2 for all the classes.\\n4.Estimate a probability for outcome i by taking the value calculated in step 2 for\\nclass i and dividing it by the sum of such values for all classes.\\n5.Assign the record to the class with the highest probability for this set of predictor\\nvalues.\\nThis naive Bayes algorithm can also be stated as an equation for the probability of\\nobserving outcome Y = i , given a set of predictor values X1,⋯,Xp:\\nP(Y=i|X1,X2, …, Xp)\\nHere is the full formula for calculating class probabilities using exact Bayes\\nclassification:\\nP(Y=i|X1,X2, …, Xp) =P(Y=i)P(X1, …, Xp|Y=i)\\nP(Y= 0)P(X1, …, Xp|Y= 0) + P(Y= 1)P(X1, …, Xp|Y= 1)\\nUnder the naive Bayes assumption of conditional independence, this equation\\nchanges into:\\nP(Y=i|X1,X2, …, Xp)\\n=P(Y=i)P(X1|Y=i)…P(Xp|Y=i)\\nP(Y= 0)P(X1|Y= 0)… P(Xp|Y= 0) + P(Y= 1)P(X1|Y= 1)… P(Xp|Y= 1)\\nWhy is this formula called “naive”? We have made a simplifying assumption that the\\nexact conditional probability  of a vector of predictor values, given observing an\\n198 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 215}),\n",
       " Document(page_content=\"outcome , is sufficiently well estimated by the product of the individual conditional\\nprobabilities PXjY=i. In other words, in estimating PXjY=i instead of\\nPX1,X2,⋯XpY=i, we are assuming Xj is independent  of all the other predictor\\nvariables Xk for k≠j.\\nSeveral packages in R can be used to estimate a naive Bayes model. The following fits\\na model to the loan payment data using the klaR  package:\\nlibrary(klaR)\\nnaive_model  <- NaiveBayes (outcome ~ purpose_  + home_ + emp_len_ ,\\n                          data = na.omit(loan_data ))\\nnaive_model $table\\n$purpose_\\n          var\\ngrouping    credit_card  debt_consolidation  home_improvement  major_purchase\\n  paid off  0.18759649          0.55215915        0.07150104      0.05359270\\n  default   0.15151515          0.57571347        0.05981209      0.03727229\\n          var\\ngrouping       medical      other small_business\\n  paid off 0.01424728  0.09990737      0.02099599\\n  default  0.01433549  0.11561025      0.04574126\\n$home_\\n          var\\ngrouping     MORTGAGE        OWN      RENT\\n  paid off 0.4894800  0.0808963  0.4296237\\n  default  0.4313440  0.0832782  0.4853778\\n$emp_len_\\n          var\\ngrouping      < 1 Year   > 1 Year\\n  paid off 0.03105289  0.96894711\\n  default  0.04728508  0.95271492\\nThe output from the model is the conditional probabilities PXjY=i.\\nIn Python  we can use sklearn.naive_bayes.MultinomialNB  from scikit-learn . We\\nneed to convert the categorical features to dummy variables before we fit the model:\\npredictors  = ['purpose_' , 'home_', 'emp_len_' ]\\noutcome = 'outcome'\\nX = pd.get_dummies (loan_data [predictors ], prefix='', prefix_sep ='')\\ny = loan_data [outcome]\\nnaive_model  = MultinomialNB (alpha=0.01, fit_prior =True)\\nnaive_model .fit(X, y)\\nIt is possible to derive the conditional probabilities from the fitted model using the\\nproperty feature_log_prob_ .\\nNaive Bayes | 199\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 216}),\n",
       " Document(page_content=\"The model can be used to predict the outcome of a new loan. We use the last value of\\nthe data set for testing:\\nnew_loan  <- loan_data [147, c('purpose_' , 'home_', 'emp_len_' )]\\nrow.names (new_loan ) <- NULL\\nnew_loan\\n         purpose_     home_  emp_len_\\n 1 small_business  MORTGAGE   > 1 Year\\nIn Python , we get this value as follows:\\nnew_loan  = X.loc[146:146, :]\\nIn this case, the model predicts a default ( R):\\npredict(naive_model , new_loan )\\n$class\\n[1] default\\nLevels: paid off default\\n$posterior\\n      paid off   default\\n[1,] 0.3463013  0.6536987\\nAs we discussed, scikit-learn ’s classification models have two methods— predict ,\\nwhich returns the predicted class, and predict_proba , which returns the class\\nprobabilities:\\nprint('predicted class: ' , naive_model .predict(new_loan )[0])\\nprobabilities  = pd.DataFrame (naive_model .predict_proba (new_loan ),\\n                             columns=loan_data [outcome].cat.categories )\\nprint('predicted probabilities' , probabilities )\\n--\\npredicted  class:  default\\npredicted  probabilities\\n    default  paid off\\n0  0.653696   0.346304\\nThe prediction also returns a posterior  estimate of the probability of default.  The\\nnaive Bayesian classifier is known to produce biased  estimates. However, where the\\ngoal is to rank  records according to the probability that Y = 1, unbiased estimates of\\nprobability are not needed, and naive Bayes produces good results.\\nNumeric Predictor Variables\\nThe Bayesian classifier works only with categorical predictors (e.g., with spam classi‐\\nfication, where the presence or absence of words, phrases, characters, and so on lies at\\nthe heart of the predictive task).  To apply naive Bayes to numerical predictors, one of\\ntwo approaches must be taken:\\n200 | Chapter 5: Classification\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 217}),\n",
       " Document(page_content='2It is certainly surprising that the first article on statistical classification was published in a journal devoted to\\neugenics. Indeed, there is a disconcerting connection between the early development of statistics and eugen‐\\nics.\\n•Bin and convert the numerical predictors to categorical predictors and apply the\\nalgorithm of the previous section.\\n•Use a probability model—for example, the normal distribution (see “Normal\\nDistribution” on page 69 )—to estimate the conditional probability PXjY=i.\\nWhen a predictor category is absent in the training data, the algo‐\\nrithm assigns zero probability  to the outcome variable in new data,\\nrather than simply ignoring this variable and using the information\\nfrom other variables, as other methods might. Most implementa‐\\ntions of Naive Bayes use a smoothing parameter (Laplace Smooth‐\\ning) to prevent this.\\nKey Ideas\\n•Naive Bayes works with categorical (factor) predictors and outcomes.\\n•It asks, “Within each outcome category, which predictor categories are most\\nprobable?”\\n•That information is then inverted to estimate probabilities of outcome categories,\\ngiven predictor values.\\nFurther Reading\\n•The Elements of Statistical Learning , 2nd ed., by Trevor Hastie, Robert Tibshirani,\\nand Jerome Friedman (Springer, 2009).\\n•There is a full chapter on naive Bayes in Data Mining for Business Analytics  by\\nGalit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Y ahav, and Kenneth\\nLichtendahl (Wiley, 2007–2020, with editions for R, Python , Excel, and JMP).\\nDiscriminant Analysis\\nDiscriminant analysis  is the earliest statistical classifier; it was introduced by R. A.\\nFisher in 1936 in an article published in the Annals of Eugenics  journal.2\\nDiscriminant Analysis | 201', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 218}),\n",
       " Document(page_content='Key Terms for Discriminant Analysis\\nCovariance\\nA measure of the extent to which one variable varies in concert with another (i.e.,\\nsimilar magnitude and direction).\\nDiscriminant function\\nThe function that, when applied to the predictor variables, maximizes the separa‐\\ntion of the classes.\\nDiscriminant weights\\nThe scores that result from the application of the discriminant function and are\\nused to estimate probabilities of belonging to one class or another.\\nWhile discriminant analysis encompasses several techniques, the most commonly\\nused is linear discriminant analysis , or LDA . The original method proposed by Fisher\\nwas actually slightly different from LDA, but the mechanics are essentially the same.\\nLDA is now less widely used with the advent of more sophisticated techniques, such\\nas tree models and logistic regression.\\nHowever, you may still encounter LDA in some applications, and it has links to other\\nmore widely used methods (such as principal components analysis; see “Principal\\nComponents Analysis” on page 284 ).\\nLinear discriminant analysis should not be confused with Latent\\nDirichlet Allocation, also referred to as LDA.  Latent Dirichlet Allo‐\\ncation is used in text and natural language processing and is unre‐\\nlated to linear discriminant analysis.\\nCovariance Matrix\\nTo understand discriminant analysis, it is first necessary to introduce the concept of\\ncovariance  between two or more variables. The covariance measures the relationship\\nbetween two variables x and z. Denote the mean for each variable by x and z (see\\n“Mean” on page 9 ). The covariance sx,z between x and z is given by:\\nsx,z=∑i= 1nxi−xzi−z\\nn− 1\\nwhere n is the number of records (note that we divide by n – 1 instead of n; see\\n“Degrees of Freedom, and n or n – 1?” on page 15 ).\\n202 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 219}),\n",
       " Document(page_content='As with the correlation coefficient (see “Correlation”  on page 30), positive values\\nindicate a positive relationship and negative values indicate a negative relationship.\\nCorrelation, however, is constrained to be between –1 and 1, whereas covariance\\nscale depends on the scale of the variables x and z. The covariance matrix  Σ for x and\\nz consists of the individual variable variances, sx2 and sz2, on the diagonal (where row\\nand column are the same variable) and the covariances between variable pairs on the\\noff-diagonals:\\nΣ=sx2sx,z\\nsz,xsz2\\nRecall that the standard deviation is used to normalize a variable to\\na z-score; the covariance matrix is used in a multivariate extension\\nof this standardization process. This is known as Mahalanobis dis‐\\ntance (see “Other Distance Metrics”  on page 242) and is related to\\nthe LDA function.\\nFisher’s Linear Discriminant\\nFor simplicity, let’s focus on a classification problem in which we want to predict a\\nbinary outcome y using just two continuous numeric variables x,z. Technically, dis‐\\ncriminant analysis assumes the predictor variables are normally distributed continu‐\\nous variables, but, in practice, the method works well even for nonextreme departures\\nfrom normality, and for binary predictors. Fisher’s linear discriminant distinguishes\\nvariation between  groups, on the one hand, from variation within  groups on the\\nother. Specifically, seeking to divide the records into two groups, linear discriminant\\nanalysis (LDA) focuses on maximizing the “between” sum of squares SSbetween (meas‐\\nuring the variation between the two groups) relative to the “within” sum of squares\\nSSwithin (measuring the within-group variation). In this case, the two groups corre‐\\nspond to the records x0,z0 for which y = 0 and the records x1,z1 for which y = 1.\\nThe method finds the linear combination wxx+wzz that maximizes that sum of\\nsquares ratio:\\nSSbetween\\nSSwithin\\nThe between sum of squares is the squared distance between the two group means,\\nand the within sum of squares is the spread around the means within each group,\\nweighted by the covariance matrix. Intuitively, by maximizing the between sum of\\nDiscriminant Analysis | 203', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 220}),\n",
       " Document(page_content=\"squares and minimizing the within sum of squares, this method yields the greatest\\nseparation between the two groups.\\nA Simple Example\\nThe MASS  package, associated with the book Modern Applied Statistics with S  by W . N.\\nVenables and B. D. Ripley (Springer, 1994), provides a function for LDA with R. The\\nfollowing applies this function to a sample of loan data using two predictor variables,\\nborrower_score  and payment_inc_ratio , and prints out the estimated linear dis‐\\ncriminator weights:\\nlibrary(MASS)\\nloan_lda  <- lda(outcome ~ borrower_score  + payment_inc_ratio ,\\n                     data=loan3000 )\\nloan_lda $scaling\\n                          LD1\\nborrower_score      7.17583880\\npayment_inc_ratio  -0.09967559\\nIn Python , we can use LinearDiscriminantAnalysis  from sklearn.discrimi\\nnant_analysis . The scalings_  property gives the estimated weights:\\nloan3000 .outcome = loan3000 .outcome.astype('category' )\\npredictors  = ['borrower_score' , 'payment_inc_ratio' ]\\noutcome = 'outcome'\\nX = loan3000 [predictors ]\\ny = loan3000 [outcome]\\nloan_lda  = LinearDiscriminantAnalysis ()\\nloan_lda .fit(X, y)\\npd.DataFrame (loan_lda .scalings_ , index=X.columns)\\nUsing Discriminant Analysis for Feature Selection\\nIf the predictor  variables are normalized prior to running LDA, the\\ndiscriminator weights are measures of variable importance, thus\\nproviding a computationally efficient method of feature selection.\\nThe lda function can predict the probability of “default” versus “paid off ”:\\npred <- predict(loan_lda )\\nhead(pred$posterior )\\n   paid off   default\\n1 0.4464563  0.5535437\\n2 0.4410466  0.5589534\\n3 0.7273038  0.2726962\\n4 0.4937462  0.5062538\\n204 | Chapter 5: Classification\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 221}),\n",
       " Document(page_content=\"5 0.3900475  0.6099525\\n6 0.5892594  0.4107406\\nThe predict_proba  method of the fitted model returns the probabilities for the\\n“default” and “paid off ” outcomes:\\npred = pd.DataFrame (loan_lda .predict_proba (loan3000 [predictors ]),\\n                    columns=loan_lda .classes_ )\\npred.head()\\nA plot of the predictions helps illustrate how LDA works. Using the output from the\\npredict  function, a plot of the estimated probability of default is produced as follows:\\ncenter <- 0.5 * (loan_lda $mean[1, ] + loan_lda $mean[2, ])\\nslope <- -loan_lda $scaling[1] / loan_lda $scaling[2]\\nintercept  <- center[2] - center[1] * slope\\nggplot(data=lda_df, aes(x=borrower_score , y=payment_inc_ratio ,\\n                        color=prob_default )) +\\n  geom_point (alpha=.6) +\\n  scale_color_gradientn (colors=c('#ca0020' , '#f7f7f7' , '#0571b0' )) +\\n  scale_x_continuous (expand=c(0,0)) +\\n  scale_y_continuous (expand=c(0,0), lim=c(0, 20)) +\\n  geom_abline (slope=slope, intercept =intercept , color='darkgreen' )\\nA similar graph is created in Python using this code:\\n# Use scalings and center of means to determine decision boundary\\ncenter = np.mean(loan_lda .means_, axis=0)\\nslope = - loan_lda .scalings_ [0] / loan_lda .scalings_ [1]\\nintercept  = center[1] - center[0] * slope\\n# payment_inc_ratio for borrower_score of 0 and 20\\nx_0 = (0 - intercept ) / slope\\nx_20 = (20 - intercept ) / slope\\nlda_df = pd.concat([loan3000 , pred['default' ]], axis=1)\\nlda_df.head()\\nfig, ax = plt.subplots (figsize=(4, 4))\\ng = sns.scatterplot (x='borrower_score' , y='payment_inc_ratio' ,\\n                    hue='default' , data=lda_df,\\n                    palette=sns.diverging_palette (240, 10, n=9, as_cmap=True),\\n                    ax=ax, legend=False)\\nax.set_ylim (0, 20)\\nax.set_xlim (0.15, 0.8)\\nax.plot((x_0, x_20), (0, 20), linewidth =3)\\nax.plot(*loan_lda .means_.transpose ())\\nDiscriminant Analysis | 205\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 222}),\n",
       " Document(page_content='The resulting plot is shown in Figure 5-1 . Data points on the left of the diagonal line\\nare predicted to default (probability greater than 0.5).\\nFigure 5-1. LDA prediction of loan default using two variables: a score of the borrower’s\\ncreditworthiness and the payment-to-income ratio\\nUsing the discriminant function weights, LDA splits the predictor space into two\\nregions, as shown by the solid line. The predictions farther away from the line in both\\ndirections have a higher level of confidence (i.e., a probability further away from 0.5).\\n206 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 223}),\n",
       " Document(page_content='Extensions of Discriminant Analysis\\nMore predictor variables: while the text and example in this section\\nused just two predictor variables, LDA works just as well with more\\nthan two predictor variables. The only limiting factor is the num‐\\nber of records (estimating the covariance matrix requires a suffi‐\\ncient number of records per variable, which is typically not an issue\\nin data science applications).\\nThere are other variants of discriminant analysis. The best known\\nis quadratic discriminant analysis (QDA). Despite its name, QDA\\nis still a linear discriminant function. The main difference is that in\\nLDA, the covariance matrix is assumed to be the same for the two\\ngroups corresponding to Y = 0 and Y = 1. In QDA, the covariance\\nmatrix is allowed to be different for the two groups. In practice, the\\ndifference in most applications is not critical.\\nKey Ideas\\n•Discriminant analysis works with continuous or categorical predictors, as well as\\nwith categorical outcomes.\\n•Using the covariance matrix, it calculates a linear discriminant function , which is\\nused to distinguish records belonging to one class from those belonging to\\nanother.\\n•This function is applied to the records to derive weights, or scores, for each\\nrecord (one weight for each possible class), which determines its estimated class.\\nFurther Reading\\n•Both The Elements of Statistical Learning , 2nd ed., by Trevor Hastie, Robert Tib‐\\nshirani, and Jerome Friedman (Springer, 2009), and its shorter cousin, An Intro‐\\nduction to Statistical Learning  by Gareth James, Daniela Witten, Trevor Hastie,\\nand Robert Tibshirani (Springer, 2013), have a section on discriminant analysis.\\n•Data Mining for Business Analytics  by Galit Shmueli, Peter Bruce, Nitin Patel,\\nPeter Gedeck, Inbal Y ahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with\\neditions for R, Python , Excel, and JMP) has a full chapter on discriminant\\nanalysis.\\n•For historical interest, Fisher’s original article on the topic, “The Use of Multiple\\nMeasurements in Taxonomic Problems, ” as published in 1936 in Annals of Eugen‐\\nics (now called Annals of Genetics ), can be found online .\\nDiscriminant Analysis | 207', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 224}),\n",
       " Document(page_content='Logistic Regression\\nLogistic regression is analogous to multiple linear regression (see Chapter 4 ), except\\nthe outcome is binary. Various transformations are employed to convert the problem\\nto one in which a linear model can be fit. Like discriminant analysis, and unlike K-\\nNearest Neighbor and naive Bayes, logistic regression is a structured model approach\\nrather than a data-centric approach. Due to its fast computational speed and its out‐\\nput of a model that lends itself to rapid scoring of new data, it is a popular method.\\nKey Terms for Logistic Regression\\nLogit\\nThe function that maps class membership probability to a range from ± ∞\\n(instead of 0 to 1).\\nSynonym\\nLog odds (see below)\\nOdds\\nThe ratio of “success” (1) to “not success” (0).\\nLog odds\\nThe response in the transformed model (now linear), which gets mapped back to\\na probability.\\nLogistic Response Function and Logit\\nThe key ingredients for logistic regression are the logistic response function  and the\\nlogit , in which we map a probability (which is on a 0–1 scale) to a more expansive\\nscale suitable for linear modeling.\\nThe first step is to think of the outcome variable not as a binary label but as the prob‐\\nability p that the label is a “1. ” Naively, we might be tempted to model p as a linear\\nfunction of the predictor variables:\\np=β0+β1x1+β2x2+⋯+βqxq\\nHowever, fitting this model does not ensure that p will end up between 0 and 1, as a\\nprobability must.\\nInstead, we model p by applying a logistic response  or inverse logit  function to the\\npredictors:\\n208 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 225}),\n",
       " Document(page_content='p=1\\n1 +e−β0+β1x1+β2x2+⋯+βqxq\\nThis transform ensures that the p stays between 0 and 1.\\nTo get the exponential expression out of the denominator, we consider odds  instead\\nof probabilities. Odds, familiar to bettors everywhere, are the ratio of “successes” (1)\\nto “nonsuccesses” (0). In terms of probabilities, odds are the probability of an event\\ndivided by the probability that the event will not occur. For example, if the probability\\nthat a horse will win is 0.5, the probability of “won’t win” is (1 – 0.5) = 0.5, and the\\nodds are 1.0:\\nOdds Y= 1 =p\\n1 −p\\nWe can obtain the probability from the odds using the inverse odds function:\\np=Odds\\n1 + Odds\\nWe combine this with the logistic response function, shown earlier, to get:\\nOdds Y= 1 =eβ0+β1x1+β2x2+⋯+βqxq\\nFinally, taking the logarithm of both sides, we get an expression that involves a linear\\nfunction of the predictors:\\nlog Odds Y= 1 =β0+β1x1+β2x2+⋯+βqxq\\nThe log-odds  function,  also known as the logit  function, maps the probability p from\\n0, 1 to any value − ∞, + ∞ —see Figure 5-2 . The transformation circle is complete;\\nwe have used a linear model to predict a probability, which we can in turn map to a\\nclass label by applying a cutoff rule—any record with a probability greater than the\\ncutoff is classified as a 1.\\nLogistic Regression | 209', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 226}),\n",
       " Document(page_content='Figure 5-2. Graph of the logit function that maps a probability to a scale suitable for a\\nlinear model\\nLogistic Regression and the GLM\\nThe response in the logistic regression formula is the log odds of a binary outcome of\\n1. We observe only the binary outcome, not the log odds, so special statistical meth‐\\nods are needed to fit the equation. Logistic regression is a special instance of a gener‐\\nalized linear model  (GLM) developed to extend linear regression to other settings.\\nIn R, to fit a logistic regression, the glm function is used with the family  parameter\\nset to binomial . The following code fits a logistic regression to the personal loan data\\nintroduced in “K-Nearest Neighbors” on page 238 :\\nlogistic_model  <- glm(outcome ~ payment_inc_ratio  + purpose_  +\\n                        home_ + emp_len_  + borrower_score ,\\n                      data=loan_data , family=\\'binomial\\' )\\nlogistic_model\\nCall:  glm(formula = outcome ~ payment_inc_ratio  + purpose_  + home_ +\\n    emp_len_  + borrower_score , family = \"binomial\" , data = loan_data )\\n210 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 227}),\n",
       " Document(page_content=\"Coefficients :\\n               (Intercept )           payment_inc_ratio\\n                   1.63809                     0.07974\\npurpose_debt_consolidation     purpose_home_improvement\\n                   0.24937                     0.40774\\n    purpose_major_purchase              purpose_medical\\n                   0.22963                     0.51048\\n             purpose_other       purpose_small_business\\n                   0.62066                     1.21526\\n                  home_OWN                    home_RENT\\n                   0.04833                     0.15732\\n         emp_len_  > 1 Year              borrower_score\\n                  -0.35673                     -4.61264\\nDegrees of Freedom: 45341 Total (i.e. Null);  45330 Residual\\nNull Deviance :     62860\\nResidual  Deviance : 57510  AIC: 57540\\nThe response is outcome , which takes a 0 if the loan is paid off and a 1 if the loan\\ndefaults. purpose_  and home_  are factor variables representing the purpose of the loan\\nand the home ownership status. As in linear regression, a factor variable with P levels\\nis represented with P – 1 columns. By default in R, the reference  coding is used, and\\nthe levels are all compared to the reference level (see “Factor Variables in Regression”\\non page 163). The reference levels for these factors are credit_card  and MORTGAGE ,\\nrespectively. The variable borrower_score  is a score from 0 to 1 representing the\\ncreditworthiness of the borrower (from poor to excellent). This variable was created\\nfrom several other variables using K-Nearest Neighbor—see “KNN as a Feature\\nEngine” on page 247 .\\nIn Python , we use the scikit-learn  class LogisticRegression  from sklearn.lin\\near_model . The arguments penalty  and C are used to prevent overfitting by L1 or L2\\nregularization. Regularization is switched on by default. In order to fit without regu‐\\nlarization, we set C to a very large value. The solver  argument selects the used mini‐\\nmizer; the method liblinear  is the default:\\npredictors  = ['payment_inc_ratio' , 'purpose_' , 'home_', 'emp_len_' ,\\n              'borrower_score' ]\\noutcome = 'outcome'\\nX = pd.get_dummies (loan_data [predictors ], prefix='', prefix_sep ='',\\n                   drop_first =True)\\ny = loan_data [outcome]\\nlogit_reg  = LogisticRegression (penalty='l2', C=1e42, solver='liblinear' )\\nlogit_reg .fit(X, y)\\nIn contrast to R, scikit-learn  derives the classes from the unique values in y (paid\\noff and default ). Internally, the classes are ordered alphabetically. As this is the reverse\\norder from the factors used in R, you will see that the coefficients are reversed. The\\nLogistic Regression | 211\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 228}),\n",
       " Document(page_content='predict  method returns the class label and predict_proba  returns the probabilities\\nin the order available from the attribute logit_reg.classes_ .\\nGeneralized Linear Models\\nGeneralized linear models (GLMs) are characterized by two main components:\\n•A probability distribution or family (binomial in the case of logistic regression)\\n•A link function—i.e., a transformation function that maps the response to the\\npredictors (logit in the case of logistic regression)\\nLogistic regression is by far the most common form of GLM. A data scientist will\\nencounter other types of GLMs. Sometimes a log link function is used instead of the\\nlogit; in practice, use of a log link is unlikely to lead to very different results for most\\napplications. The Poisson distribution is commonly used to model count data (e.g.,\\nthe number of times a user visits a web page in a certain amount of time). Other fam‐\\nilies include negative binomial and gamma, often used to model elapsed time (e.g.,\\ntime to failure). In contrast to logistic regression, application of GLMs with these\\nmodels is more nuanced and involves greater care. These are best avoided unless you\\nare familiar with and understand the utility and pitfalls of these methods.\\nPredicted Values from Logistic Regression\\nThe predicted value from logistic regression is in terms of the log odds:\\nY= log Odds Y= 1 . The predicted probability is given by the logistic response\\nfunction:\\np=1\\n1 +e−Y\\nFor example, look at the predictions from the model logistic_model  in R:\\npred <- predict(logistic_model )\\nsummary(pred)\\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.\\n-2.704774  -0.518825  -0.008539   0.002564   0.505061   3.509606\\nIn Python , we can convert the probabilities into a data frame and use the describe\\nmethod to get these characteristics of the distribution:\\npred = pd.DataFrame (logit_reg .predict_log_proba (X),\\n                    columns=loan_data [outcome].cat.categories )\\npred.describe ()\\n212 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 229}),\n",
       " Document(page_content='Converting these values to probabilities is a simple transform:\\nprob <- 1/(1 + exp(-pred))\\n> summary(prob)\\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\\n0.06269 0.37313 0.49787 0.50000 0.62365 0.97096\\nThe probabilities are directly available using the predict_proba  methods in scikit-\\nlearn :\\npred = pd.DataFrame (logit_reg .predict_proba (X),\\n                    columns=loan_data [outcome].cat.categories )\\npred.describe ()\\nThese are on a scale from 0 to 1 and don’t yet declare whether the predicted value is\\ndefault or paid off. We could declare any value greater than 0.5 as default. In practice,\\na lower cutoff is often appropriate if the goal is to identify members of a rare class\\n(see “The Rare Class Problem” on page 223 ).\\nInterpreting the Coefficients  and Odds Ratios\\nOne advantage of logistic regression is that it produces a model that can be scored to \\nnew data rapidly, without recomputation. Another is the relative ease of interpreta‐\\ntion of the model, as compared with other classification methods.  The key conceptual\\nidea is understanding an odds ratio . The odds ratio is easiest to understand for a\\nbinary factor variable X:\\nodds ratio =Odds Y= 1 X= 1\\nOdds Y= 1 X= 0\\nThis is interpreted as the odds that Y = 1 when X = 1 versus the odds that Y = 1 when\\nX = 0. If the odds ratio is 2, then the odds that Y = 1 are two times higher when X = 1\\nversus when X = 0.\\nWhy bother with an odds ratio rather than probabilities? We work with odds because\\nthe coefficient βj in the logistic regression is the log of the odds ratio for Xj.\\nAn example will make this more explicit. For the model fit in “Logistic Regression\\nand the GLM” on page 210 , the regression coefficient for purpose_small_business  is\\n1.21526. This means that a loan to a small business compared to a loan to pay off\\ncredit card debt reduces the odds of defaulting versus being paid off by\\nexp 1.21526 ≈ 3.4 . Clearly, loans for the purpose of creating or expanding a small\\nbusiness are considerably riskier than other types of loans.\\nFigure 5-3  shows the relationship between the odds ratio and the log-odds ratio for\\nodds ratios greater than 1. Because the coefficients are on the log scale, an increase of\\n1 in the coefficient results in an increase of exp 1≈ 2.72  in the odds ratio.\\nLogistic Regression | 213', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 230}),\n",
       " Document(page_content='Figure 5-3. The relationship between the odds ratio and the log-odds ratio\\nOdds ratios for numeric variables X can be interpreted similarly: they measure the\\nchange in the odds ratio for a unit change in X. For example, the effect of increasing\\nthe payment-to-income ratio from, say, 5 to 6 increases the odds of the loan default‐\\ning by a factor of exp 0.08244 ≈ 1.09 . The variable borrower_score  is a score on the\\nborrowers’ creditworthiness and ranges from 0 (low) to 1 (high). The odds of the best\\nborrowers relative to the worst borrowers defaulting on their loans is smaller by a\\nfactor  of exp − 4.61264 ≈ 0.01 . In other words, the default risk from the borrowers\\nwith the poorest creditworthiness is 100 times greater than that of the best borrowers!\\nLinear and Logistic Regression: Similarities and Differences\\nLinear regression and logistic regression share many commonalities. Both assume a\\nparametric linear form relating the predictors with the response. Exploring and find‐\\ning the best model are done in very similar ways. Extensions to the linear model, like\\nthe use of a spline transform of a predictor (see “Splines”  on page 189), are equally\\napplicable in the logistic regression setting. Logistic regression differs in two funda‐\\nmental ways:\\n214 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 231}),\n",
       " Document(page_content='•The way the model is fit (least squares is not applicable)\\n•The nature and analysis of the residuals from the model\\nFitting the model\\nLinear regression is fit using least squares, and the quality of the fit is evaluated using\\nRMSE and R-squared statistics. In logistic regression (unlike in linear regression),\\nthere is no closed-form solution, and the model must be fit using maximum likelihood\\nestimation  (MLE).  Maximum likelihood estimation is a process that tries to find the\\nmodel that is most likely to have produced the data we see. In the logistic regression\\nequation, the response is not 0 or 1 but rather an estimate of the log odds that the\\nresponse is 1. The MLE finds the solution such that the estimated log odds best\\ndescribes the observed outcome. The mechanics of the algorithm involve a quasi-\\nNewton optimization that iterates between a scoring step ( Fisher’s scoring ), based on\\nthe current parameters, and an update to the parameters to improve the fit.\\nMaximum Likelihood Estimation\\nHere is a bit more detail, if you like statistical symbols: start with a set of data\\nX1,X2,⋯,Xn and a probability model PθX1,X2,⋯,Xn that depends on a set of\\nparameters θ. The goal of MLE is to find the set of parameters θ that maximizes the\\nvalue of PθX1,X2,⋯,Xn; that is, it maximizes the probability of observing\\nX1,X2,⋯,Xn given the model P. In the fitting process, the  model is evaluated using\\na metric called deviance :\\ndeviance = − 2 log PθX1,X2,⋯,Xn\\nLower deviance corresponds to a better fit.\\nFortunately, most practitioners don’t need to concern themselves with the details of\\nthe fitting algorithm since this is handled by the software. Most data scientists will\\nnot need to worry about the fitting method, other than understanding that it is a way\\nto find a good model under certain assumptions.\\nLogistic Regression | 215', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 232}),\n",
       " Document(page_content='Handling Factor Variables\\nIn logistic regression, factor variables should be coded as in linear\\nregression; see “Factor Variables in Regression” on page 163. In R\\nand other software, this is normally handled automatically, and\\ngenerally reference encoding is used. All of the other classification\\nmethods covered in this chapter typically use the one hot encoder\\nrepresentation (see “One Hot Encoder” on page 242). In Python ’s\\nscikit-learn , it is easiest to use one hot encoding, which means\\nthat only n – 1  of the resulting dummies can be used in the\\nregression.\\nAssessing the Model\\nLike other classification methods, logistic regression is assessed by how accurately  the\\nmodel classifies new data (see “Evaluating Classification Models”  on page 219). As\\nwith linear regression, some additional standard statistical tools are available to\\nexamine and improve the model. Along with the estimated coefficients, R reports the\\nstandard error of the coefficients (SE), a z-value, and a p-value:\\nsummary(logistic_model )\\nCall:\\nglm(formula = outcome ~ payment_inc_ratio  + purpose_  + home_ +\\n    emp_len_  + borrower_score , family = \"binomial\" , data = loan_data )\\nDeviance  Residuals :\\n     Min        1Q    Median        3Q       Max\\n-2.51951   -1.06908   -0.05853    1.07421   2.15528\\nCoefficients :\\n                            Estimate  Std. Error z value Pr(>|z|)\\n(Intercept )                 1.638092    0.073708   22.224  < 2e-16 ***\\npayment_inc_ratio            0.079737    0.002487   32.058  < 2e-16 ***\\npurpose_debt_consolidation   0.249373    0.027615    9.030  < 2e-16 ***\\npurpose_home_improvement     0.407743    0.046615    8.747  < 2e-16 ***\\npurpose_major_purchase       0.229628    0.053683    4.277 1.89e-05  ***\\npurpose_medical              0.510479    0.086780    5.882 4.04e-09  ***\\npurpose_other                0.620663    0.039436   15.738  < 2e-16 ***\\npurpose_small_business       1.215261    0.063320   19.192  < 2e-16 ***\\nhome_OWN                     0.048330    0.038036    1.271    0.204\\nhome_RENT                    0.157320    0.021203    7.420 1.17e-13  ***\\nemp_len_  > 1 Year          -0.356731    0.052622   -6.779 1.21e-11  ***\\nborrower_score              -4.612638    0.083558  -55.203  < 2e-16 ***\\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\\n(Dispersion  parameter  for binomial  family taken to be 1)\\n    Null deviance : 62857  on 45341  degrees of freedom\\n216 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 233}),\n",
       " Document(page_content=\"Residual  deviance : 57515  on 45330  degrees of freedom\\nAIC: 57539\\nNumber of Fisher Scoring iterations : 4\\nThe package statsmodels  has an implementation for generalized linear model ( GLM)\\nthat provides similarly detailed information:\\ny_numbers  = [1 if yi == 'default'  else 0 for yi in y]\\nlogit_reg_sm  = sm.GLM(y_numbers , X.assign(const=1),\\n                      family=sm.families .Binomial ())\\nlogit_result  = logit_reg_sm .fit()\\nlogit_result .summary()\\nInterpretation of the p-value comes with the same caveat as in regression and should\\nbe viewed more as a relative indicator of variable importance (see “ Assessing the\\nModel”  on page 153) than as a formal measure of statistical significance. A logistic\\nregression model, which has a binary response, does not have an associated RMSE or\\nR-squared. Instead, a logistic regression model is typically evaluated using more gen‐\\neral metrics for classification; see “Evaluating Classification Models” on page 219 .\\nMany other concepts for linear regression carry over to the logistic regression setting\\n(and other GLMs). For example, you can use stepwise regression, fit interaction\\nterms, or include spline terms. The same concerns regarding confounding and corre‐\\nlated variables apply to logistic regression (see “Interpreting the Regression Equation”\\non page 169). Y ou can fit generalized additive models (see “Generalized Additive\\nModels” on page 192 ) using the mgcv  package in R:\\nlogistic_gam  <- gam(outcome ~ s(payment_inc_ratio ) + purpose_  +\\n                    home_ + emp_len_  + s(borrower_score ),\\n                    data=loan_data , family='binomial' )\\nThe formula interface of statsmodels  also supports these extensions in Python :\\nimport statsmodels.formula.api  as smf\\nformula = ('outcome ~ bs(payment_inc_ratio, df=4) + purpose_ + '  +\\n           'home_ + emp_len_ + bs(borrower_score, df=4)' )\\nmodel = smf.glm(formula=formula, data=loan_data , family=sm.families .Binomial ())\\nresults = model.fit()\\nAnalysis of residuals\\nOne area where logistic regression differs from linear regression is in the analysis of\\nthe residuals. As in linear regression (see Figure 4-9 ), it is straightforward to compute \\npartial residuals in R:\\nterms <- predict(logistic_gam , type='terms')\\npartial_resid  <- resid(logistic_model ) + terms\\ndf <- data.frame (payment_inc_ratio  = loan_data [, 'payment_inc_ratio' ],\\n                 terms = terms[, 's(payment_inc_ratio)' ],\\n                 partial_resid  = partial_resid [, 's(payment_inc_ratio)' ])\\nLogistic Regression | 217\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 234}),\n",
       " Document(page_content=\"ggplot(df, aes(x=payment_inc_ratio , y=partial_resid , solid = FALSE)) +\\n  geom_point (shape=46, alpha=0.4) +\\n  geom_line (aes(x=payment_inc_ratio , y=terms),\\n            color='red', alpha=0.5, size=1.5) +\\n  labs(y='Partial Residual' )\\nThe resulting plot is displayed in Figure 5-4 . The estimated fit, shown by the line,\\ngoes between two sets of point clouds. The top cloud corresponds to a response of 1\\n(defaulted loans), and the bottom cloud corresponds to a response of 0 (loans paid\\noff). This is very typical of residuals from a logistic regression since the output is\\nbinary. The prediction is measured as the logit (log of the odds ratio), which will\\nalways be some finite value. The actual value, an absolute 0 or 1, corresponds to an\\ninfinite logit, either positive or negative, so the residuals (which get added to the fit‐\\nted value) will never equal 0. Hence the plotted points lie in clouds either above or\\nbelow the fitted line in the partial residual plot. Partial residuals in logistic regression,\\nwhile less valuable than in regression, are still useful to confirm nonlinear behavior\\nand identify highly influential records.\\nThere is currently no implementation of partial residuals in any of the major Python\\npackages. We provide Python  code to create the partial residual plot in the accompa‐\\nnying source code repository.\\nFigure 5-4. Partial residuals from logistic regression\\n218 | Chapter 5: Classification\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 235}),\n",
       " Document(page_content='Some of the output from the summary  function can effectively be\\nignored. The dispersion parameter does not apply to logistic\\nregression and is there for other types of GLMs. The residual devi‐\\nance and the number of scoring iterations are related to the maxi‐\\nmum likelihood fitting method; see “Maximum Likelihood\\nEstimation” on page 215 .\\nKey Ideas\\n•Logistic regression is like linear regression, except that the outcome is a binary\\nvariable.\\n•Several transformations are needed to get the model into a form that can be fit as\\na linear model, with the log of the odds ratio as the response variable.\\n•After the linear model is fit (by an iterative process), the log odds is mapped back\\nto a probability.\\n•Logistic regression is popular because it is computationally fast and produces a\\nmodel that can be scored to new data with only a few arithmetic operations.\\nFurther Reading\\n•The standard reference on logistic regression is Applied Logistic Regression , 3rd\\ned., by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant (Wiley, 2013).\\n•Also popular are two books by Joseph Hilbe: Logistic Regression Models  (very\\ncomprehensive, 2017) and Practical Guide to Logistic Regression  (compact, 2015),\\nboth from Chapman & Hall/CRC Press.\\n•Both The Elements of Statistical Learning , 2nd ed., by Trevor Hastie, Robert Tib‐\\nshirani, and Jerome Friedman (Springer, 2009), and its shorter cousin, An Intro‐\\nduction to Statistical Learning  by Gareth James, Daniela Witten, Trevor Hastie,\\nand Robert Tibshirani (Springer, 2013), have a section on logistic regression.\\n•Data Mining for Business Analytics  by Galit Shmueli, Peter Bruce, Nitin Patel,\\nPeter Gedeck, Inbal Y ahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with\\neditions for R, Python , Excel, and JMP) has a full chapter on logistic regression.\\nEvaluating Classification  Models\\nIt is common in predictive modeling to train a number of different models, apply\\neach to a holdout sample, and assess their performance. Sometimes, after a number of\\nmodels have been evaluated and tuned, and if there are enough data, a third holdout\\nEvaluating Classification  Models | 219', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 236}),\n",
       " Document(page_content='sample, not used previously, is used to estimate how the chosen model will perform\\nwith completely new data. Different disciplines and practitioners will also use the\\nterms validation  and test to refer to the holdout sample(s). Fundamentally, the assess‐\\nment process attempts to learn which model produces the most accurate and useful\\npredictions.\\nKey Terms for Evaluating Classification  Models\\nAccuracy\\nThe percent (or proportion) of cases classified correctly.\\nConfusion matrix\\nA tabular display (2×2 in the binary case) of the record counts by their predicted\\nand actual classification status.\\nSensitivity\\nThe percent (or proportion) of all 1s that are correctly classified as 1s.\\nSynonym\\nRecall\\nSpecificity\\nThe percent (or proportion) of all 0s that are correctly classified as 0s.\\nPrecision\\nThe percent (proportion) of predicted 1s that are actually 1s.\\nROC curve\\nA plot of sensitivity versus specificity.\\nLift\\nA measure of how effective the model is at identifying (comparatively rare) 1s at\\ndifferent probability cutoffs.\\nA simple way to measure classification performance is to count the proportion of pre‐\\ndictions that are correct, i.e., measure the accuracy . Accuracy is simply a measure of\\ntotal error:\\naccuracy =∑TruePositive + ∑TrueNegative\\nSampleSize\\n220 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 237}),\n",
       " Document(page_content=\"3Not all methods provide unbiased estimates of probability. In most cases, it is sufficient that the method pro‐\\nvide a ranking equivalent to the rankings that would result from an unbiased probability estimate; the cutoff\\nmethod is then functionally equivalent.In most classification algorithms, each case is assigned an “estimated probability of\\nbeing a 1. ”3 The default decision point, or cutoff, is typically 0.50 or 50%. If the proba‐\\nbility is above 0.5, the classification is “1”; otherwise it is “0. ” An alternative default\\ncutoff is the prevalent probability of 1s in the data.\\nConfusion Matrix\\nAt the heart of classification metrics is the confusion matrix . The confusion matrix is a\\ntable showing the number of correct and incorrect predictions categorized by type of\\nresponse. Several packages are available in R and Python  to compute a confusion\\nmatrix, but in the binary case, it is simple to compute one by hand.\\nTo illustrate the confusion matrix, consider the logistic_gam  model that was trained\\non a balanced data set with an equal number of defaulted and paid-off loans (see\\nFigure 5-4 ). Following the usual conventions, Y = 1 corresponds to the event of inter‐\\nest (e.g., default), and Y = 0 corresponds to a negative (or usual) event (e.g., paid off).\\nThe following computes the confusion matrix for the logistic_gam  model applied to\\nthe entire (unbalanced) training set in R:\\npred <- predict(logistic_gam , newdata=train_set )\\npred_y <- as.numeric (pred > 0)\\ntrue_y <- as.numeric (train_set $outcome=='default' )\\ntrue_pos  <- (true_y==1) & (pred_y==1)\\ntrue_neg  <- (true_y==0) & (pred_y==0)\\nfalse_pos  <- (true_y==0) & (pred_y==1)\\nfalse_neg  <- (true_y==1) & (pred_y==0)\\nconf_mat  <- matrix(c(sum(true_pos ), sum(false_pos ),\\n                     sum(false_neg ), sum(true_neg )), 2, 2)\\ncolnames (conf_mat ) <- c('Yhat = 1' , 'Yhat = 0' )\\nrownames (conf_mat ) <- c('Y = 1', 'Y = 0')\\nconf_mat\\n      Yhat = 1 Yhat = 0\\nY = 1 14295    8376\\nY = 0 8052     14619\\nIn Python :\\npred = logit_reg .predict(X)\\npred_y = logit_reg .predict(X) == 'default'\\ntrue_y = y == 'default'\\ntrue_pos  = true_y & pred_y\\ntrue_neg  = ~true_y & ~pred_y\\nfalse_pos  = ~true_y & pred_y\\nfalse_neg  = true_y & ~pred_y\\nEvaluating Classification  Models | 221\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 238}),\n",
       " Document(page_content=\"conf_mat  = pd.DataFrame ([[np.sum(true_pos ), np.sum(false_neg )],\\n                         [np.sum(false_pos ), np.sum(true_neg )]],\\n                       index=['Y = default' , 'Y = paid off' ],\\n                       columns=['Yhat = default' , 'Yhat = paid off' ])\\nconf_mat\\nThe predicted outcomes are columns and the true outcomes are the rows. The diago‐\\nnal elements of the matrix show the number of correct predictions, and the off-\\ndiagonal elements show the number of incorrect predictions. For example, 14,295\\ndefaulted loans were correctly predicted as a default, but 8,376 defaulted loans were\\nincorrectly predicted as paid off.\\nFigure 5-5  shows the relationship between the confusion matrix for a binary response\\nY and different metrics (see “Precision, Recall, and Specificity”  on page 223 for more\\non the metrics). As with the example for the loan data, the actual response is along\\nthe rows and the predicted response is along the columns. The diagonal boxes (upper\\nleft, lower right) show when the predictions Y correctly predict the response. One\\nimportant metric not explicitly called out is the false positive rate (the mirror image\\nof precision). When 1s are rare, the ratio of false positives to all predicted positives\\ncan be high, leading to the unintuitive situation in which a predicted 1 is most likely a\\n0. This problem plagues medical screening tests (e.g., mammograms) that are widely\\napplied: due to the relative rarity of the condition, positive test results most likely do\\nnot mean breast cancer. This leads to much confusion in the public.\\nFigure 5-5. Confusion matrix for a binary response and various metrics\\nHere, we present the actual response along the rows and the pre‐\\ndicted response along the columns, but it is not uncommon to see\\nthis reversed. A notable example is the popular caret  package in R.\\n222 | Chapter 5: Classification\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 239}),\n",
       " Document(page_content='The Rare Class Problem\\nIn many cases, there is an imbalance in the classes to be predicted, with one class\\nmuch more prevalent than the other—for example, legitimate insurance claims versus\\nfraudulent ones, or browsers versus purchasers at a website. The rare class (e.g., the\\nfraudulent claims) is usually the class of more interest and is typically designated 1, in\\ncontrast to the more prevalent 0s. In the typical scenario, the 1s are the more impor‐\\ntant case, in the sense that misclassifying them as 0s is costlier than misclassifying 0s\\nas 1s. For example, correctly identifying a fraudulent insurance claim may save thou‐\\nsands of dollars. On the other hand, correctly identifying a nonfraudulent claim\\nmerely saves you the cost and effort of going through the claim by hand with a more\\ncareful review (which is what you would do if the claim were tagged as “fraudulent”).\\nIn such cases, unless the classes are easily separable, the most accurate classification\\nmodel may be one that simply classifies everything as a 0. For example, if only 0.1% of\\nthe browsers at a web store end up purchasing, a model that predicts that each\\nbrowser will leave without purchasing will be 99.9% accurate. However, it will be use‐\\nless. Instead, we would be happy with a model that is less accurate overall but is good\\nat picking out the purchasers, even if it misclassifies some nonpurchasers along the\\nway.\\nPrecision, Recall, and Specificity\\nMetrics other than pure accuracy—metrics that are more nuanced—are commonly\\nused in evaluating classification models. Several of these have a long history in statis‐\\ntics—especially biostatistics, where they are used to describe the expected perfor‐\\nmance of diagnostic tests. The precision  measures the accuracy  of a predicted positive\\noutcome (see Figure 5-5 ):\\nprecision =∑TruePositive\\n∑TruePositive + ∑FalsePositive\\nThe recall , also known as sensitivity , measures the strength of the model to predict a\\npositive outcome—the proportion of the 1s that it correctly identifies (see\\nFigure 5-5 ). The term sensitivity  is used a lot in biostatistics and medical diagnostics,\\nwhereas recall  is used more in the machine learning community. The definition of\\nrecall is:\\nrecall =∑TruePositive\\n∑TruePositive + ∑FalseNegative\\nEvaluating Classification  Models | 223', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 240}),\n",
       " Document(page_content=\"4The ROC curve was first used during World War II to describe the performance of radar receiving stations,\\nwhose job was to correctly identify (classify) reflected radar signals and alert defense forces to incoming\\naircraft.Another metric used is specificity , which measures a model’s ability to predict a nega‐\\ntive outcome:\\nspecificity =∑TrueNegative\\n∑TrueNegative + ∑FalsePositive\\nWe can calculate the three metrics from conf_mat  in R:\\n# precision\\nconf_mat [1, 1] / sum(conf_mat [,1])\\n# recall\\nconf_mat [1, 1] / sum(conf_mat [1,])\\n# specificity\\nconf_mat [2, 2] / sum(conf_mat [2,])\\nHere is the equivalent code to calculate the metrics in Python :\\nconf_mat  = confusion_matrix (y, logit_reg .predict(X))\\nprint('Precision' , conf_mat [0, 0] / sum(conf_mat [:, 0]))\\nprint('Recall' , conf_mat [0, 0] / sum(conf_mat [0, :]))\\nprint('Specificity' , conf_mat [1, 1] / sum(conf_mat [1, :]))\\nprecision_recall_fscore_support (y, logit_reg .predict(X),\\n                                labels=['default' , 'paid off' ])\\nscikit-learn  has a custom method precision_recall_fscore_support  that calcu‐\\nlates precision and recall/specificity all at once.\\nROC Curve\\nY ou can see that there is a trade-off between recall and specificity. Capturing more 1s\\ngenerally means misclassifying more 0s as 1s. The ideal classifier would do an excel‐\\nlent job of classifying the 1s, without misclassifying more 0s as 1s.\\nThe metric that captures this trade-off is the “Receiver Operating Characteristics”\\ncurve, usually referred to as the ROC curve . The ROC curve plots recall (sensitivity)\\non the y-axis against specificity on the x-axis.4 The ROC curve shows the trade-off\\nbetween recall and specificity as you change the cutoff to determine how to classify a\\nrecord. Sensitivity (recall) is plotted on the y-axis, and you may encounter two forms\\nin which the x-axis is labeled:\\n224 | Chapter 5: Classification\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 241}),\n",
       " Document(page_content=\"•Specificity plotted on the x-axis, with 1 on the left and 0 on the right\\n•1-Specificity plotted on the x-axis, with 0 on the left and 1 on the right\\nThe curve looks identical whichever way it is done. The process to compute the ROC\\ncurve is:\\n1.Sort the records by the predicted probability of being a 1, starting with the most\\nprobable and ending with the least probable.\\n2.Compute the cumulative specificity and recall based on the sorted records.\\nComputing the ROC curve in R is straightforward. The following code computes\\nROC for the loan data:\\nidx <- order(-pred)\\nrecall <- cumsum(true_y[idx] == 1) / sum(true_y == 1)\\nspecificity  <- (sum(true_y == 0) - cumsum(true_y[idx] == 0)) / sum(true_y == 0)\\nroc_df <- data.frame (recall = recall, specificity  = specificity )\\nggplot(roc_df, aes(x=specificity , y=recall)) +\\n  geom_line (color='blue') +\\n  scale_x_reverse (expand=c(0, 0)) +\\n  scale_y_continuous (expand=c(0, 0)) +\\n  geom_line (data=data.frame (x=(0:100) / 100), aes(x=x, y=1-x),\\n            linetype ='dotted' , color='red')\\nIn Python , we can use the scikit-learn  function sklearn.metrics.roc_curve  to\\ncalculate the required information for the ROC curve. Y ou can find similar packages\\nfor R, e.g., ROCR :\\nfpr, tpr, thresholds  = roc_curve (y, logit_reg .predict_proba (X)[:,0],\\n                                 pos_label ='default' )\\nroc_df = pd.DataFrame ({'recall' : tpr, 'specificity' : 1 - fpr})\\nax = roc_df.plot(x='specificity' , y='recall' , figsize=(4, 4), legend=False)\\nax.set_ylim (0, 1)\\nax.set_xlim (1, 0)\\nax.plot((1, 0), (0, 1))\\nax.set_xlabel ('specificity' )\\nax.set_ylabel ('recall' )\\nThe result is shown in Figure 5-6 . The dotted diagonal line corresponds to a classifier\\nno better than random chance. An extremely effective classifier (or, in medical situa‐\\ntions, an extremely effective diagnostic test) will have an ROC that hugs the upper-\\nleft corner—it will correctly identify lots of 1s without misclassifying lots of 0s as 1s.\\nFor this model, if we want a classifier with a specificity of at least 50%, then the recall\\nis about 75%.\\nEvaluating Classification  Models | 225\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 242}),\n",
       " Document(page_content='Figure 5-6. ROC curve for the loan data\\nPrecision-Recall Curve\\nIn addition to ROC curves, it can be illuminating to examine the\\nprecision-recall (PR) curve . PR curves are computed in a similar\\nway except that the data is ordered from least to most probable and\\ncumulative precision and recall statistics are computed. PR curves\\nare especially useful in evaluating data with highly unbalanced out‐\\ncomes.\\nAUC\\nThe ROC curve is a valuable graphical tool, but by itself doesn’t constitute a single\\nmeasure for the performance of a classifier. The ROC curve can be used, however, to\\nproduce the area underneath the curve (AUC) metric. AUC is simply the total area\\nunder the ROC curve. The larger the value of AUC, the more effective the classifier.\\nAn AUC of 1 indicates a perfect classifier: it gets all the 1s correctly classified, and it\\ndoesn’t misclassify any 0s as 1s.\\nA completely ineffective classifier—the diagonal line—will have an AUC of 0.5.\\n226 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 243}),\n",
       " Document(page_content=\"Figure 5-7  shows the area under the ROC curve for the loan model. The value of\\nAUC can be computed by a numerical integration in R:\\nsum(roc_df$recall[-1] * diff(1 - roc_df$specificity ))\\n    [1] 0.6926172\\nIn Python , we can either calculate the accuracy as shown for R or use scikit-learn ’s\\nfunction sklearn.metrics.roc_auc_score . Y ou will need to provide the expected\\nvalue as 0 or 1:\\nprint(np.sum(roc_df.recall[:-1] * np.diff(1 - roc_df.specificity )))\\nprint(roc_auc_score ([1 if yi == 'default'  else 0 for yi in y],\\n                    logit_reg .predict_proba (X)[:, 0]))\\nThe model has an AUC of about 0.69, corresponding to a relatively weak classifier.\\nFigure 5-7. Area under the ROC curve for the loan data\\nEvaluating Classification  Models | 227\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 244}),\n",
       " Document(page_content='False Positive Rate Confusion\\nFalse positive/negative rates are often confused or conflated with\\nspecificity or sensitivity (even in publications and software!).\\nSometimes the false positive rate is defined as the proportion of\\ntrue negatives that test positive. In many cases (such as network\\nintrusion detection), the term is used to refer to the proportion of\\npositive signals that are true negatives.\\nLift\\nUsing the AUC as a metric to evaluate a model is an improvement over simple accu‐\\nracy, as it can assess how well a classifier handles the trade-off between overall accu‐\\nracy and the need to identify the more important 1s. But it does not completely\\naddress the rare-case problem, where you need to lower the model’s probability cutoff\\nbelow 0.5 to avoid having all records classified as 0. In such cases, for a record to be\\nclassified as a 1, it might be sufficient to have a probability of 0.4, 0.3, or lower. In\\neffect, we end up overidentifying 1s, reflecting their greater importance.\\nChanging this cutoff will improve your chances of catching the 1s (at the cost of mis‐\\nclassifying more 0s as 1s). But what is the optimum cutoff?\\nThe concept of lift lets you defer answering that question. Instead, you consider the\\nrecords in order of their predicted probability of being 1s. Say, of the top 10% classi‐\\nfied as 1s, how much better did the algorithm do, compared to the benchmark of sim‐\\nply picking blindly? If you can get 0.3% response in this top decile instead of the 0.1%\\nyou get overall by picking randomly, the algorithm is said to have a lift (also called\\ngains ) of 3 in the top decile.  A lift chart (gains chart) quantifies this over the range of\\nthe data. It can be produced decile by decile, or continuously over the range of the\\ndata.\\nTo compute a lift chart, you first produce a cumulative gains chart  that shows the\\nrecall on the y-axis and the total number of records on the x-axis. The lift curve  is the\\nratio of the cumulative gains to the diagonal line corresponding to random selection.\\nDecile gains charts  are one of the oldest techniques in predictive modeling, dating\\nfrom the days before internet commerce. They were particularly popular among\\ndirect mail professionals. Direct mail is an expensive method of advertising if applied\\nindiscriminately, and advertisers used predictive models (quite simple ones, in the\\nearly days) to identify the potential customers with the likeliest prospect of payoff.\\n228 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 245}),\n",
       " Document(page_content='Uplift\\nSometimes the term uplift  is used to mean the same thing as lift.\\nAn alternate meaning is used in a more restrictive setting, when an\\nA/B test has been conducted and the treatment (A or B) is then\\nused as a predictor variable in a predictive model. The uplift is the\\nimprovement in response predicted for an individual case  with\\ntreatment A versus treatment B. This is determined by scoring the\\nindividual case first with the predictor set to A, and then again with\\nthe predictor toggled to B. Marketers and political campaign con‐\\nsultants use this method to determine which of two messaging\\ntreatments should be used with which customers or voters.\\nA lift curve lets you look at the consequences of setting different probability cutoffs\\nfor classifying records as 1s. It can be an intermediate step in settling on an appropri‐\\nate cutoff level. For example, a tax authority might have only a certain amount of\\nresources that it can spend on tax audits, and it wants to spend them on the likeliest\\ntax cheats. With its resource constraint in mind, the authority would use a lift chart to\\nestimate where to draw the line between tax returns selected for audit and those left\\nalone.\\nKey Ideas\\n•Accuracy (the percent of predicted classifications that are correct) is but a first\\nstep in evaluating a model.\\n•Other metrics (recall, specificity, precision) focus on more specific performance\\ncharacteristics (e.g., recall measures how good a model is at correctly identifying\\n1s).\\n•AUC (area under the ROC curve) is a common metric for the ability of a model\\nto distinguish 1s from 0s.\\n•Similarly, lift measures how effective a model is in identifying the 1s, and it is\\noften calculated decile by decile, starting with the most probable 1s.\\nFurther Reading\\nEvaluation and assessment are typically covered in the context of a particular model\\n(e.g., K-Nearest Neighbors or decision trees); three books that handle the subject in\\nits own chapter are:\\n•Data Mining , 3rd ed., by Ian Whitten, Eibe Frank, and Mark Hall (Morgan Kauf‐\\nmann, 2011).\\nEvaluating Classification  Models | 229', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 246}),\n",
       " Document(page_content='•Modern Data Science with R  by Benjamin Baumer, Daniel Kaplan, and Nicholas\\nHorton (Chapman & Hall/CRC Press, 2017).\\n•Data Mining for Business Analytics  by Galit Shmueli, Peter Bruce, Nitin Patel,\\nPeter Gedeck, Inbal Y ahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with\\neditions for R, Python , Excel, and JMP).\\nStrategies for Imbalanced Data\\nThe previous section dealt with evaluation of classification models using metrics that\\ngo beyond simple accuracy and are suitable for imbalanced data—data in which the\\noutcome of interest (purchase on a website, insurance fraud, etc.) is rare. In this sec‐\\ntion, we look at additional strategies that can improve predictive modeling perfor‐\\nmance with imbalanced data.\\nKey Terms for Imbalanced Data\\nUndersample\\nUse fewer of the prevalent class records in the classification model.\\nSynonym\\nDownsample\\nOversample\\nUse more of the rare class records in the classification model, bootstrapping if\\nnecessary.\\nSynonym\\nUpsample\\nUp weight or down weight\\nAttach more (or less) weight to the rare (or prevalent) class in the model.\\nData generation\\nLike bootstrapping, except each new bootstrapped record is slightly different\\nfrom its source.\\nz-score\\nThe value that results after standardization.\\nK\\nThe number of neighbors considered in the nearest neighbor calculation.\\n230 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 247}),\n",
       " Document(page_content=\"Undersampling\\nIf you have enough data, as is the case with the loan data, one solution is to under‐\\nsample  (or downsample) the prevalent class, so the data to be modeled is more bal‐\\nanced between 0s and 1s. The basic idea in undersampling is that the data for the\\ndominant class has many redundant records. Dealing with a smaller, more balanced\\ndata set yields benefits in model performance, and it makes it easier to prepare the\\ndata and to explore and pilot models.\\nHow much data is enough? It depends on the application, but in general, having tens\\nof thousands of records for the less dominant class is enough. The more easily distin‐\\nguishable the 1s are from the 0s, the less data needed.\\nThe loan data analyzed in “Logistic Regression” on page 208 was based on a balanced\\ntraining set: half of the loans were paid off, and the other half were in default. The\\npredicted values were similar: half of the probabilities were less than 0.5, and half\\nwere greater than 0.5. In the full data set, only about 19% of the loans were in default,\\nas shown in R:\\nmean(full_train_set $outcome=='default' )\\n[1] 0.1889455\\nIn Python :\\nprint('percentage of loans in default: ' ,\\n      100 * np.mean(full_train_set .outcome == 'default' ))\\nWhat happens if we use the full data set to train the model? Let’s see what this looks\\nlike in R:\\nfull_model  <- glm(outcome ~ payment_inc_ratio  + purpose_  + home_ +\\n                            emp_len_ + dti + revol_bal  + revol_util ,\\n                 data=full_train_set , family='binomial' )\\npred <- predict(full_model )\\nmean(pred > 0)\\n[1] 0.003942094\\nAnd in Python :\\npredictors  = ['payment_inc_ratio' , 'purpose_' , 'home_', 'emp_len_' ,\\n              'dti', 'revol_bal' , 'revol_util' ]\\noutcome = 'outcome'\\nX = pd.get_dummies (full_train_set [predictors ], prefix='', prefix_sep ='',\\n                   drop_first =True)\\ny = full_train_set [outcome]\\nfull_model  = LogisticRegression (penalty='l2', C=1e42, solver='liblinear' )\\nfull_model .fit(X, y)\\nprint('percentage of loans predicted to default: ' ,\\n      100 * np.mean(full_model .predict(X) == 'default' ))\\nStrategies for Imbalanced Data | 231\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 248}),\n",
       " Document(page_content='5Due to differences in implementation, results in Python  differ slightly: 1%, or about 1/18 of the expected\\nnumber.Only 0.39% of the loans are predicted to be in default, or less than 1/47 of the\\nexpected number.5 The loans that were paid off overwhelm the loans in default\\nbecause the model is trained using all the data equally. Thinking about it intuitively,\\nthe presence of so many nondefaulting loans, coupled with the inevitable variability\\nin predictor data, means that, even for a defaulting loan, the model is likely to find\\nsome nondefaulting loans that it is similar to, by chance. When a balanced sample\\nwas used, roughly 50% of the loans were predicted to be in default.\\nOversampling and Up/Down Weighting\\nOne criticism of the undersampling method is that it throws away data and is not\\nusing all the information at hand.  If you have a relatively small data set, and the rarer\\nclass contains a few hundred or a few thousand records, then undersampling the\\ndominant class has the risk of throwing out useful information. In this case, instead\\nof downsampling the dominant case, you should oversample (upsample) the rarer\\nclass by drawing additional rows with replacement (bootstrapping).\\nY ou can achieve a similar effect by weighting the data.  Many classification algorithms\\ntake a weight argument that will allow you to up/down weight the data. For example,\\napply a weight vector to the loan data using the weight  argument to glm in R:\\nwt <- ifelse(full_train_set $outcome==\\'default\\' ,\\n             1 / mean(full_train_set $outcome == \\'default\\' ), 1)\\nfull_model  <- glm(outcome ~ payment_inc_ratio  + purpose_  + home_ +\\n                            emp_len_ + dti + revol_bal  + revol_util ,\\n                  data=full_train_set , weight=wt, family=\\'quasibinomial\\' )\\npred <- predict(full_model )\\nmean(pred > 0)\\n[1] 0.5767208\\nMost scikit-learn  methods allow specifying weights in the fit function using the\\nkeyword argument sample_weight :\\ndefault_wt  = 1 / np.mean(full_train_set .outcome == \\'default\\' )\\nwt = [default_wt  if outcome == \\'default\\'  else 1\\n      for outcome in full_train_set .outcome]\\nfull_model  = LogisticRegression (penalty=\"l2\", C=1e42, solver=\\'liblinear\\' )\\nfull_model .fit(X, y, sample_weight =wt)\\nprint(\\'percentage of loans predicted to default (weighting): \\' ,\\n      100 * np.mean(full_model .predict(X) == \\'default\\' ))\\nThe weights for loans that default are set to 1\\np, where p is the probability of default.\\nThe nondefaulting loans have a weight of 1. The sums of the weights for the default‐\\n232 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 249}),\n",
       " Document(page_content='ing loans and nondefaulting loans are roughly equal. The mean of the predicted val‐\\nues is now about 58% instead of 0.39%.\\nNote that weighting provides an alternative to both upsampling the rarer class and\\ndownsampling the dominant class.\\nAdapting the Loss Function\\nMany classification and regression algorithms optimize a certain\\ncriteria or loss function . For example, logistic regression attempts to\\nminimize the deviance. In the literature, some propose to modify\\nthe loss function in order to avoid the problems caused by a rare\\nclass. In practice, this is hard to do: classification algorithms can be\\ncomplex and difficult to modify. Weighting is an easy way to\\nchange the loss function, discounting errors for records with low\\nweights in favor of records with higher weights.\\nData Generation\\nA variation of upsampling via bootstrapping (see “Oversampling and Up/Down\\nWeighting”  on page 232) is data generation  by perturbing existing records to create\\nnew records. The intuition behind this idea is that since we observe only a limited set\\nof instances, the algorithm doesn’t have a rich set of information to build classifica‐\\ntion “rules. ” By creating new records that are similar but not identical to existing\\nrecords, the algorithm has a chance to learn a more robust set of rules. This notion is\\nsimilar in spirit to ensemble statistical models such as boosting and bagging (see\\nChapter 6 ).\\nThe idea gained traction with the publication of the SMOTE  algorithm, which stands\\nfor “Synthetic Minority Oversampling Technique. ” The SMOTE algorithm finds a\\nrecord that is similar to the record being upsampled (see “K-Nearest Neighbors”  on\\npage 238) and creates a synthetic record that is a randomly weighted average of the\\noriginal record and the neighboring record, where the weight is generated separately\\nfor each predictor. The number of synthetic oversampled records created depends on\\nthe oversampling ratio required to bring the data set into approximate balance with\\nrespect to outcome classes.\\nThere are several implementations of SMOTE in R. The most comprehensive package\\nfor handling unbalanced data is unbalanced . It offers a variety of techniques, includ‐\\ning a “Racing” algorithm to select the best method. However, the SMOTE algorithm\\nis simple enough that it can be implemented directly in R using the FNN package.\\nThe Python  package imbalanced-learn  implements a variety of methods with an API\\nthat is compatible with scikit-learn . It provides various methods for over- and\\nStrategies for Imbalanced Data | 233', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 250}),\n",
       " Document(page_content='undersampling and support for using these techniques with boosting and bagging\\nclassifiers.\\nCost-Based Classification\\nIn practice, accuracy and AUC are a poor man’s way to choose a classification rule.\\nOften, an estimated cost can be assigned to false positives versus false negatives, and\\nit is more appropriate to incorporate these costs to determine the best cutoff when\\nclassifying 1s and 0s. For example, suppose the expected cost of a default of a new\\nloan is C and the expected return from a paid-off loan is R. Then the expected return\\nfor that loan is:\\nexpected return = PY= 0 ×R+PY= 1 ×C\\nInstead of simply labeling a loan as default or paid off, or determining the probability\\nof default, it makes more sense to determine if the loan has a positive expected\\nreturn. Predicted probability of default is an intermediate step, and it must be com‐\\nbined with the loan’s total value to determine expected profit, which is the ultimate\\nplanning metric of business. For example, a smaller value loan might be passed over\\nin favor of a larger one with a slightly higher predicted default probability.\\nExploring the Predictions\\nA single metric, such as AUC, cannot evaluate all aspects of the suitability of a model\\nfor a situation. Figure 5-8  displays the decision rules for four different models fit to\\nthe loan data using just two predictor variables: borrower_score  and pay\\nment_inc_ratio . The models are linear discriminant analysis (LDA), logistic linear\\nregression, logistic regression fit using a generalized additive model (GAM), and a\\ntree model (see “Tree Models” on page 249). The region to the upper left of the lines\\ncorresponds to a predicted default. LDA and logistic linear regression give nearly\\nidentical results in this case. The tree model produces the least regular rule, with two\\nsteps. Finally, the GAM fit of the logistic regression represents a compromise between\\nthe tree model and the linear model.\\n234 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 251}),\n",
       " Document(page_content='Figure 5-8. Comparison of the classification  rules for four different  methods\\nIt is not easy to visualize the prediction rules in higher dimensions or, in the case of\\nthe GAM and the tree model, even to generate the regions for such rules.\\nIn any case, exploratory analysis of predicted values is always warranted.\\nKey Ideas\\n•Highly imbalanced data (i.e., where the interesting outcomes, the 1s, are rare) are\\nproblematic for classification algorithms.\\n•One strategy for working with imbalanced data is to balance the training data via\\nundersampling the abundant case (or oversampling the rare case).\\n•If using all the 1s still leaves you with too few 1s, you can bootstrap the rare cases,\\nor use SMOTE to create synthetic data similar to existing rare cases.\\n•Imbalanced data usually indicates that correctly classifying one class (the 1s) has\\nhigher value, and that value ratio should be built into the assessment metric.\\nStrategies for Imbalanced Data | 235', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 252}),\n",
       " Document(page_content='Further Reading\\n•Tom Fawcett, author of Data Science for Business , has a good article on imbal‐\\nanced classes .\\n•For more on SMOTE, see Nitesh V . Chawla, Kevin W . Bowyer, Lawrence O. Hall,\\nand W . Philip Kegelmeyer, “SMOTE: Synthetic Minority Over-sampling Techni‐\\nque, ”  Journal of Artificial  Intelligence Research  16 (2002): 321–357.\\n•Also see the Analytics Vidhya Content Team’s “Practical Guide to Deal with\\nImbalanced Classification Problems in R, ” March 28, 2016.\\nSummary\\nClassification, the process of predicting which of two or more categories a record\\nbelongs to, is a fundamental tool of predictive analytics. Will a loan default (yes or\\nno)? Will it prepay? Will a web visitor click on a link? Will they purchase something?\\nIs an insurance claim fraudulent? Often in classification problems, one class is of pri‐\\nmary interest (e.g., the fraudulent insurance claim), and in binary classification, this\\nclass is designated as a 1, with the other, more prevalent class being a 0. Often, a key\\npart of the process is estimating a propensity score , a probability of belonging to the\\nclass of interest. A common scenario is one in which the class of interest is relatively\\nrare. When evaluating a classifier, there are a variety of model assessment metrics that\\ngo beyond simple accuracy; these are important in the rare-class situation, when clas‐\\nsifying all records as 0s can yield high accuracy.\\n236 | Chapter 5: Classification', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 253}),\n",
       " Document(page_content='CHAPTER 6\\nStatistical Machine Learning\\nRecent advances in statistics have been devoted to developing more powerful auto‐\\nmated techniques for predictive modeling—both regression and classification. These\\nmethods, like those discussed in the previous chapter, are supervised methods —they\\nare trained on data where outcomes are known and learn to predict outcomes in new\\ndata.  They fall under the umbrella of statistical machine learning  and are distin‐\\nguished from classical statistical methods in that they are data-driven and do not seek\\nto impose linear or other overall structure on the data. The K-Nearest Neighbors\\nmethod, for example, is quite simple: classify a record in accordance with how similar\\nrecords are classified. The most successful and widely used techniques are based on\\nensemble learning  applied to decision trees . The basic idea of ensemble learning is to\\nuse many models to form a prediction, as opposed to using just a single model. Deci‐\\nsion trees are a flexible and automatic technique to learn rules about the relationships\\nbetween predictor variables and outcome variables. It turns out that the combination\\nof ensemble learning with decision trees leads to some of the best performing off-the-\\nshelf predictive modeling techniques.\\nThe development of many of the techniques in statistical machine learning can be\\ntraced back to the statisticians Leo Breiman (see Figure 6-1 ) at the University of Cali‐\\nfornia at Berkeley and Jerry Friedman at Stanford University.  Their work, along with\\nthat of other researchers at Berkeley and Stanford, started with the development of\\ntree models in 1984. The subsequent development of ensemble methods of bagging\\nand boosting in the 1990s established the foundation of statistical machine learning.\\n237', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 254}),\n",
       " Document(page_content='1This and subsequent sections in this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter\\nGedeck; used with permission.\\nFigure 6-1. Leo Breiman, who was a professor of statistics at UC Berkeley, was at the\\nforefront of the development of many techniques in a data scientist’s toolkit today\\nMachine Learning Versus Statistics\\nIn the context of predictive modeling, what is the difference\\nbetween machine learning and statistics? There is not a bright line\\ndividing the two disciplines. Machine learning tends to be focused\\nmore on developing efficient algorithms that scale to large data in\\norder to optimize the predictive model. Statistics generally pays\\nmore attention to the probabilistic theory and underlying structure\\nof the model. Bagging, and the random forest (see “Bagging and\\nthe Random Forest” on page 259), grew up firmly in the statistics\\ncamp. Boosting (see “Boosting”  on page 270), on the other hand,\\nhas been developed in both disciplines but receives more attention\\non the machine learning side of the divide. Regardless of the his‐\\ntory, the promise of boosting ensures that it will thrive as a techni‐\\nque in both statistics and machine learning.\\nK-Nearest Neighbors\\nThe idea behind K-Nearest Neighbors (KNN) is very simple.1 For each record to be\\nclassified or predicted:\\n1.Find K records that have similar features (i.e., similar predictor values).\\n2.For classification, find out what the majority class is among those similar records\\nand assign that class to the new record.\\n3.For prediction (also called KNN regression ), find the average among those similar\\nrecords, and predict that average for the new record.\\n238 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 255}),\n",
       " Document(page_content='Key Terms for K-Nearest Neighbors\\nNeighbor\\nA record that has similar predictor values to another record.\\nDistance metrics\\nMeasures that sum up in a single number how far one record is from another.\\nStandardization\\nSubtract the mean and divide by the standard deviation.\\nSynonym\\nNormalization\\nz-score\\nThe value that results after standardization.\\nK\\nThe number of neighbors considered in the nearest neighbor calculation.\\nKNN is one of the simpler prediction/classification techniques: there is no model to\\nbe fit (as in regression). This doesn’t mean that using KNN is an automatic proce‐\\ndure. The prediction results depend on how the features are scaled, how similarity is\\nmeasured, and how big K is set. Also, all predictors must be in numeric form. We will\\nillustrate how to use the KNN method with a classification example.\\nA Small Example: Predicting Loan Default\\nTable 6-1  shows a few records of personal loan data from LendingClub. LendingClub\\nis a leader in peer-to-peer lending in which pools of investors make personal loans to\\nindividuals.  The goal of an analysis would be to predict the outcome of a new poten‐\\ntial loan: paid off versus default.\\nTable 6-1. A few records and columns for LendingClub loan data\\nOutcome Loan amount Income Purpose Years employed Home ownership State\\nPaid off 10000 79100 debt_consolidation 11 MORTGAGE NV\\nPaid off 9600 48000 moving 5 MORTGAGE TN\\nPaid off 18800 120036 debt_consolidation 11 MORTGAGE MD\\nDefault 15250 232000 small_business 9 MORTGAGE CA\\nPaid off 17050 35000 debt_consolidation 4 RENT MD\\nPaid off 5500 43000 debt_consolidation 4 RENT KS\\nK-Nearest Neighbors | 239', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 256}),\n",
       " Document(page_content=\"2For this example, we take the first row in the loan200  data set as the newloan  and exclude it from the data set\\nfor training.Consider a very simple model with just two predictor variables: dti, which is the\\nratio of debt payments (excluding mortgage) to income, and payment_inc_ratio ,\\nwhich is the ratio of the loan payment to income. Both ratios are multiplied by 100.\\nUsing a small set of 200 loans, loan200 , with known binary outcomes (default or no-\\ndefault, specified in the predictor outcome200 ), and with K set to 20, the KNN esti‐\\nmate for a new loan to be predicted, newloan , with dti=22.5  and\\npayment_inc_ratio=9  can be calculated in R as follows:2\\nnewloan <- loan200[1, 2:3, drop=FALSE]\\nknn_pred  <- knn(train=loan200[-1, 2:3], test=newloan, cl=loan200[-1, 1], k=20)\\nknn_pred  == 'paid off'\\n[1] TRUE\\nThe KNN prediction is for the loan to default.\\nWhile R has a native knn function, the contributed R package FNN, for Fast Nearest\\nNeighbor , scales more effectively to big data and provides more flexibility.\\nThe scikit-learn  package provides a fast and efficient implementation of KNN in\\nPython :\\npredictors  = ['payment_inc_ratio' , 'dti']\\noutcome = 'outcome'\\nnewloan = loan200.loc[0:0, predictors ]\\nX = loan200.loc[1:, predictors ]\\ny = loan200.loc[1:, outcome]\\nknn = KNeighborsClassifier (n_neighbors =20)\\nknn.fit(X, y)\\nknn.predict(newloan)\\nFigure 6-2  gives a visual display of this example. The new loan to be predicted is the\\ncross in the middle. The squares (paid off) and circles (default) are the training data.\\nThe large black circle shows the boundary of the nearest 20 points. In this case, 9\\ndefaulted loans lie within the circle, as compared with 11 paid-off loans. Hence the\\npredicted outcome of the loan is paid off. Note that if we consider only three nearest\\nneighbors, the prediction would be that the loan defaults.\\n240 | Chapter 6: Statistical Machine Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 257}),\n",
       " Document(page_content='Figure 6-2. KNN prediction of loan default using two variables: debt-to-income ratio\\nand loan-payment-to-income ratio\\nWhile the output of KNN for classification is typically a binary\\ndecision, such as default or paid off in the loan data, KNN routines\\nusually offer the opportunity to output a probability (propensity)\\nbetween 0 and 1. The probability is based on the fraction of one\\nclass in the K nearest neighbors. In the preceding example, this\\nprobability of default would have been estimated at 9\\n20, or 0.45.\\nUsing a probability score lets you use classification rules other than\\nsimple majority votes (probability of 0.5). This is especially impor‐\\ntant in problems with imbalanced classes; see “Strategies for Imbal‐\\nanced Data” on page 230. For example, if the goal is to identify\\nmembers of a rare class, the cutoff would typically be set below\\n50%. One common approach is to set the cutoff at the probability\\nof the rare event.\\nDistance Metrics\\nSimilarity (nearness) is determined using a distance metric , which is a function that\\nmeasures how far two records ( x1, x2, …, xp) and ( u1, u2, …, up) are from one another.\\nThe most popular distance metric between two vectors is Euclidean distance . To\\nK-Nearest Neighbors | 241', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 258}),\n",
       " Document(page_content='measure the Euclidean distance between two vectors, subtract one from the other,\\nsquare the differences, sum them, and take the square root:\\nx1−u12+x2−u22+⋯+xp−up2.\\nAnother common distance metric for numeric data is Manhattan distance :\\n|x1−u1| + | x2−u2| +⋯+ |xp−up|\\nEuclidean distance corresponds to the straight-line distance between two points (e.g.,\\nas the crow flies). Manhattan distance is the distance between two points traversed in\\na single direction at a time (e.g., traveling along rectangular city blocks). For this rea‐\\nson, Manhattan distance is a useful approximation if similarity is defined as point-to-\\npoint travel time.\\nIn measuring distance between two vectors, variables (features) that are measured\\nwith comparatively large scale will dominate the measure. For example, for the loan\\ndata, the distance would be almost solely a function of the income and loan amount\\nvariables, which are measured in tens or hundreds of thousands. Ratio variables\\nwould count for practically nothing in comparison. We address this problem by\\nstandardizing the data; see “Standardization (Normalization, z-Scores)” on page 243 .\\nOther Distance Metrics\\nThere are numerous other metrics for measuring distance between\\nvectors. For numeric data, Mahalanobis distance  is attractive since\\nit accounts for the correlation between two variables. This is useful\\nsince if two variables are highly correlated, Mahalanobis will essen‐\\ntially treat these as a single variable in terms of distance. Euclidean\\nand Manhattan distance do not account for the correlation, effec‐\\ntively placing greater weight on the attribute that underlies those\\nfeatures. Mahalanobis distance is the Euclidean distance between\\nthe principal components (see “Principal Components Analysis”\\non page 284). The downside of using Mahalanobis distance is\\nincreased computational effort and complexity; it is computed\\nusing the covariance matrix  (see “Covariance Matrix” on page 202 ).\\nOne Hot Encoder\\nThe loan data in Table 6-1  includes several factor (string) variables.  Most statistical\\nand machine learning models require this type of variable to be converted to a series\\nof binary dummy variables conveying the same information, as in Table 6-2 . Instead\\nof a single variable denoting the home occupant status as “owns with a mortgage, ”\\n242 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 259}),\n",
       " Document(page_content='“owns with no mortgage, ” “rents, ” or “other, ” we end up with four binary variables.\\nThe first would be “owns with a mortgage—Y/N, ” the second would be “owns with no\\nmortgage—Y/N, ” and so on. This one predictor, home occupant status, thus yields a\\nvector with one 1 and three 0s that can be used in statistical and machine learning\\nalgorithms. The phrase one hot encoding  comes from digital circuit terminology,\\nwhere it describes circuit settings in which only one bit is allowed to be positive (hot).\\nTable 6-2. Representing home ownership factor data in Table 6-1  as a numeric dummy\\nvariable\\nOWNS_WITH_MORTGAGE OWNS_WITHOUT_MORTGAGE OTHER RENT\\n1 0 0 0\\n1 0 0 0\\n1 0 0 0\\n1 0 0 0\\n0 0 0 1\\n0 0 0 1\\nIn linear and logistic regression, one hot encoding causes problems\\nwith multicollinearity; see “Multicollinearity”  on page 172. In such\\ncases, one dummy is omitted (its value can be inferred from the\\nother values). This is not an issue with KNN and other methods\\ndiscussed in this book.\\nStandardization (Normalization, z-Scores)\\nIn measurement, we are often not so much interested in “how much” but in “how dif‐\\nferent from the average. ” Standardization, also called normalization , puts all variables\\non similar scales by subtracting the mean and dividing by the standard deviation; in\\nthis way, we ensure that a variable does not overly influence a model simply due to\\nthe scale of its original measurement:\\nz=x−x\\ns\\nThe result of this transformation is commonly referred to as a z-score . Measurements\\nare then stated in terms of “standard deviations away from the mean. ”\\nNormalization  in this statistical context is not to be confused with\\ndatabase normalization , which is the removal of redundant data\\nand the verification of data dependencies.\\nK-Nearest Neighbors | 243', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 260}),\n",
       " Document(page_content='For KNN and a few other procedures (e.g., principal components analysis and clus‐\\ntering), it is essential to consider standardizing the data prior to applying the proce‐\\ndure. To illustrate this idea, KNN is applied to the loan data using dti and\\npayment_inc_ratio  (see “ A Small Example: Predicting Loan Default”  on page 239)\\nplus two other variables: revol_bal , the total revolving credit available to the appli‐\\ncant in dollars, and revol_util , the percent of the credit being used. The new record\\nto be predicted is shown here:\\nnewloan\\n  payment_inc_ratio  dti revol_bal  revol_util\\n1            2.3932   1      1687        9.4\\nThe magnitude of revol_bal , which is in dollars, is much bigger than that of the\\nother variables. The knn function returns the index of the nearest neighbors as an\\nattribute nn.index , and this can be used to show the top-five closest rows in loan_df :\\nloan_df <- model.matrix (~ -1 + payment_inc_ratio  + dti + revol_bal  +\\n                          revol_util , data=loan_data )\\nnewloan <- loan_df[1, , drop=FALSE]\\nloan_df <- loan_df[-1,]\\noutcome <- loan_data [-1, 1]\\nknn_pred  <- knn(train=loan_df, test=newloan, cl=outcome, k=5)\\nloan_df[attr(knn_pred , \"nn.index\" ),]\\n        payment_inc_ratio   dti revol_bal  revol_util\\n35537             1.47212 1.46      1686       10.0\\n33652             3.38178 6.37      1688        8.4\\n25864             2.36303 1.39      1691        3.5\\n42954             1.28160 7.14      1684        3.9\\n43600             4.12244 8.98      1684        7.2\\nFollowing the model fit, we can use the kneighbors  method to identify the five clos‐\\nest rows in the training set with scikit-learn :\\npredictors  = [\\'payment_inc_ratio\\' , \\'dti\\', \\'revol_bal\\' , \\'revol_util\\' ]\\noutcome = \\'outcome\\'\\nnewloan = loan_data .loc[0:0, predictors ]\\nX = loan_data .loc[1:, predictors ]\\ny = loan_data .loc[1:, outcome]\\nknn = KNeighborsClassifier (n_neighbors =5)\\nknn.fit(X, y)\\nnbrs = knn.kneighbors (newloan)\\nX.iloc[nbrs[1][0], :]\\nThe value of revol_bal  in these neighbors is very close to its value in the new record,\\nbut the other predictor variables are all over the map and essentially play no role in\\ndetermining neighbors.\\n244 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 261}),\n",
       " Document(page_content='Compare this to KNN applied to the standardized data using the R function scale ,\\nwhich computes the z-score for each variable:\\nloan_df <- model.matrix (~ -1 + payment_inc_ratio  + dti + revol_bal  +\\n                          revol_util , data=loan_data )\\nloan_std  <- scale(loan_df)\\nnewloan_std  <- loan_std [1, , drop=FALSE]\\nloan_std  <- loan_std [-1,]\\nloan_df <- loan_df[-1,]  \\noutcome <- loan_data [-1, 1]\\nknn_pred  <- knn(train=loan_std , test=newloan_std , cl=outcome, k=5)\\nloan_df[attr(knn_pred , \"nn.index\" ),]\\n        payment_inc_ratio    dti  revol_bal   revol_util\\n2081            2.61091    1.03       1218         9.7\\n1439            2.34343    0.51        278         9.9\\n30216           2.71200    1.34       1075         8.5\\n28543           2.39760    0.74       2917         7.4\\n44738           2.34309    1.37        488         7.2\\nWe need to remove the first row from loan_df  as well, so that the row numbers\\ncorrespond to each other.\\nThe sklearn.preprocessing.StandardScaler  method is first trained with the pre‐\\ndictors and is subsequently used to transform the data set prior to training the KNN\\nmodel:\\nnewloan = loan_data .loc[0:0, predictors ]\\nX = loan_data .loc[1:, predictors ]\\ny = loan_data .loc[1:, outcome]\\nscaler = preprocessing .StandardScaler ()\\nscaler.fit(X * 1.0)\\nX_std = scaler.transform (X * 1.0)\\nnewloan_std  = scaler.transform (newloan * 1.0)\\nknn = KNeighborsClassifier (n_neighbors =5)\\nknn.fit(X_std, y)\\nnbrs = knn.kneighbors (newloan_std )\\nX.iloc[nbrs[1][0], :]\\nThe five nearest neighbors are much more alike in all the variables, providing a more\\nsensible result. Note that the results are displayed on the original scale, but KNN was\\napplied to the scaled data and the new loan to be predicted.\\nK-Nearest Neighbors | 245', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 262}),\n",
       " Document(page_content='Using the z-score is just one way to rescale variables. Instead of the\\nmean, a more robust estimate of location could be used, such as the\\nmedian.  Likewise, a different estimate of scale such as the inter‐\\nquartile range could be used instead of the standard deviation.\\nSometimes, variables are “squashed” into the 0–1 range. It’s also\\nimportant to realize that scaling each variable to have unit variance\\nis somewhat arbitrary. This implies that each variable is thought to\\nhave the same importance in predictive power. If you have subjec‐\\ntive knowledge that some variables are more important than oth‐\\ners, then these could be scaled up. For example, with the loan data,\\nit is reasonable to expect that the payment-to-income ratio is very\\nimportant.\\nNormalization (standardization) does not change the distributional\\nshape of the data; it does not make it normally shaped if it was not\\nalready normally shaped (see “Normal Distribution” on page 69 ).\\nChoosing K\\nThe choice of K is very important to the performance of KNN. The simplest choice is\\nto set K= 1, known as the 1-nearest neighbor classifier. The prediction is intuitive: it\\nis based on finding the data record in the training set most similar to the new record\\nto be predicted. Setting K= 1 is rarely the best choice; you’ll almost always obtain\\nsuperior performance by using K > 1-nearest neighbors.\\nGenerally speaking, if K is too low, we may be overfitting: including the noise in the\\ndata. Higher values of K provide smoothing that reduces the risk of overfitting in the\\ntraining data. On the other hand, if K is too high, we may oversmooth the data and\\nmiss out on KNN’s ability to capture the local structure in the data, one of its main\\nadvantages.\\nThe K that best balances between overfitting and oversmoothing is typically deter‐\\nmined by accuracy metrics and, in particular, accuracy with holdout or validation\\ndata. There is no general rule about the best K—it depends greatly on the nature of\\nthe data. For highly structured data with little noise, smaller values of K work best.\\nBorrowing a term from the signal processing community, this type of data is some‐\\ntimes referred to as having a high signal-to-noise ratio  (SNR ). Examples of data with a\\ntypically high SNR are data sets for handwriting and speech recognition. For noisy\\ndata with less structure (data with a low SNR), such as the loan data, larger values of\\nK are appropriate. Typically, values of K fall in the range 1 to 20. Often, an odd num‐\\nber is chosen to avoid ties.\\n246 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 263}),\n",
       " Document(page_content='Bias-Variance Trade-off\\nThe tension between oversmoothing and overfitting is an instance\\nof the bias-variance trade-off , a ubiquitous problem in statistical\\nmodel fitting. Variance refers to the modeling error that occurs\\nbecause of the choice of training data; that is, if you were to choose\\na different set of training data, the resulting model would be differ‐\\nent. Bias refers to the modeling error that occurs because you have\\nnot properly identified the underlying real-world scenario; this\\nerror would not disappear if you simply added more training data.\\nWhen a flexible model is overfit, the variance increases. Y ou can\\nreduce this by using a simpler model, but the bias may increase due\\nto the loss of flexibility in modeling the real underlying situation.  A\\ngeneral approach to handling this trade-off is through cross-\\nvalidation . See “Cross-Validation” on page 155  for more details.\\nKNN as a Feature Engine\\nKNN gained its popularity due to its simplicity and intuitive nature. In terms of per‐\\nformance, KNN by itself is usually not competitive with more sophisticated classifica‐\\ntion techniques. In practical model fitting, however, KNN can be used to add “local\\nknowledge” in a staged process with other classification techniques:\\n1.KNN is run on the data, and for each record, a classification (or quasi-probability\\nof a class) is derived.\\n2.That result is added as a new feature to the record, and another classification\\nmethod is then run on the data. The original predictor variables are thus used\\ntwice.\\nAt first you might wonder whether this process, since it uses some predictors twice,\\ncauses a problem with multicollinearity (see “Multicollinearity”  on page 172). This is\\nnot an issue, since the information being incorporated into the second-stage model is\\nhighly local, derived only from a few nearby records, and is therefore additional\\ninformation and not redundant.\\nY ou can think of this staged use of KNN as a form of ensemble\\nlearning, in which multiple predictive modeling methods are used\\nin conjunction with one another. It can also be considered as a\\nform of feature engineering in which the aim is to derive features\\n(predictor variables) that have predictive power. Often this involves\\nsome manual review of the data; KNN gives a fairly automatic way\\nto do this.\\nK-Nearest Neighbors | 247', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 264}),\n",
       " Document(page_content='For example, consider the King County housing data. In pricing a home for sale, a\\nrealtor will base the price on similar homes recently sold, known as “comps. ” In\\nessence, realtors are doing a manual version of KNN: by looking at the sale prices of\\nsimilar homes, they can estimate what a home will sell for. We can create a new fea‐\\nture for a statistical model to mimic the real estate professional by applying KNN to\\nrecent sales. The predicted value is the sales price, and the existing predictor variables\\ncould include location, total square feet, type of structure, lot size, and number of\\nbedrooms and bathrooms. The new predictor variable (feature) that we add via KNN\\nis the KNN predictor for each record (analogous to the realtors’ comps). Since we are\\npredicting a numerical value, the average of the K-Nearest Neighbors is used instead\\nof a majority vote (known as KNN regression ).\\nSimilarly, for the loan data, we can create features that represent different aspects of\\nthe loan process. For example, the following R code would build a feature that repre‐\\nsents a borrower’s creditworthiness:\\nborrow_df  <- model.matrix (~ -1 + dti + revol_bal  + revol_util  + open_acc  +\\n                            delinq_2yrs_zero  + pub_rec_zero , data=loan_data )\\nborrow_knn  <- knn(borrow_df , test=borrow_df , cl=loan_data [, \\'outcome\\' ],\\n                  prob=TRUE, k=20)\\nprob <- attr(borrow_knn , \"prob\")\\nborrow_feature  <- ifelse(borrow_knn  == \\'default\\' , prob, 1 - prob)\\nsummary(borrow_feature )\\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\\n  0.000   0.400   0.500   0.501   0.600   0.950\\nWith scikit-learn , we use the predict_proba  method of the trained model to get\\nthe probabilities:\\npredictors  = [\\'dti\\', \\'revol_bal\\' , \\'revol_util\\' , \\'open_acc\\' ,\\n              \\'delinq_2yrs_zero\\' , \\'pub_rec_zero\\' ]\\noutcome = \\'outcome\\'\\nX = loan_data [predictors ]\\ny = loan_data [outcome]\\nknn = KNeighborsClassifier (n_neighbors =20)\\nknn.fit(X, y)\\nloan_data [\\'borrower_score\\' ] = knn.predict_proba (X)[:, 1]\\nloan_data [\\'borrower_score\\' ].describe ()\\nThe result is a feature that predicts the likelihood a borrower will default based on his\\ncredit history.\\n248 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 265}),\n",
       " Document(page_content='3The term CART is a registered trademark of Salford Systems related to their specific implementation of tree\\nmodels.Key Ideas\\n•K-Nearest Neighbors (KNN) classifies a record by assigning it to the class that\\nsimilar records belong to.\\n•Similarity (distance) is determined by Euclidian distance or other related metrics.\\n•The number of nearest neighbors to compare a record to, K, is determined by\\nhow well the algorithm performs on training data, using different values for K.\\n•Typically, the predictor variables are standardized so that variables of large scale\\ndo not dominate the distance metric.\\n•KNN is often used as a first stage in predictive modeling, and the predicted value\\nis added back into the data as a predictor  for second-stage (non-KNN) modeling.\\nTree Models\\nTree models, also called Classification  and Regression Trees  (CART ),3 decision trees , or\\njust trees , are an effective and popular classification (and regression) method initially\\ndeveloped by Leo Breiman and others in 1984.  Tree models, and their more powerful\\ndescendants random forests  and boosted trees  (see “Bagging and the Random Forest”\\non page 259 and “Boosting”  on page 270), form the basis for the most widely\\nused and powerful predictive modeling tools in data science for regression and\\nclassification.\\nKey Terms for Trees\\nRecursive partitioning\\nRepeatedly dividing and subdividing the data with the goal of making the out‐\\ncomes in each final subdivision as homogeneous as possible.\\nSplit value\\nA predictor value that divides the records into those where that predictor is less\\nthan the split value, and those where it is more.\\nNode\\nIn the decision tree, or in the set of corresponding branching rules, a node is the\\ngraphical or rule representation of a split value.\\nTree Models | 249', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 266}),\n",
       " Document(page_content='Leaf\\nThe end of a set of if-then rules, or branches of a tree—the rules that bring you to\\nthat leaf provide one of the classification rules for any record in a tree.\\nLoss\\nThe number of misclassifications at a stage in the splitting process; the more los‐\\nses, the more impurity.\\nImpurity\\nThe extent to which a mix of classes is found in a subpartition of the data (the\\nmore mixed, the more impure).\\nSynonym\\nHeterogeneity\\nAntonyms\\nHomogeneity, purity\\nPruning\\nThe process of taking a fully grown tree and progressively cutting its branches\\nback to reduce overfitting.\\nA tree model is a set of “if-then-else” rules that are easy to understand and to imple‐\\nment.  In contrast to linear and logistic regression, trees have the ability to discover\\nhidden patterns corresponding to complex interactions in the data. However, unlike\\nKNN or naive Bayes, simple tree models can be expressed in terms of predictor rela‐\\ntionships that are easily interpretable.\\nDecision Trees in Operations Research\\nThe term decision trees  has a different (and older) meaning in deci‐\\nsion science and operations research, where it refers to a human\\ndecision analysis process. In this meaning, decision points, possible\\noutcomes, and their estimated probabilities are laid out in a\\nbranching diagram, and the decision path with the maximum\\nexpected value is chosen.\\nA Simple Example\\nThe two main packages to fit tree models in R are rpart  and tree . Using the rpart\\npackage, a model is fit to a sample of 3,000 records of the loan data using the variables\\npayment_inc_ratio  and borrower_score  (see “K-Nearest Neighbors”  on page 238\\nfor a description of the data):\\nlibrary(rpart)\\nloan_tree  <- rpart(outcome ~ borrower_score  + payment_inc_ratio ,\\n                   data=loan3000 , control=rpart.control (cp=0.005))\\n250 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 267}),\n",
       " Document(page_content=\"plot(loan_tree , uniform=TRUE, margin=0.05)\\ntext(loan_tree )\\nThe sklearn.tree.DecisionTreeClassifier  provides an implementation of a deci‐\\nsion tree. The dmba  package provides a convenience function to create a visualization\\ninside a Jupyter notebook:\\npredictors  = ['borrower_score' , 'payment_inc_ratio' ]\\noutcome = 'outcome'\\nX = loan3000 [predictors ]\\ny = loan3000 [outcome]\\nloan_tree  = DecisionTreeClassifier (random_state =1, criterion ='entropy' ,\\n                                   min_impurity_decrease =0.003)\\nloan_tree .fit(X, y)\\nplotDecisionTree (loan_tree , feature_names =predictors ,\\n                 class_names =loan_tree .classes_ )\\nThe resulting tree is shown in Figure 6-3 . Due to the different implementations, you\\nwill find that the results from R and Python  are not identical; this is expected. These\\nclassification rules are determined by traversing through a hierarchical tree, starting\\nat the root and moving left if the node is true and right if not, until a leaf is reached.\\nTypically, the tree is plotted upside-down, so the root is at the top and the leaves are\\nat the bottom. For example, if we get a loan with borrower_score  of 0.6 and a\\npayment_inc_ratio  of 8.0, we end up at the leftmost leaf and predict the loan will be\\npaid off.\\nFigure 6-3. The rules for a simple tree model fit to the loan data\\nTree Models | 251\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 268}),\n",
       " Document(page_content='A nicely printed version of the tree is also easily produced in R:\\nloan_tree\\nn= 3000\\nnode), split, n, loss, yval, (yprob)\\n    * denotes terminal  node\\n1) root 3000 1445 paid off (0.5183333  0.4816667 )\\n  2) borrower_score >=0.575 878  261 paid off (0.7027335  0.2972665 ) *\\n  3) borrower_score < 0.575 2122  938 default (0.4420358  0.5579642 )\\n    6) borrower_score >=0.375 1639  802 default (0.4893228  0.5106772 )\\n      12) payment_inc_ratio < 10.42265  1157  547 paid off (0.5272256  0.4727744 )\\n        24) payment_inc_ratio < 4.42601 334  139 paid off (0.5838323  0.4161677 ) *\\n        25) payment_inc_ratio >=4.42601 823  408 paid off (0.5042527  0.4957473 )\\n          50) borrower_score >=0.475 418  190 paid off (0.5454545  0.4545455 ) *\\n          51) borrower_score < 0.475 405  187 default (0.4617284  0.5382716 ) *\\n      13) payment_inc_ratio >=10.42265  482  192 default (0.3983402  0.6016598 ) *\\n    7) borrower_score < 0.375 483  136 default (0.2815735  0.7184265 ) *\\nThe depth of the tree is shown by the indent. Each node corresponds to a provisional\\nclassification determined by the prevalent outcome in that partition. The “loss” is the\\nnumber of misclassifications yielded by the provisional classification in a partition.\\nFor example, in node 2, there were 261 misclassifications out of a total of 878 total\\nrecords. The values in the parentheses correspond to the proportion of records that\\nare paid off or in default, respectively. For example, in node 13, which predicts\\ndefault, over 60 percent of the records are loans that are in default.\\nThe scikit-learn  documentation describes how to create a text representation of a\\ndecision tree model. We included a convenience function in our dmba  package:\\nprint(textDecisionTree (loan_tree ))\\n--\\nnode=0 test node: go to node 1 if 0 <= 0.5750000178813934  else to node 6\\n  node=1 test node: go to node 2 if 0 <= 0.32500000298023224  else to node 3\\n    node=2 leaf node: [[0.785, 0.215]]\\n    node=3 test node: go to node 4 if 1 <= 10.42264986038208  else to node 5\\n      node=4 leaf node: [[0.488, 0.512]]\\n      node=5 leaf node: [[0.613, 0.387]]\\n  node=6 test node: go to node 7 if 1 <= 9.19082498550415  else to node 10\\n    node=7 test node: go to node 8 if 0 <= 0.7249999940395355  else to node 9\\n      node=8 leaf node: [[0.247, 0.753]]\\n      node=9 leaf node: [[0.073, 0.927]]\\n    node=10 leaf node: [[0.457, 0.543]]\\nThe Recursive Partitioning Algorithm\\nThe algorithm to construct a decision tree, called recursive partitioning , is straightfor‐\\nward and intuitive. The data is repeatedly partitioned using predictor values that do\\nthe best job of separating the data into relatively homogeneous partitions. Figure 6-4\\n252 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 269}),\n",
       " Document(page_content='shows the partitions created for the tree in Figure 6-3 . The first rule, depicted by rule\\n1, is borrower_score >= 0.575  and segments the right portion of the plot. The sec‐\\nond rule is borrower_score < 0.375  and segments the left portion.\\nFigure 6-4. The first three rules for a simple tree model fit to the loan data\\nSuppose we have a response variable Y and a set of P predictor variables Xj for\\nj= 1,⋯,P. For a partition A of records, recursive partitioning will find the best way\\nto partition A into two subpartitions:\\n1.For each predictor variable Xj:\\na.For each value sj of Xj:\\ni.Split the records in A with Xj values < sj as one partition, and the remaining\\nrecords where Xj ≥ sj as another partition.\\nii.Measure the homogeneity of classes within each subpartition of A.\\nb.Select the value of sj that produces maximum within-partition homogeneity of\\nclass.\\n2.Select the variable Xj and the split value sj that produces maximum within-\\npartition homogeneity of class.\\nTree Models | 253', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 270}),\n",
       " Document(page_content='Now comes the recursive part:\\n1.Initialize A with the entire data set.\\n2.Apply the partitioning algorithm to split A into two subpartitions, A1 and A2.\\n3.Repeat step 2 on subpartitions A1 and A2.\\n4.The algorithm terminates when no further partition can be made that sufficiently\\nimproves the homogeneity of the partitions.\\nThe end result is a partitioning of the data, as in Figure 6-4 , except in P-dimensions,\\nwith each partition predicting an outcome of 0 or 1 depending on the majority vote\\nof the response in that partition.\\nIn addition to a binary 0/1 prediction, tree models can produce a\\nprobability estimate based on the number of 0s and 1s in the parti‐\\ntion.  The estimate is simply the sum of 0s or 1s in the partition\\ndivided by the number of observations in the partition:\\nProb Y= 1 =Number of 1s in the partition\\nSize of the partition\\nThe estimated Prob Y= 1 can then be converted to a binary deci‐\\nsion; for example, set the estimate to 1 if Prob( Y = 1) > 0.5.\\nMeasuring Homogeneity or Impurity\\nTree models recursively create partitions (sets of records), A, that predict an outcome\\nof Y = 0 or Y = 1. Y ou can see from the preceding algorithm that we need a way to\\nmeasure homogeneity, also called class purity , within a partition. Or equivalently, we\\nneed to measure the impurity of a partition. The accuracy of the predictions is the\\nproportion p of misclassified records within that partition, which ranges from 0 (per‐\\nfect) to 0.5 (purely random guessing).\\nIt turns out that accuracy is not a good measure for impurity. Instead, two common\\nmeasures for impurity are the Gini impurity  and entropy  of information . While these\\n(and other) impurity measures apply to classification problems with more than two\\nclasses, we focus on the binary case. The Gini impurity for a set of records A is:\\nIA=p1 −p\\nThe entropy measure is given by:\\nIA= − plog2p−1 −plog21 −p\\n254 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 271}),\n",
       " Document(page_content='Figure 6-5  shows that Gini impurity (rescaled) and entropy measures are similar, with\\nentropy giving higher impurity scores for moderate and high accuracy rates.\\nFigure 6-5. Gini impurity and entropy measures\\nGini Coefficient\\nGini impurity is not to be confused with the Gini coefficient . They\\nrepresent similar concepts, but the Gini coefficient is limited to the\\nbinary classification problem and is related to the AUC metric (see\\n“ AUC” on page 226 ).\\nThe impurity metric is used in the splitting algorithm described earlier. For each pro‐\\nposed partition of the data, impurity is measured for each of the partitions that result\\nfrom the split. A weighted average is then calculated, and whichever partition (at each\\nstage) yields the lowest weighted average is selected.\\nTree Models | 255', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 272}),\n",
       " Document(page_content='Stopping the Tree from Growing\\nAs the tree grows bigger, the splitting rules become more detailed, and the tree gradu‐\\nally shifts from identifying “big” rules that identify real and reliable relationships in\\nthe data to “tiny” rules that reflect only noise. A fully grown tree results in completely\\npure leaves and, hence, 100% accuracy in classifying the data that it is trained on.\\nThis accuracy is, of course, illusory—we have overfit (see “Bias-Variance Trade-off ”\\non page 247) the data, fitting the noise in the training data, not the signal that we\\nwant to identify in new data.\\nWe need some way to determine when to stop growing a tree at a stage that will gen‐\\neralize to new data. There are various ways to stop splitting in R and Python :\\n•Avoid splitting a partition if a resulting subpartition is too small, or if a terminal\\nleaf is too small. In rpart  (R), these constraints are controlled separately by the\\nparameters minsplit  and minbucket , respectively, with defaults of 20 and 7. In\\nPython ’s DecisionTreeClassifier , we can control this using the parameters\\nmin_samples_split  (default 2) and min_samples_leaf  (default 1).\\n•Don’t split a partition if the new partition does not “significantly” reduce the\\nimpurity. In rpart , this is controlled by the complexity parameter  cp, which is a\\nmeasure of how complex a tree is—the more complex, the greater the value of cp.\\nIn practice, cp is used to limit tree growth by attaching a penalty to additional\\ncomplexity (splits) in a tree. DecisionTreeClassifier  (Python ) has the parame‐\\nter min_impurity_decrease , which limits splitting based on a weighted impurity\\ndecrease value. Here, smaller values will lead to more complex trees.\\nThese methods involve arbitrary rules and can be useful for exploratory work, but we\\ncan’t easily determine optimum values (i.e., values that maximize predictive accuracy\\nwith new data). We need to combine cross-validation with either systematically\\nchanging the model parameters or modifying the tree through pruning.\\nControlling tree complexity in R\\nWith the complexity parameter, cp, we can estimate what size tree will perform best\\nwith new data. If cp is too small, then the tree will overfit the data, fitting noise and\\nnot signal. On the other hand, if cp is too large, then the tree will be too small and\\nhave little predictive power. The default in rpart  is 0.01, although for larger data sets,\\nyou are likely to find this is too large. In the previous example, cp was set to 0.005\\nsince the default led to a tree with a single split. In exploratory analysis, it is sufficient\\nto simply try a few values.\\n256 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 273}),\n",
       " Document(page_content='Determining the optimum cp is an instance of the bias-variance trade-off. The most\\ncommon way to estimate a good value of cp is via cross-validation (see “Cross-\\nValidation” on page 155 ):\\n1.Partition the data into training and validation (holdout) sets.\\n2.Grow the tree with the training data.\\n3.Prune it successively, step by step, recording cp (using the training  data) at each\\nstep.\\n4.Note the cp that corresponds to the minimum error (loss) on the validation  data.\\n5.Repartition the data into training and validation, and repeat the growing, prun‐\\ning, and cp recording process.\\n6.Do this again and again, and average the cps that reflect minimum error for each\\ntree.\\n7.Go back to the original data, or future data, and grow a tree, stopping at this opti‐\\nmum cp value.\\nIn rpart , you can use the argument cptable  to produce a table of the cp values and\\ntheir associated cross-validation error ( xerror  in R), from which you can determine\\nthe cp value that has the lowest cross-validation error.\\nControlling tree complexity in Python\\nNeither the complexity parameter nor pruning is available in scikit-learn ’s decision\\ntree implementation. The solution is to use grid search over combinations of different\\nparameter values. For example, we can vary max_depth  in the range 5 to 30 and\\nmin_samples_split  between 20 and 100. The GridSearchCV  method in scikit-\\nlearn  is a convenient way to combine the exhaustive search through all combinations\\nwith cross-validation. An optimal parameter set is then selected using the cross-\\nvalidated model performance.\\nPredicting a Continuous Value\\nPredicting a continuous value (also termed regression ) with a tree follows the same\\nlogic and procedure, except that impurity is measured by squared deviations  from the\\nmean (squared errors) in each subpartition, and predictive performance is judged by\\nthe square root of the mean squared error (RMSE) (see “ Assessing the Model”  on\\npage 153 ) in each partition.\\nscikit-learn  has the sklearn.tree.DecisionTreeRegressor  method to train a\\ndecision tree regression model.\\nTree Models | 257', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 274}),\n",
       " Document(page_content='How Trees Are Used\\nOne of the big obstacles faced by predictive modelers in organizations is the per‐\\nceived “black box” nature of the methods they use, which gives rise to opposition\\nfrom other elements of the organization. In this regard, the tree model has two\\nappealing aspects:\\n•Tree models provide a visual tool for exploring the data, to gain an idea of what\\nvariables are important and how they relate to one another. Trees can capture\\nnonlinear relationships among predictor variables.\\n•Tree models provide a set of rules that can be effectively communicated to non‐\\nspecialists, either for implementation or to “sell” a data mining project.\\nWhen it comes to prediction, however, harnessing the results from multiple trees is\\ntypically more powerful than using just a single tree. In particular, the random forest\\nand boosted tree algorithms almost always provide superior predictive accuracy and\\nperformance (see “Bagging and the Random Forest” on page 259 and “Boosting”  on\\npage 270 ), but the aforementioned advantages of a single tree are lost.\\nKey Ideas\\n•Decision trees produce a set of rules to classify or predict an outcome.\\n•The rules correspond to successive partitioning of the data into subpartitions.\\n•Each partition, or split, references a specific value of a predictor variable and\\ndivides the data into records where that predictor value is above or below that\\nsplit value.\\n•At each stage, the tree algorithm chooses the split that minimizes the outcome\\nimpurity within each subpartition.\\n•When no further splits can be made, the tree is fully grown and each terminal\\nnode, or leaf, has records of a single class; new cases following that rule (split)\\npath would be assigned that class.\\n•A fully grown tree overfits the data and must be pruned back so that it captures\\nsignal and not noise.\\n•Multiple-tree algorithms like random forests and boosted trees yield better pre‐\\ndictive performance, but they lose the rule-based communicative power of single\\ntrees.\\n258 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 275}),\n",
       " Document(page_content='Further Reading\\n•Analytics Vidhya Content Team, “Tree Based Algorithms: A Complete Tutorial\\nfrom Scratch (in R & Python )”, April 12, 2016.\\n•Terry M. Therneau, Elizabeth J. Atkinson, and the Mayo Foundation, “ An Intro‐\\nduction to Recursive Partitioning Using the RPART Routines” , April 11, 2019.\\nBagging and the Random Forest\\nIn 1906, the statistician Sir Francis Galton was visiting a county fair in England, at\\nwhich a contest was being held to guess the dressed weight of an ox that was on\\nexhibit.  There were 800 guesses, and while the individual guesses varied widely, both\\nthe mean and the median came out within 1% of the ox’s true weight. James Suro‐\\nwiecki has explored this phenomenon in his book The Wisdom of Crowds  (Doubleday,\\n2004). This principle applies to predictive models as well: averaging (or taking major‐\\nity votes) of multiple models—an ensemble  of models—turns out to be more accurate\\nthan just selecting one model.\\nKey Terms for Bagging and the Random Forest\\nEnsemble\\nForming a prediction by using a collection of models.\\nSynonym\\nModel averaging\\nBagging\\nA general technique to form a collection of models by bootstrapping the data.\\nSynonym\\nBootstrap aggregation\\nRandom forest\\nA type of bagged estimate based on decision tree models.\\nSynonym\\nBagged decision trees\\nVariable importance\\nA measure of the importance of a predictor variable in the performance of the\\nmodel.\\nThe ensemble approach has been applied to and across many different modeling\\nmethods, most publicly in the Netflix Prize, in which Netflix offered a $1 million\\nBagging and the Random Forest | 259', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 276}),\n",
       " Document(page_content='prize to any contestant who came up with a model that produced a 10% improvement\\nin predicting the rating that a Netflix customer would award a movie. The simple ver‐\\nsion of ensembles is as follows:\\n1.Develop a predictive model and record the predictions for a given data set.\\n2.Repeat for multiple models on the same data.\\n3.For each record to be predicted, take an average (or a weighted average, or a\\nmajority vote) of the predictions.\\nEnsemble methods have been applied most systematically and effectively to decision\\ntrees. Ensemble tree models are so powerful that they provide a way to build good\\npredictive models with relatively little effort.\\nGoing beyond the simple ensemble algorithm, there are two main variants of ensem‐\\nble models: bagging  and boosting . In the case of ensemble tree models, these are\\nreferred to as random forest  models and boosted tree  models.  This section focuses on\\nbagging; boosting is covered in “Boosting” on page 270 .\\nBagging\\nBagging, which stands for “bootstrap aggregating, ” was introduced by Leo Breiman in\\n1994. Suppose we have a response Y and P predictor variables \\ud835=X1,X2,⋯,XP with\\nN records.\\nBagging is like the basic algorithm for ensembles, except that, instead of fitting the\\nvarious models to the same data, each new model is fitted to a bootstrap resample.\\nHere is the algorithm presented more formally:\\n1.Initialize M, the number of models to be fit, and n, the number of records to\\nchoose ( n < N). Set the iteration m= 1.\\n2.Take a bootstrap resample (i.e., with replacement) of n records from the training\\ndata to form a subsample Ym and \\ud835m (the bag).\\n3.Train a model using Ym and \\ud835m to create a set of decision rules fm\\ud835.\\n4.Increment the model counter m=m+ 1. If m <= M, go to step 2.\\nIn the case where fm predicts the probability Y= 1, the bagged estimate is given by:\\nf=1\\nMf1\\ud835+f2\\ud835+⋯+fM\\ud835\\n260 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 277}),\n",
       " Document(page_content='4The term random forest  is a trademark of Leo Breiman and Adele Cutler and licensed to Salford Systems.\\nThere is no standard nontrademark name, and the term random forest is as synonymous with the algorithm\\nas Kleenex is with facial tissues.Random Forest\\nThe random forest  is based on applying bagging to decision trees, with one important\\nextension: in addition to sampling the records, the algorithm also samples the vari‐\\nables.4 In traditional decision trees, to determine how to create a subpartition of a\\npartition A, the algorithm makes the choice of variable and split point by minimizing\\na criterion such as Gini impurity (see “Measuring Homogeneity or Impurity”  on page\\n254). With random forests, at each stage of the algorithm, the choice of variable is\\nlimited to a random subset of variables . Compared to the basic tree algorithm (see\\n“The Recursive Partitioning Algorithm” on page 252), the random forest algorithm\\nadds two more steps: the bagging discussed earlier (see “Bagging and the Random\\nForest” on page 259 ), and the bootstrap sampling of variables at each split:\\n1.Take a bootstrap (with replacement) subsample from the records .\\n2.For the first split, sample p < P variables  at random without replacement.\\n3.For each of the sampled variables Xj1,Xj2, ...,Xjp, apply the splitting\\nalgorithm:\\na.For each value sjk of Xjk:\\ni.Split the records in partition A, with Xj(k) < sj(k) as one partition and the\\nremaining records where Xjk≥sjk as another partition.\\nii.Measure the homogeneity of classes within each subpartition of A.\\nb.Select the value of sjk that produces maximum within-partition homogeneity\\nof class.\\n4.Select the variable Xjk and the split value sjk that produces maximum within-\\npartition homogeneity of class.\\n5.Proceed to the next split and repeat the previous steps, starting with step 2.\\n6.Continue with additional splits, following the same procedure until the tree is\\ngrown.\\n7.Go back to step 1, take another bootstrap subsample, and start the process over\\nagain.\\nHow many variables to sample at each step? A rule of thumb is to choose P where P\\nis the number of predictor variables. The package randomForest  implements the ran‐\\ndom forest in R. The following applies this package to the loan data (see “K-Nearest\\nNeighbors” on page 238  for a description of the data):\\nBagging and the Random Forest | 261', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 278}),\n",
       " Document(page_content=\"rf <- randomForest (outcome ~ borrower_score  + payment_inc_ratio ,\\n                   data=loan3000 )\\nrf\\nCall:\\n randomForest (formula = outcome ~ borrower_score  + payment_inc_ratio ,\\n     data = loan3000 )\\n            Type of random forest: classification\\n                     Number of trees: 500\\nNo. of variables  tried at each split: 1\\n     OOB estimate  of error rate: 39.17%\\nConfusion  matrix:\\n        default  paid off  class.error\\ndefault     873       572   0.39584775\\npaid off    603       952   0.38778135\\nIn Python , we use the method sklearn.ensemble.RandomForestClassifier :\\npredictors  = ['borrower_score' , 'payment_inc_ratio' ]\\noutcome = 'outcome'\\nX = loan3000 [predictors ]\\ny = loan3000 [outcome]\\nrf = RandomForestClassifier (n_estimators =500, random_state =1, oob_score =True)\\nrf.fit(X, y)\\nBy default, 500 trees are trained. Since there are only two variables in the predictor\\nset, the algorithm randomly selects the variable on which to split at each stage (i.e., a\\nbootstrap subsample of size 1).\\nThe out-of-bag  (OOB ) estimate of error is the error rate for the trained models,\\napplied to the data left out of the training set for that tree. Using the output from the\\nmodel, the OOB error can be plotted versus the number of trees in the random forest\\nin R:\\nerror_df  = data.frame (error_rate =rf$err.rate [,'OOB'],\\n                      num_trees =1:rf$ntree)\\nggplot(error_df , aes(x=num_trees , y=error_rate )) +\\n  geom_line ()\\nThe RandomForestClassifier  implementation has no easy way to get out-of-bag\\nestimates as a function of number of trees in the random forest. We can train a\\nsequence of classifiers with an increasing number of trees and keep track of the\\noob_score_  values. This method is, however, not efficient:\\n262 | Chapter 6: Statistical Machine Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 279}),\n",
       " Document(page_content=\"n_estimator  = list(range(20, 510, 5))\\noobScores  = []\\nfor n in n_estimator :\\n    rf = RandomForestClassifier (n_estimators =n, criterion ='entropy' ,\\n                                max_depth =5, random_state =1, oob_score =True)\\n    rf.fit(X, y)\\n    oobScores .append(rf.oob_score_ )\\ndf = pd.DataFrame ({ 'n': n_estimator , 'oobScore' : oobScores  })\\ndf.plot(x='n', y='oobScore' )\\nThe result is shown in Figure 6-6 . The error rate rapidly decreases from over 0.44\\nbefore stabilizing around 0.385. The predicted values can be obtained from the pre\\ndict  function and plotted as follows in R:\\npred <- predict(rf, prob=TRUE)\\nrf_df <- cbind(loan3000 , pred = pred)\\nggplot(data=rf_df, aes(x=borrower_score , y=payment_inc_ratio ,\\n                       shape=pred, color=pred, size=pred)) +\\n    geom_point (alpha=.8) +\\n    scale_color_manual (values = c('paid off' ='#b8e186' , 'default' ='#d95f02' )) +\\n    scale_shape_manual (values = c('paid off' =0, 'default' =1)) +\\n    scale_size_manual (values = c('paid off' =0.5, 'default' =2))\\nIn Python , we can create a similar plot as follows:\\npredictions  = X.copy()\\npredictions ['prediction' ] = rf.predict(X)\\npredictions .head()\\nfig, ax = plt.subplots (figsize=(4, 4))\\npredictions .loc[predictions .prediction =='paid off' ].plot(\\n    x='borrower_score' , y='payment_inc_ratio' , style='.',\\n    markerfacecolor ='none', markeredgecolor ='C1', ax=ax)\\npredictions .loc[predictions .prediction =='default' ].plot(\\n    x='borrower_score' , y='payment_inc_ratio' , style='o',\\n    markerfacecolor ='none', markeredgecolor ='C0', ax=ax)\\nax.legend(['paid off' , 'default' ]);\\nax.set_xlim (0, 1)\\nax.set_ylim (0, 25)\\nax.set_xlabel ('borrower_score' )\\nax.set_ylabel ('payment_inc_ratio' )\\nBagging and the Random Forest | 263\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 280}),\n",
       " Document(page_content='Figure 6-6. An example of the improvement in accuracy of the random forest with the\\naddition of more trees\\nThe plot, shown in Figure 6-7 , is quite revealing about the nature of the random\\nforest.\\nThe random forest method is a “black box” method. It produces more accurate pre‐\\ndictions than a simple tree, but the simple tree’s intuitive decision rules are lost. The\\nrandom forest predictions are also somewhat noisy: note that some borrowers with a\\nvery high score, indicating high creditworthiness, still end up with a prediction of\\ndefault. This is a result of some unusual records in the data and demonstrates the\\ndanger of overfitting by the random forest (see “Bias-Variance Trade-off ” on page\\n247).\\n264 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 281}),\n",
       " Document(page_content='Figure 6-7. The predicted outcomes from the random forest applied to the loan default\\ndata\\nVariable Importance\\nThe power of the random forest algorithm shows itself when you build predictive\\nmodels for data with many features and records. It has the ability to automatically\\ndetermine which predictors are important and discover complex relationships\\nbetween predictors corresponding to interaction terms (see “Interactions and Main\\nEffects”  on page 174). For example, fit a model to the loan default data with all col‐\\numns included. The following shows this in R:\\nrf_all <- randomForest (outcome ~ ., data=loan_data , importance =TRUE)\\nrf_all\\nCall:\\n randomForest (formula = outcome ~ ., data = loan_data , importance  = TRUE)\\n               Type of random forest: classification\\n                     Number of trees: 500\\nNo. of variables  tried at each split: 4\\n        OOB estimate  of  error rate: 33.79%\\nBagging and the Random Forest | 265', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 282}),\n",
       " Document(page_content=\"Confusion  matrix:\\n         paid off default class.error\\npaid off    14676    7995   0.3526532\\ndefault      7325   15346   0.3231000\\nAnd in Python :\\npredictors  = ['loan_amnt' , 'term', 'annual_inc' , 'dti', 'payment_inc_ratio' ,\\n              'revol_bal' , 'revol_util' , 'purpose' , 'delinq_2yrs_zero' ,\\n              'pub_rec_zero' , 'open_acc' , 'grade', 'emp_length' , 'purpose_' ,\\n              'home_', 'emp_len_' , 'borrower_score' ]\\noutcome = 'outcome'\\nX = pd.get_dummies (loan_data [predictors ], drop_first =True)\\ny = loan_data [outcome]\\nrf_all = RandomForestClassifier (n_estimators =500, random_state =1)\\nrf_all.fit(X, y)\\nThe argument importance=TRUE  requests that the randomForest  store additional\\ninformation about the importance of different variables. The function varImpPlot\\nwill plot the relative performance of the variables (relative to permuting that vari‐\\nable):\\nvarImpPlot (rf_all, type=1) \\nvarImpPlot (rf_all, type=2) \\nmean decrease in accuracy\\nmean decrease in node impurity\\nIn Python , the RandomForestClassifier  collects information about feature impor‐\\ntance during training and makes it available with the field feature_importances_ :\\nimportances  = rf_all.feature_importances_\\nThe “Gini decrease” is available as the feature_importance_  property of the fitted\\nclassifier.  Accuracy decrease, however, is not available out of the box for Python . We\\ncan calculate it ( scores ) using the following code:\\nrf = RandomForestClassifier (n_estimators =500)\\nscores = defaultdict (list)\\n# cross-validate the scores on a number of different random splits of the data\\nfor _ in range(3):\\n    train_X, valid_X, train_y, valid_y = train_test_split (X, y, test_size =0.3)\\n    rf.fit(train_X, train_y)\\n    acc = metrics.accuracy_score (valid_y, rf.predict(valid_X))\\n    for column in X.columns:\\n        X_t = valid_X.copy()\\n        X_t[column] = np.random.permutation (X_t[column].values)\\n266 | Chapter 6: Statistical Machine Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 283}),\n",
       " Document(page_content=\"shuff_acc  = metrics.accuracy_score (valid_y, rf.predict(X_t))\\n        scores[column].append((acc-shuff_acc )/acc)\\nThe result is shown in Figure 6-8 . A similar graph can be created with this Python\\ncode:\\ndf = pd.DataFrame ({\\n    'feature' : X.columns,\\n    'Accuracy decrease' : [np.mean(scores[column]) for column in X.columns],\\n    'Gini decrease' : rf_all.feature_importances_ ,\\n})\\ndf = df.sort_values ('Accuracy decrease' )\\nfig, axes = plt.subplots (ncols=2, figsize=(8, 4.5))\\nax = df.plot(kind='barh', x='feature' , y='Accuracy decrease' ,\\n             legend=False, ax=axes[0])\\nax.set_ylabel ('')\\nax = df.plot(kind='barh', x='feature' , y='Gini decrease' ,\\n             legend=False, ax=axes[1])\\nax.set_ylabel ('')\\nax.get_yaxis ().set_visible (False)\\nThere are two ways to measure variable importance:\\n•By the decrease in accuracy of the model if the values of a variable are randomly\\npermuted ( type=1 ). Randomly permuting the values has the effect of removing\\nall predictive power for that variable. The accuracy is computed from the out-of-\\nbag data (so this measure is effectively a cross-validated estimate).\\n•By the mean decrease in the Gini impurity score (see “Measuring Homogeneity\\nor Impurity” on page 254) for all of the nodes that were split on a variable\\n(type=2 ). This measures how much including that variable improves the purity\\nof the nodes. This measure is based on the training set and is therefore less relia‐\\nble than a measure calculated on out-of-bag data.\\nBagging and the Random Forest | 267\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 284}),\n",
       " Document(page_content='Figure 6-8. The importance of variables for the full model fit to the loan data\\n268 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 285}),\n",
       " Document(page_content='The top and bottom panels of Figure 6-8  show variable importance according to the\\ndecrease in accuracy and in Gini impurity, respectively. The variables in both panels\\nare ranked by the decrease in accuracy. The variable importance scores produced by\\nthese two measures are quite different.\\nSince the accuracy decrease is a more reliable metric, why should we use the Gini\\nimpurity decrease measure? By default, randomForest  computes only this Gini\\nimpurity: Gini impurity is a byproduct of the algorithm, whereas model accuracy by\\nvariable requires extra computations (randomly permuting the data and predicting\\nthis data). In cases where computational complexity is important, such as in a pro‐\\nduction setting where thousands of models are being fit, it may not be worth the extra\\ncomputational effort. In addition, the Gini decrease sheds light on which variables\\nthe random forest is using to make its splitting rules (recall that this information,\\nreadily visible in a simple tree, is effectively lost in a random forest).\\nHyperparameters\\nThe random forest, as with many statistical machine learning algorithms, can be con‐\\nsidered a black-box algorithm with knobs to adjust how the box works.  These knobs\\nare called hyperparameters , which are parameters that you need to set before fitting a\\nmodel; they are not optimized as part of the training process. While traditional statis‐\\ntical models require choices (e.g., the choice of predictors to use in a regression\\nmodel), the hyperparameters for random forest are more critical, especially to avoid\\noverfitting. In particular, the two most important hyperparameters for the random\\nforest are:\\nnodesize /min_samples_leaf\\nThe minimum size for terminal nodes (leaves in the tree). The default is 1 for\\nclassification and 5 for regression in R. The scikit-learn  implementation in\\nPython  uses a default of 1 for both.\\nmaxnodes /max_leaf_nodes\\nThe maximum number of nodes in each decision tree. By default, there is no\\nlimit and the largest tree will be fit subject to the constraints of nodesize . Note\\nthat in Python , you specify the maximum number of terminal nodes. The two\\nparameters are related:\\nmaxnodes = 2max_leaf _nodes − 1\\nIt may be tempting to ignore these parameters and simply go with the default values.\\nHowever, using the defaults may lead to overfitting when you apply the random for‐\\nest to noisy data. When you increase nodesize /min_samples_leaf  or set maxnodes /\\nmax_leaf_nodes , the algorithm will fit smaller trees and is less likely to create\\nBagging and the Random Forest | 269', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 286}),\n",
       " Document(page_content='spurious  predictive rules. Cross-validation (see “Cross-Validation”  on page 155) can\\nbe used to test the effects of setting different values for hyperparameters.\\nKey Ideas\\n•Ensemble models improve model accuracy by combining the results from many\\nmodels.\\n•Bagging is a particular type of ensemble model based on fitting many models to\\nbootstrapped samples of the data and averaging the models.\\n•Random forest is a special type of bagging applied to decision trees. In addition\\nto resampling the data, the random forest algorithm samples the predictor vari‐\\nables when splitting the trees.\\n•A useful output from the random forest is a measure of variable importance that\\nranks the predictors in terms of their contribution to model accuracy.\\n•The random forest has a set of hyperparameters that should be tuned using\\ncross-validation to avoid overfitting.\\nBoosting\\nEnsemble models have become a standard tool for predictive modeling.  Boosting  is a\\ngeneral technique to create an ensemble of models. It was developed around the same\\ntime as bagging  (see “Bagging and the Random Forest” on page 259). Like bagging,\\nboosting is most commonly used with decision trees. Despite their similarities, boost‐\\ning takes a very different approach—one that comes with many more bells and whis‐\\ntles. As a result, while bagging can be done with relatively little tuning, boosting\\nrequires much greater care in its application. If these two methods were cars, bagging\\ncould be considered a Honda Accord (reliable and steady), whereas boosting could be\\nconsidered a Porsche (powerful but requires more care).\\nIn linear regression models, the residuals are often examined to see if the fit can be\\nimproved (see “Partial Residual Plots and Nonlinearity”  on page 185). Boosting takes\\nthis concept much further and fits a series of models, in which each successive model\\nseeks to minimize the error of the previous model. Several variants of the algorithm\\nare commonly used: Adaboost , gradient boosting , and stochastic gradient boosting . The\\nlatter, stochastic gradient boosting, is the most general and widely used. Indeed, with\\nthe right choice of parameters, the algorithm can emulate the random forest.\\n270 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 287}),\n",
       " Document(page_content='Key Terms for Boosting\\nEnsemble\\nForming a prediction by using a collection of models.\\nSynonym\\nModel averaging\\nBoosting\\nA general technique to fit a sequence of models by giving more weight to the\\nrecords with large residuals for each successive round.\\nAdaboost\\nAn early version of boosting that reweights the data based on the residuals.\\nGradient boosting\\nA more general form of boosting that is cast in terms of minimizing a cost\\nfunction.\\nStochastic gradient boosting\\nThe most general algorithm for boosting that incorporates resampling of records\\nand columns in each round.\\nRegularization\\nA technique to avoid overfitting by adding a penalty term to the cost function on\\nthe number of parameters in the model.\\nHyperparameters\\nParameters that need to be set before fitting the algorithm.\\nThe Boosting Algorithm\\nThere are various boosting algorithms, and the basic idea behind all of them is essen‐\\ntially the same. The easiest to understand is Adaboost, which proceeds as follows:\\n1.Initialize M, the maximum number of models to be fit, and set the iteration\\ncounter m= 1. Initialize the observation weights wi= 1/ N for i= 1, 2, ..., N. Initi‐\\nalize the ensemble model F0= 0.\\n2.Using the observation weights w1,w2, ...,wN, train a model fm that minimizes the\\nweighted error em defined by summing the weights for the misclassified\\nobservations.\\n3.Add the model to the ensemble: Fm=Fm− 1+αmfm where αm=log 1 − em\\nem.\\nBoosting | 271', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 288}),\n",
       " Document(page_content='4.Update the weights w1,w2, ...,wN so that the weights are increased for the obser‐\\nvations that were misclassified. The size of the increase depends on αm, with\\nlarger values of αm leading to bigger weights.\\n5.Increment the model counter m=m+ 1. If m≤M, go to step 2.\\nThe boosted estimate is given by:\\nF=α1f1+α2f2+⋯+αMfM\\nBy increasing the weights for the observations that were misclassified, the algorithm\\nforces the models to train more heavily on the data for which it performed poorly.\\nThe factor αm ensures that models with lower error have a bigger weight.\\nGradient boosting is similar to Adaboost but casts the problem as an optimization of\\na cost function. Instead of adjusting weights, gradient boosting fits models to a\\npseudo-residual , which has the effect of training more heavily on the larger residuals.\\nIn the spirit of the random forest, stochastic gradient boosting adds randomness to\\nthe algorithm by sampling observations and predictor variables at each stage.\\nXGBoost\\nThe most widely used public domain software for boosting is XGBoost, an imple‐\\nmentation of stochastic gradient boosting originally developed by Tianqi Chen and\\nCarlos Guestrin at the University of Washington. A computationally efficient imple‐\\nmentation with many options, it is available as a package for most major data science\\nsoftware languages. In R, XGBoost is available as the package xgboost  and with the\\nsame name also for Python .\\nThe method xgboost  has many parameters that can, and should, be adjusted (see\\n“Hyperparameters and Cross-Validation” on page 279). Two very important parame‐\\nters are subsample , which controls the fraction of observations that should be sam‐\\npled at each iteration, and eta, a shrinkage factor applied to αm in the boosting\\nalgorithm (see “The Boosting Algorithm” on page 271). Using subsample  makes\\nboosting act like the random forest except that the sampling is done without replace‐\\nment. The shrinkage parameter eta is helpful to prevent overfitting by reducing the\\nchange in the weights (a smaller change in the weights means the algorithm is less\\nlikely to overfit to the training set). The following applies xgboost  in R to the loan\\ndata with just two predictor variables:\\npredictors  <- data.matrix (loan3000 [, c(\\'borrower_score\\' , \\'payment_inc_ratio\\' )])\\nlabel <- as.numeric (loan3000 [,\\'outcome\\' ]) - 1\\nxgb <- xgboost(data=predictors , label=label, objective =\"binary:logistic\" ,\\n               params=list(subsample =0.63, eta=0.1), nrounds=100)\\n272 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 289}),\n",
       " Document(page_content=\"[1] train-error:0.358333\\n[2] train-error:0.346333\\n[3] train-error:0.347333\\n...\\n[99] train-error:0.239333\\n[100] train-error:0.241000\\nNote that xgboost  does not support the formula syntax, so the predictors need to be\\nconverted to a data.matrix  and the response needs to be converted to 0/1 variables.\\nThe objective  argument tells xgboost  what type of problem this is; based on this,\\nxgboost  will choose a metric to optimize.\\nIn Python , xgboost  has two different interfaces: a scikit-learn  API and a more\\nfunctional interface like in R. To be consistent with other scikit-learn  methods,\\nsome parameters were renamed. For example, eta is renamed to learning_rate ;\\nusing eta will not fail, but it will not have the desired effect:\\npredictors  = ['borrower_score' , 'payment_inc_ratio' ]\\noutcome = 'outcome'\\nX = loan3000 [predictors ]\\ny = loan3000 [outcome]\\nxgb = XGBClassifier (objective ='binary:logistic' , subsample =0.63)\\nxgb.fit(X, y)\\n--\\nXGBClassifier (base_score =0.5, booster='gbtree' , colsample_bylevel =1,\\n       colsample_bynode =1, colsample_bytree =1, gamma=0, learning_rate =0.1,\\n       max_delta_step =0, max_depth =3, min_child_weight =1, missing=None,\\n       n_estimators =100, n_jobs=1, nthread=None, objective ='binary:logistic' ,\\n       random_state =0, reg_alpha =0, reg_lambda =1, scale_pos_weight =1, seed=None,\\n       silent=None, subsample =0.63, verbosity =1)\\nThe predicted values can be obtained from the predict  function in R and, since there\\nare only two variables, plotted versus the predictors:\\npred <- predict(xgb, newdata=predictors )\\nxgb_df <- cbind(loan3000 , pred_default  = pred > 0.5, prob_default  = pred)\\nggplot(data=xgb_df, aes(x=borrower_score , y=payment_inc_ratio ,\\n                  color=pred_default , shape=pred_default , size=pred_default )) +\\n         geom_point (alpha=.8) +\\n         scale_color_manual (values = c('FALSE'='#b8e186' , 'TRUE'='#d95f02' )) +\\n         scale_shape_manual (values = c('FALSE'=0, 'TRUE'=1)) +\\n         scale_size_manual (values = c('FALSE'=0.5, 'TRUE'=2))\\nThe same figure can be created in Python  using the following code:\\nfig, ax = plt.subplots (figsize=(6, 4))\\nxgb_df.loc[xgb_df.prediction =='paid off' ].plot(\\n    x='borrower_score' , y='payment_inc_ratio' , style='.',\\n    markerfacecolor ='none', markeredgecolor ='C1', ax=ax)\\nBoosting | 273\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 290}),\n",
       " Document(page_content=\"xgb_df.loc[xgb_df.prediction =='default' ].plot(\\n    x='borrower_score' , y='payment_inc_ratio' , style='o',\\n    markerfacecolor ='none', markeredgecolor ='C0', ax=ax)\\nax.legend(['paid off' , 'default' ]);\\nax.set_xlim (0, 1)\\nax.set_ylim (0, 25)\\nax.set_xlabel ('borrower_score' )\\nax.set_ylabel ('payment_inc_ratio' )\\nThe result is shown in Figure 6-9 . Qualitatively, this is similar to the predictions from\\nthe random forest; see Figure 6-7 . The predictions are somewhat noisy in that some\\nborrowers with a very high borrower score still end up with a prediction of default.\\nFigure 6-9. The predicted outcomes from XGBoost applied to the loan default data\\nRegularization: Avoiding Overfitting\\nBlind application of xgboost  can lead to unstable models as a result of overfitting  to\\nthe training data. The problem with overfitting is twofold:\\n274 | Chapter 6: Statistical Machine Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 291}),\n",
       " Document(page_content=\"•The accuracy of the model on new data not in the training set will be degraded.\\n•The predictions from the model are highly variable, leading to unstable results.\\nAny modeling technique is potentially prone to overfitting. For example, if too many\\nvariables are included in a regression equation, the model may end up with spurious\\npredictions. However, for most statistical techniques, overfitting can be avoided by a\\njudicious selection of predictor variables. Even the random forest generally produces\\na reasonable model without tuning the parameters.\\nThis, however, is not the case for xgboost . Fit xgboost  to the loan data for a training\\nset with all of the variables included in the model. In R, you can do this as follows:\\nseed <- 400820\\npredictors  <- data.matrix (loan_data [, -which(names(loan_data ) %in%\\n                                       'outcome' )])\\nlabel <- as.numeric (loan_data $outcome) - 1\\ntest_idx  <- sample(nrow(loan_data ), 10000)\\nxgb_default  <- xgboost(data=predictors [-test_idx ,], label=label[-test_idx ],\\n                       objective ='binary:logistic' , nrounds=250, verbose=0)\\npred_default  <- predict(xgb_default , predictors [test_idx ,])\\nerror_default  <- abs(label[test_idx ] - pred_default ) > 0.5\\nxgb_default $evaluation_log [250,]\\nmean(error_default )\\n-\\niter train_error\\n1:  250    0.133043\\n[1] 0.3529\\nWe use the function train_test_split  in Python  to split the data set into training\\nand test sets:\\npredictors  = ['loan_amnt' , 'term', 'annual_inc' , 'dti', 'payment_inc_ratio' ,\\n              'revol_bal' , 'revol_util' , 'purpose' , 'delinq_2yrs_zero' ,\\n              'pub_rec_zero' , 'open_acc' , 'grade', 'emp_length' , 'purpose_' ,\\n              'home_', 'emp_len_' , 'borrower_score' ]\\noutcome = 'outcome'\\nX = pd.get_dummies (loan_data [predictors ], drop_first =True)\\ny = pd.Series([1 if o == 'default'  else 0 for o in loan_data [outcome]])\\ntrain_X, valid_X, train_y, valid_y = train_test_split (X, y, test_size =10000)\\nxgb_default  = XGBClassifier (objective ='binary:logistic' , n_estimators =250,\\n                            max_depth =6, reg_lambda =0, learning_rate =0.3,\\n                            subsample =1)\\nxgb_default .fit(train_X, train_y)\\npred_default  = xgb_default .predict_proba (valid_X)[:, 1]\\nBoosting | 275\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 292}),\n",
       " Document(page_content=\"error_default  = abs(valid_y - pred_default ) > 0.5\\nprint('default: ' , np.mean(error_default ))\\nThe test set consists of 10,000 randomly sampled records from the full data, and the\\ntraining set consists of the remaining records. Boosting leads to an error rate of only\\n13.3% for the training set. The test set, however, has a much higher error rate of\\n35.3%. This is a result of overfitting: while boosting can explain the variability in the\\ntraining set very well, the prediction rules do not apply to new data.\\nBoosting provides several parameters to avoid overfitting, including the parameters\\neta (or learning_rate ) and subsample  (see “XGBoost”  on page 272). Another\\napproach is regularization , a technique that modifies the cost function in order to\\npenalize  the complexity of the model. Decision trees are fit by minimizing cost\\ncriteria  such as Gini’s impurity score (see “Measuring Homogeneity or Impurity” on\\npage 254 ). In xgboost , it is possible to modify the cost function by adding a term that\\nmeasures the complexity of the model.\\nThere are two parameters in xgboost  to regularize the model: alpha  and lambda ,\\nwhich correspond to Manhattan distance (L1-regularization) and squared Euclidean\\ndistance (L2-regularization), respectively (see “Distance Metrics”  on page 241).\\nIncreasing these parameters will penalize more complex models and reduce the size\\nof the trees that are fit. For example, see what happens if we set lambda  to 1,000 in R:\\nxgb_penalty  <- xgboost(data=predictors [-test_idx ,], label=label[-test_idx ],\\n                       params=list(eta=.1, subsample =.63, lambda=1000),\\n                       objective ='binary:logistic' , nrounds=250, verbose=0)\\npred_penalty  <- predict(xgb_penalty , predictors [test_idx ,])\\nerror_penalty  <- abs(label[test_idx ] - pred_penalty ) > 0.5\\nxgb_penalty $evaluation_log [250,]\\nmean(error_penalty )\\n-\\niter train_error\\n1:  250     0.30966\\n[1] 0.3286\\nIn the scikit-learn  API, the parameters are called reg_alpha  and reg_lambda :\\nxgb_penalty  = XGBClassifier (objective ='binary:logistic' , n_estimators =250,\\n                            max_depth =6, reg_lambda =1000, learning_rate =0.1,\\n                            subsample =0.63)\\nxgb_penalty .fit(train_X, train_y)\\npred_penalty  = xgb_penalty .predict_proba (valid_X)[:, 1]\\nerror_penalty  = abs(valid_y - pred_penalty ) > 0.5\\nprint('penalty: ' , np.mean(error_penalty ))\\nNow the training error is only slightly lower than the error on the test set.\\n276 | Chapter 6: Statistical Machine Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 293}),\n",
       " Document(page_content=\"The predict  method in R offers a convenient argument, ntreelimit , that forces only\\nthe first i trees to be used in the prediction. This lets us directly compare the in-\\nsample versus out-of-sample error rates as more models are included:\\nerror_default  <- rep(0, 250)\\nerror_penalty  <- rep(0, 250)\\nfor(i in 1:250){\\n  pred_def  <- predict(xgb_default , predictors [test_idx ,], ntreelimit =i)\\n  error_default [i] <- mean(abs(label[test_idx ] - pred_def ) >= 0.5)\\n  pred_pen  <- predict(xgb_penalty , predictors [test_idx ,], ntreelimit =i)\\n  error_penalty [i] <- mean(abs(label[test_idx ] - pred_pen ) >= 0.5)\\n}\\nIn Python , we can call the predict_proba  method with the ntree_limit  argument:\\nresults = []\\nfor i in range(1, 250):\\n    train_default  = xgb_default .predict_proba (train_X, ntree_limit =i)[:, 1]\\n    train_penalty  = xgb_penalty .predict_proba (train_X, ntree_limit =i)[:, 1]\\n    pred_default  = xgb_default .predict_proba (valid_X, ntree_limit =i)[:, 1]\\n    pred_penalty  = xgb_penalty .predict_proba (valid_X, ntree_limit =i)[:, 1]\\n    results.append({\\n        'iterations' : i,\\n        'default train' : np.mean(abs(train_y - train_default ) > 0.5),\\n        'penalty train' : np.mean(abs(train_y - train_penalty ) > 0.5),\\n        'default test' : np.mean(abs(valid_y - pred_default ) > 0.5),\\n        'penalty test' : np.mean(abs(valid_y - pred_penalty ) > 0.5),\\n    })\\nresults = pd.DataFrame (results)\\nresults.head()\\nThe output from the model returns the error for the training set in the component\\nxgb_default$evaluation_log . By combining this with the out-of-sample errors, we\\ncan plot the errors versus the number of iterations:\\nerrors <- rbind(xgb_default $evaluation_log ,\\n                xgb_penalty $evaluation_log ,\\n                ata.frame (iter=1:250, train_error =error_default ),\\n                data.frame (iter=1:250, train_error =error_penalty ))\\nerrors$type <- rep(c('default train' , 'penalty train' ,\\n                     'default test' , 'penalty test' ), rep(250, 4))\\nggplot(errors, aes(x=iter, y=train_error , group=type)) +\\n  geom_line (aes(linetype =type, color=type))\\nWe can use the pandas  plot method to create the line graph. The axis returned from\\nthe first plot allows us to overlay additional lines onto the same graph. This is a pat‐\\ntern that many of Python ’s graph packages support:\\nax = results.plot(x='iterations' , y='default test' )\\nresults.plot(x='iterations' , y='penalty test' , ax=ax)\\nresults.plot(x='iterations' , y='default train' , ax=ax)\\nresults.plot(x='iterations' , y='penalty train' , ax=ax)\\nBoosting | 277\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 294}),\n",
       " Document(page_content='The result, displayed in Figure 6-10 , shows how the default model steadily improves\\nthe accuracy for the training set but actually gets worse for the test set. The penalized\\nmodel does not exhibit this behavior.\\nFigure 6-10. The error rate of the default XGBoost versus a penalized version of XGBoost\\nRidge Regression and the Lasso\\nAdding a penalty on the complexity of a model to help avoid overfitting dates back to\\nthe 1970s. Least squares regression minimizes the residual sum of squares (RSS); see\\n“Least Squares” on page 148. Ridge regression  minimizes the sum of squared residuals\\nplus a penalty term that is a function of the number and size of the coefficients:\\n∑\\ni= 1n\\nYi−b0−b1Xi−⋯bXp2+λb12+⋯+bp2\\nThe value of λ determines how much the coefficients are penalized; larger values pro‐\\nduce models that are less likely to overfit the data. The Lasso  is similar, except that it \\nuses Manhattan distance instead of Euclidean distance as a penalty term:\\n∑\\ni= 1n\\nYi−b0−b1Xi−⋯bXp2+αb1+⋯+bp\\n278 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 295}),\n",
       " Document(page_content=\"The xgboost  parameters lambda  (reg_lambda ) and alpha  (reg_alpha ) are acting in a\\nsimilar manner.\\nUsing Euclidean distance is also known as L2 regularization, and using Manhattan\\ndistance as L1 regularization. The xgboost  parameters lambda  (reg_lambda ) and\\nalpha  (reg_alpha ) are acting in a similar manner.\\nHyperparameters and Cross-Validation\\nxgboost  has a daunting array of hyperparameters; see “XGBoost Hyperparameters”\\non page 281 for a discussion. As seen in “Regularization: Avoiding Overfitting”  on\\npage 274, the specific choice can dramatically change the model fit. Given a huge\\ncombination of hyperparameters to choose from, how should we be guided in our\\nchoice? A standard solution to this problem is to use cross-validation ; see “Cross-\\nValidation” on page 155 . Cross-validation randomly splits up the data into K different\\ngroups, also called folds . For each fold, a model is trained on the data not in the fold\\nand then evaluated on the data in the fold. This yields a measure of accuracy of the\\nmodel on out-of-sample data. The best set of hyperparameters is the one given by the\\nmodel with the lowest overall error as computed by averaging the errors from each of\\nthe folds.\\nTo illustrate the technique, we apply it to parameter selection for xgboost . In this\\nexample, we explore two parameters: the shrinkage parameter eta (learning_rate —\\nsee “XGBoost” on page 272 ) and the maximum depth of trees max_depth . The param‐\\neter max_depth  is the maximum depth of a leaf node to the root of the tree with a\\ndefault value of six. This gives us another way to control overfitting: deep trees tend\\nto be more complex and may overfit the data. First we set up the folds and parameter\\nlist. In R, this is done as follows:\\nN <- nrow(loan_data )\\nfold_number  <- sample(1:5, N, replace=TRUE)\\nparams <- data.frame (eta = rep(c(.1, .5, .9), 3),\\n                     max_depth  = rep(c(3, 6, 12), rep(3,3)))\\nNow we apply the preceding algorithm to compute the error for each model and each\\nfold using five folds:\\nerror <- matrix(0, nrow=9, ncol=5)\\nfor(i in 1:nrow(params)){\\n  for(k in 1:5){\\n    fold_idx  <- (1:N)[fold_number  == k]\\n    xgb <- xgboost(data=predictors [-fold_idx ,], label=label[-fold_idx ],\\n                   params=list(eta=params[i, 'eta'],\\n                               max_depth =params[i, 'max_depth' ]),\\n                   objective ='binary:logistic' , nrounds=100, verbose=0)\\n    pred <- predict(xgb, predictors [fold_idx ,])\\n    error[i, k] <- mean(abs(label[fold_idx ] - pred) >= 0.5)\\nBoosting | 279\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 296}),\n",
       " Document(page_content=\"}\\n}\\nIn the following Python  code, we create all possible combinations of hyperparameters\\nand fit and evaluate models with each combination:\\nidx = np.random.choice(range(5), size=len(X), replace=True)\\nerror = []\\nfor eta, max_depth  in product([0.1, 0.5, 0.9], [3, 6, 9]):  \\n    xgb = XGBClassifier (objective ='binary:logistic' , n_estimators =250,\\n                        max_depth =max_depth , learning_rate =eta)\\n    cv_error  = []\\n    for k in range(5):\\n        fold_idx  = idx == k\\n        train_X = X.loc[~fold_idx ]; train_y = y[~fold_idx ]\\n        valid_X = X.loc[fold_idx ]; valid_y = y[fold_idx ]\\n        xgb.fit(train_X, train_y)\\n        pred = xgb.predict_proba (valid_X)[:, 1]\\n        cv_error .append(np.mean(abs(valid_y - pred) > 0.5))\\n    error.append({\\n        'eta': eta,\\n        'max_depth' : max_depth ,\\n        'avg_error' : np.mean(cv_error )\\n    })\\n    print(error[-1])\\nerrors = pd.DataFrame (error)\\nWe use the function itertools.product  from the Python  standard library to\\ncreate all possible combinations of the two hyperparameters.\\nSince we are fitting 45 total models, this can take a while. The errors are stored as a\\nmatrix with the models along the rows and folds along the columns. Using the func‐\\ntion rowMeans , we can compare the error rate for the different parameter sets:\\navg_error  <- 100 * round(rowMeans (error), 4)\\ncbind(params, avg_error )\\n  eta max_depth  avg_error\\n1 0.1         3     32.90\\n2 0.5         3     33.43\\n3 0.9         3     34.36\\n4 0.1         6     33.08\\n5 0.5         6     35.60\\n6 0.9         6     37.82\\n7 0.1        12     34.56\\n8 0.5        12     36.83\\n9 0.9        12     38.18\\nCross-validation suggests that using shallower trees with a smaller value of eta/learn\\ning_rate  yields more accurate results. Since these models are also more stable, the\\nbest parameters to use are eta=0.1  and max_depth=3  (or possibly max_depth=6 ).\\n280 | Chapter 6: Statistical Machine Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 297}),\n",
       " Document(page_content='XGBoost Hyperparameters\\nThe hyperparameters for xgboost  are primarily used to balance overfitting with the\\naccuracy and computational complexity.  For a complete discussion of the parameters,\\nrefer to the xgboost  documentation .\\neta/learning_rate\\nThe shrinkage factor between 0 and 1 applied to α in the boosting algorithm. The\\ndefault is 0.3, but for noisy data, smaller values are recommended (e.g., 0.1). In\\nPython , the default value is 0.1.\\nnrounds /n_estimators\\nThe number of boosting rounds. If eta is set to a small value, it is important to\\nincrease the number of rounds since the algorithm learns more slowly. As long as\\nsome parameters are included to prevent overfitting, having more rounds doesn’t\\nhurt.\\nmax_depth\\nThe maximum depth of the tree (the default is 6). In contrast to the random for‐\\nest, which fits very deep trees, boosting usually fits shallow trees. This has the\\nadvantage of avoiding spurious complex interactions in the model that can arise\\nfrom noisy data. In Python , the default is 3.\\nsubsample  and colsample_bytree\\nFraction of the records to sample without replacement and the fraction of predic‐\\ntors to sample for use in fitting the trees. These parameters, which are similar to\\nthose in random forests, help avoid overfitting. The default is 1.0.\\nlambda /reg_lambda  and alpha /reg_alpha\\nThe regularization parameters to help control overfitting (see “Regularization:\\nAvoiding Overfitting” on page 274). Default values for Python  are reg_lambda=1\\nand reg_alpha=0 . In R, both values have default of 0.\\nBoosting | 281', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 298}),\n",
       " Document(page_content='Key Ideas\\n•Boosting is a class of ensemble models based on fitting a sequence of models,\\nwith more weight given to records with large errors in successive rounds.\\n•Stochastic gradient boosting is the most general type of boosting and offers the\\nbest performance. The most common form of stochastic gradient boosting uses\\ntree models.\\n•XGBoost is a popular and computationally efficient software package for stochas‐\\ntic gradient boosting; it is available in all common languages used in data science.\\n•Boosting is prone to overfitting the data, and the hyperparameters need to be\\ntuned to avoid this.\\n•Regularization is one way to avoid overfitting by including a penalty term on the\\nnumber of parameters (e.g., tree size) in a model.\\n•Cross-validation is especially important for boosting due to the large number of\\nhyperparameters that need to be set.\\nSummary\\nThis chapter has described two classification and prediction methods that “learn”\\nflexibly and locally from data, rather than starting with a structural model (e.g., a lin‐\\near regression) that is fit to the entire data set. K-Nearest Neighbors is a simple pro‐\\ncess that looks around at similar records and assigns their majority class (or average\\nvalue) to the record being predicted. Trying various cutoff (split) values of predictor\\nvariables, tree models iteratively divide the data into sections and subsections that are\\nincreasingly homogeneous with respect to class. The most effective split values form a\\npath, and also a “rule, ” to a classification or prediction. Tree models are a very power‐\\nful and popular predictive tool, often outperforming other methods. They have given\\nrise to various ensemble methods (random forests, boosting, bagging) that sharpen\\nthe predictive power of trees.\\n282 | Chapter 6: Statistical Machine Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 299}),\n",
       " Document(page_content='CHAPTER 7\\nUnsupervised Learning\\nThe term unsupervised learning  refers to statistical methods that extract meaning\\nfrom data without training a model on labeled data (data where an outcome of inter‐\\nest is known). In Chapters 4 to 6, the goal is to build a model (set of rules) to predict a\\nresponse variable from a set of predictor variables. This is supervised learning. In\\ncontrast, unsupervised learning also constructs a model of the data, but it does not\\ndistinguish between a response variable and predictor variables.\\nUnsupervised learning can be used to achieve different goals.  In some cases, it can be\\nused to create a predictive rule in the absence of a labeled response. Clustering  meth‐\\nods can be used to identify meaningful groups of data. For example, using the web\\nclicks and demographic data of a user on a website, we may be able to group together\\ndifferent types of users. The website could then be personalized to these different\\ntypes.\\nIn other cases, the goal may be to reduce the dimension  of the data to a more manage‐\\nable set of variables. This reduced set could then be used as input into a predictive\\nmodel, such as regression or classification. For example, we may have thousands of\\nsensors to monitor an industrial process. By reducing the data to a smaller set of fea‐\\ntures, we may be able to build a more powerful and interpretable model to predict\\nprocess failure than could be built by including data streams from thousands of\\nsensors.\\nFinally, unsupervised learning can be viewed as an extension of the exploratory data\\nanalysis (see Chapter 1 ) to situations in which you are confronted with a large num‐\\nber of variables and records. The aim is to gain insight into a set of data and how the\\ndifferent variables relate to each other. Unsupervised techniques allow you to sift\\nthrough and analyze these variables and discover relationships.\\n283', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 300}),\n",
       " Document(page_content='1This and subsequent sections in this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter\\nGedeck; used with permission.\\nUnsupervised Learning and Prediction\\nUnsupervised learning can play an important role in prediction,\\nboth for regression and classification problems. In some cases, we\\nwant to predict a category in the absence of any labeled data. For\\nexample, we might want to predict the type of vegetation in an area\\nfrom a set of satellite sensory data. Since we don’t have a response\\nvariable to train a model, clustering gives us a way to identify com‐\\nmon patterns and categorize the regions.\\nClustering is an especially important tool for the “cold-start prob‐\\nlem. ” In this type of problem, such as launching a new marketing\\ncampaign or identifying potential new types of fraud or spam, we\\ninitially may not have any response to train a model. Over time, as\\ndata is collected, we can learn more about the system and build a\\ntraditional predictive model. But clustering helps us start the learn‐\\ning process more quickly by identifying population segments.\\nUnsupervised learning is also important as a building block for\\nregression and classification techniques. With big data, if a small\\nsubpopulation is not well represented in the overall population, the\\ntrained model may not perform well for that subpopulation. With\\nclustering, it is possible to identify and label subpopulations. Sepa‐\\nrate models can then be fit to the different subpopulations. Alter‐\\nnatively, the subpopulation can be represented with its own feature,\\nforcing the overall model to explicitly consider subpopulation\\nidentity as a predictor.\\nPrincipal Components Analysis\\nOften, variables will vary together (covary), and some of the variation in one is\\nactually duplicated by variation in another (e.g., restaurant checks and tips).  Principal\\ncomponents analysis (PCA) is a technique to discover the way in which numeric vari‐\\nables covary.1\\n284 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 301}),\n",
       " Document(page_content='Key Terms for Principal Components Analysis\\nPrincipal component\\nA linear combination of the predictor variables.\\nLoadings\\nThe weights that transform the predictors into the components.\\nSynonym\\nWeights\\nScreeplot\\nA plot of the variances of the components, showing the relative importance of the\\ncomponents, either as explained variance or as proportion of explained variance.\\nThe idea in PCA is to combine multiple numeric predictor variables into a smaller set\\nof variables, which are weighted linear combinations of the original set.  The smaller\\nset of variables, the principal components , “explains” most of the variability of the full\\nset of variables, reducing the dimension of the data. The weights used to form the\\nprincipal components reveal the relative contributions of the original variables to the\\nnew principal components.\\nPCA was first proposed by Karl Pearson . In what was perhaps the first paper on\\nunsupervised learning, Pearson recognized that in many problems there is variability\\nin the predictor variables, so he developed PCA as a technique to model this variabil‐\\nity. PCA can be viewed as the unsupervised version of linear discriminant analysis;\\nsee“Discriminant Analysis” on page 201 .\\nA Simple Example\\nFor two variables, X1 and X2, there are two principal components Zi (i= 1 or 2):\\nZi=wi, 1X1+wi, 2X2\\nThe weights wi, 1,wi, 2 are known as the component loadings . These transform the\\noriginal variables into the principal components. The first principal component, Z1, is\\nthe linear combination that best explains the total variation. The second principal\\ncomponent, Z2, is orthogonal to the first and explains as much of the remaining var‐\\niation as it can. (If there were additional components, each additional one would be\\northogonal to the others.)\\nPrincipal Components Analysis | 285', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 302}),\n",
       " Document(page_content=\"It is also common to compute principal components on deviations\\nfrom the means of the predictor variables, rather than on the values\\nthemselves.\\nY ou can compute principal components in R using the princomp  function. The fol‐\\nlowing performs a PCA on the stock price returns for Chevron (CVX) and Exxon‐\\nMobil (XOM):\\noil_px <- sp500_px [, c('CVX', 'XOM')]\\npca <- princomp (oil_px)\\npca$loadings\\nLoadings :\\n    Comp.1 Comp.2\\nCVX -0.747  0.665\\nXOM -0.665 -0.747\\n               Comp.1 Comp.2\\nSS loadings        1.0    1.0\\nProportion  Var    0.5    0.5\\nCumulative  Var    0.5    1.0\\nIn Python , we can use the scikit-learn  implementation sklearn.decomposi\\ntion.PCA :\\npcs = PCA(n_components =2)\\npcs.fit(oil_px)\\nloadings  = pd.DataFrame (pcs.components_ , columns=oil_px.columns)\\nloadings\\nThe weights for CVX and XOM for the first principal component are –0.747 and\\n–0.665 , and for the second principal component they are 0.665 and –0.747. How to\\ninterpret this? The first principal component is essentially an average of CVX and\\nXOM, reflecting the correlation between the two energy companies. The second prin‐\\ncipal component measures when the stock prices of CVX and XOM diverge.\\nIt is instructive to plot the principal components with the data. Here we create a visu‐\\nalization in R:\\nloadings  <- pca$loadings\\nggplot(data=oil_px, aes(x=CVX, y=XOM)) +\\n  geom_point (alpha=.3) +\\n  stat_ellipse (type='norm', level=.99) +\\n  geom_abline (intercept  = 0, slope = loadings [2,1]/loadings [1,1]) +\\n  geom_abline (intercept  = 0, slope = loadings [2,2]/loadings [1,2])\\n286 | Chapter 7: Unsupervised Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 303}),\n",
       " Document(page_content='The following code creates a similar visualization in Python :\\ndef abline(slope, intercept , ax):\\n    \"\"\"Calculate coordinates of a line based on slope and intercept\"\"\"\\n    x_vals = np.array(ax.get_xlim ())\\n    return (x_vals, intercept  + slope * x_vals)\\nax = oil_px.plot.scatter(x=\\'XOM\\', y=\\'CVX\\', alpha=0.3, figsize=(4, 4))\\nax.set_xlim (-3, 3)\\nax.set_ylim (-3, 3)\\nax.plot(*abline(loadings .loc[0, \\'CVX\\'] / loadings .loc[0, \\'XOM\\'], 0, ax),\\n        \\'--\\', color=\\'C1\\')\\nax.plot(*abline(loadings .loc[1, \\'CVX\\'] / loadings .loc[1, \\'XOM\\'], 0, ax),\\n        \\'--\\', color=\\'C1\\')\\nThe result is shown in Figure 7-1 .\\nFigure 7-1. The principal components for the stock returns for Chevron (CVX) and\\nExxonMobil (XOM)\\nThe dashed lines show the direction of the two principal components: the first one is\\nalong the long axis of the ellipse, and the second one is along the short axis. Y ou can\\nsee that a majority of the variability in the two stock returns is explained by the first\\nprincipal component. This makes sense since energy stock prices tend to move as a\\ngroup.\\nPrincipal Components Analysis | 287', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 304}),\n",
       " Document(page_content='The weights for the first principal component are both negative,\\nbut reversing the sign of all the weights does not change the princi‐\\npal component. For example, using weights of 0.747 and 0.665 for\\nthe first principal component is equivalent to the negative weights,\\njust as an infinite line defined by the origin and 1,1 is the same as\\none defined by the origin and –1, –1.\\nComputing the Principal Components\\nGoing from two variables to more variables is straightforward. For the first compo‐\\nnent, simply include the additional predictor variables in the linear combination,\\nassigning weights that optimize the collection of the covariation from all the predic‐\\ntor variables into this first principal component ( covariance  is the statistical term; see\\n“Covariance Matrix” on page 202). Calculation of principal components is a classic\\nstatistical method, relying on either the correlation matrix of the data or the cova‐\\nriance matrix, and it executes rapidly, not relying on iteration. As noted earlier, prin‐\\ncipal components analysis works only with numeric variables, not categorical ones.\\nThe full process can be described as follows:\\n1.In creating the first principal component, PCA arrives at the linear combination\\nof predictor variables that maximizes the percent of total variance explained.\\n2.This linear combination then becomes the first “new” predictor, Z1.\\n3.PCA repeats this process, using the same variables with different weights, to cre‐\\nate a second new predictor, Z2. The weighting is done such that Z1 and Z2 are\\nuncorrelated.\\n4.The process continues until you have as many new variables, or components, Zi\\nas original variables Xi.\\n5.Choose to retain as many components as are needed to account for most of the\\nvariance.\\n6.The result so far is a set of weights for each component. The final step is to con‐\\nvert the original data into new principal component scores by applying the\\nweights to the original values. These new scores can then be used as the reduced\\nset of predictor variables.\\n288 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 305}),\n",
       " Document(page_content=\"Interpreting Principal Components\\nThe nature of the principal components often reveals information about the structure\\nof the data. There are a couple of standard visualization displays to help you glean\\ninsight about the principal components. One such method is a screeplot  to visualize\\nthe relative importance of principal components (the name derives from the resem‐\\nblance of the plot to a scree slope; here, the y-axis is the eigenvalue). The following R\\ncode shows an example for a few top companies in the S&P 500:\\nsyms <- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\\n   'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\\ntop_sp <- sp500_px [row.names (sp500_px )>='2005-01-01' , syms]\\nsp_pca <- princomp (top_sp)\\nscreeplot (sp_pca)\\nThe information to create a loading plot from the scikit-learn  result is available in\\nexplained_variance_ . Here, we convert it into a pandas  data frame and use it to\\nmake a bar chart:\\nsyms = sorted(['AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',\\n               'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST'])\\ntop_sp = sp500_px .loc[sp500_px .index >= '2011-01-01' , syms]\\nsp_pca = PCA()\\nsp_pca.fit(top_sp)\\nexplained_variance  = pd.DataFrame (sp_pca.explained_variance_ )\\nax = explained_variance .head(10).plot.bar(legend=False, figsize=(4, 4))\\nax.set_xlabel ('Component' )\\nAs seen in Figure 7-2 , the variance of the first principal component is quite large (as\\nis often the case), but the other top principal components are significant.\\nPrincipal Components Analysis | 289\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 306}),\n",
       " Document(page_content=\"Figure 7-2. A screeplot for a PCA of top stocks from the S&P 500\\nIt can be especially revealing to plot the weights of the top principal components.  One\\nway to do this in R is to use the gather  function from the tidyr  package in conjunc‐\\ntion with ggplot :\\nlibrary(tidyr)\\nloadings  <- sp_pca$loadings [,1:5]\\nloadings $Symbol <- row.names (loadings )\\nloadings  <- gather(loadings , 'Component' , 'Weight' , -Symbol)\\nggplot(loadings , aes(x=Symbol, y=Weight)) +\\n  geom_bar (stat='identity' ) +\\n  facet_grid (Component  ~ ., scales='free_y' )\\nHere is the code to create the same visualization in Python :\\nloadings  = pd.DataFrame (sp_pca.components_ [0:5, :], columns=top_sp.columns)\\nmaxPC = 1.01 * np.max(np.max(np.abs(loadings .loc[0:5, :])))\\nf, axes = plt.subplots (5, 1, figsize=(5, 5), sharex=True)\\nfor i, ax in enumerate (axes):\\n    pc_loadings  = loadings .loc[i, :]\\n    colors = ['C0' if l > 0 else 'C1' for l in pc_loadings ]\\n    ax.axhline(color='#888888' )\\n290 | Chapter 7: Unsupervised Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 307}),\n",
       " Document(page_content=\"pc_loadings .plot.bar(ax=ax, color=colors)\\n    ax.set_ylabel (f'PC{i+1}' )\\n    ax.set_ylim (-maxPC, maxPC)\\nThe loadings for the top five components are shown in Figure 7-3 . The loadings for\\nthe first principal component have the same sign: this is typical for data in which all\\nthe columns share a common factor (in this case, the overall stock market trend). The\\nsecond component captures the price changes of energy stocks as compared to the\\nother stocks. The third component is primarily a contrast in the movements of Apple\\nand CostCo. The fourth component contrasts the movements of Schlumberger (SLB)\\nto the other energy stocks. Finally, the fifth component is mostly dominated by finan‐\\ncial companies.\\nFigure 7-3. The loadings for the top five principal components of stock price returns\\nPrincipal Components Analysis | 291\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 308}),\n",
       " Document(page_content='How Many Components to Choose?\\nIf your goal is to reduce the dimension of the data, you must decide\\nhow many principal components to select. The most common\\napproach is to use an ad hoc rule to select the components that\\nexplain “most” of the variance. Y ou can do this visually through the\\nscreeplot, as, for example, in Figure 7-2 . Alternatively, you could\\nselect the top components such that the cumulative variance\\nexceeds a threshold, such as 80%. Also, you can inspect the load‐\\nings to determine if the component has an intuitive interpretation.\\nCross-validation provides a more formal method to select the\\nnumber of significant components (see “Cross-Validation”  on page\\n155 for more).\\nCorrespondence Analysis\\nPCA cannot be used for categorical data; however, a somewhat related technique is\\ncorrespondence analysis . The goal is to recognize associations between categories, or\\nbetween categorical features. The similarities between correspondence analysis and\\nprincipal components analysis are mainly under the hood—the matrix algebra for\\ndimension scaling. Correspondence analysis is used mainly for graphical analysis of\\nlow-dimensional categorical data and is not used in the same way that PCA is for\\ndimension reduction as a preparatory step with big data.\\nThe input can be seen as a table, with rows representing one variable and columns\\nanother, and the cells representing record counts. The output (after some matrix alge‐\\nbra) is a biplot —a scatterplot with axes scaled (and with percentages indicating how\\nmuch variance is explained by that dimension). The meaning of the units on the axes\\nis not intuitively connected to the original data, and the main value of the scatterplot\\nis to illustrate graphically variables that are associated with one another (by proximity\\non the plot). See for example, Figure 7-4 , in which household tasks are arrayed\\naccording to whether they are done jointly or solo (vertical axis), and whether wife or\\nhusband has primary responsibility (horizontal axis). Correspondence analysis is\\nmany decades old, as is the spirit of this example, judging by the assignment of tasks.\\nThere are a variety of packages for correspondence analysis in R. Here, we use the\\npackage ca:\\nca_analysis  <- ca(housetasks )\\nplot(ca_analysis )\\n292 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 309}),\n",
       " Document(page_content='In Python , we can use the prince  package, which implements correspondence analy‐\\nsis using the scikit-learn  API:\\nca = prince.CA(n_components =2)\\nca = ca.fit(housetasks )\\nca.plot_coordinates (housetasks , figsize=(6, 6))\\nFigure 7-4. Graphical representation of a correspondence analysis of house task data\\nPrincipal Components Analysis | 293', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 310}),\n",
       " Document(page_content='Key Ideas\\n•Principal components are linear combinations of the predictor variables\\n(numeric data only).\\n•Principal components are calculated so as to minimize correlation between com‐\\nponents, reducing redundancy.\\n•A limited number of components will typically explain most of the variance in\\nthe outcome variable.\\n•The limited set of principal components can then be used in place of the (more\\nnumerous) original predictors, reducing dimensionality.\\n•A superficially similar technique for categorical data is correspondence analysis,\\nbut it is not useful in a big data context.\\nFurther Reading\\nFor a detailed look at the use of cross-validation in principal components, see Rasmus\\nBro, K. Kjeldahl, A.K. Smilde, and Henk A. L. Kiers, “Cross-Validation of Component\\nModels: A Critical Look at Current Methods” , Analytical and Bioanalytical Chemistry\\n390, no. 5 (2008).\\nK-Means Clustering\\nClustering is a technique to divide data into different groups, where the records in\\neach group are similar to one another. A goal of clustering is to identify significant\\nand meaningful groups of data. The groups can be used directly, analyzed in more\\ndepth, or passed as a feature or an outcome to a predictive regression or classification\\nmodel. K-means  was the first clustering method to be developed; it is still widely\\nused, owing its popularity to the relative simplicity of the algorithm and its ability to\\nscale to large data sets.\\nKey Terms for K-Means Clustering\\nCluster\\nA group of records that are similar.\\nCluster mean\\nThe vector of variable means for the records in a cluster.\\nK\\nThe number of clusters.\\n294 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 311}),\n",
       " Document(page_content='K-means divides the data into K clusters by minimizing the sum of the squared dis‐\\ntances of each record to the mean  of its assigned cluster. This is referred to as the\\nwithin-cluster sum of squares  or within-cluster SS . K-means does not ensure the clus‐\\nters will have the same size but finds the clusters that are the best separated.\\nNormalization\\nIt is typical to normalize (standardize) continuous variables by sub‐\\ntracting the mean and dividing by the standard deviation. Other‐\\nwise, variables with large scale will dominate the clustering process\\n(see “Standardization (Normalization, z-Scores)” on page 243 ).\\nA Simple Example\\nStart by considering a data set with n records and just two variables, x and y. Suppose\\nwe want to split the data into K= 4 clusters. This means assigning each record xi,yi\\nto a cluster k. Given an assignment of nk records to cluster k, the center of the cluster\\nxk,yk is the mean of the points in the cluster:\\nx¯k=1\\nnk∑\\ni∈\\nCluster kxi\\ny¯k=1\\nnk∑\\ni∈\\nCluster kyi\\nCluster Mean\\nIn clustering records with multiple variables (the typical case), the\\nterm cluster mean  refers not to a single number but to the vector of\\nmeans of the variables.\\nThe sum of squares within a cluster is given by:\\nSSk= ∑\\ni∈Cluster kxi−xk2+yi−yk2\\nK-means finds the assignment of records that minimizes within-cluster sum of\\nsquares across all four clusters SS1+ SS2+ SS3+ SS4:\\n∑\\nk= 14\\nSSk\\nK-Means Clustering | 295', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 312}),\n",
       " Document(page_content=\"A typical use of clustering is to locate natural, separate clusters in the data. Another\\napplication is to divide the data into a predetermined number of separate groups,\\nwhere clustering is used to ensure the groups are as different as possible from one\\nanother.\\nFor example, suppose we want to divide daily stock returns into four groups. K-\\nmeans clustering can be used to separate the data into the best groupings. Note that\\ndaily stock returns are reported in a fashion that is, in effect, standardized, so we do\\nnot need to normalize the data. In R, K-means clustering can be performed using the\\nkmeans  function. For example, the following finds four clusters based on two vari‐\\nables—the daily stock returns for ExxonMobil ( XOM) and Chevron ( CVX):\\ndf <- sp500_px [row.names (sp500_px )>='2011-01-01' , c('XOM', 'CVX')]\\nkm <- kmeans(df, centers=4)\\nWe use the sklearn.cluster.KMeans  method from scikit-learn  in Python :\\ndf = sp500_px .loc[sp500_px .index >= '2011-01-01' , ['XOM', 'CVX']]\\nkmeans = KMeans(n_clusters =4).fit(df)\\nThe cluster assignment for each record is returned as the cluster  component ( R):\\n> df$cluster <- factor(km$cluster)\\n> head(df)\\n                  XOM        CVX cluster\\n2011-01-03  0.73680496   0.2406809        2\\n2011-01-04  0.16866845  -0.5845157        1\\n2011-01-05  0.02663055   0.4469854        2\\n2011-01-06  0.24855834  -0.9197513        1\\n2011-01-07  0.33732892   0.1805111        2\\n2011-01-10  0.00000000  -0.4641675        1\\nIn scikit-learn , the cluster labels are available in the labels_  field:\\ndf['cluster' ] = kmeans.labels_\\ndf.head()\\nThe first six records are assigned to either cluster 1 or cluster 2. The means of the\\nclusters are also returned ( R):\\n> centers <- data.frame (cluster=factor(1:4), km$centers)\\n> centers\\n  cluster        XOM        CVX\\n1       1 -0.3284864  -0.5669135\\n2       2  0.2410159   0.3342130\\n3       3 -1.1439800  -1.7502975\\n4       4  0.9568628   1.3708892\\n296 | Chapter 7: Unsupervised Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 313}),\n",
       " Document(page_content=\"In scikit-learn , the cluster centers are available in the cluster_centers_  field:\\ncenters = pd.DataFrame (kmeans.cluster_centers_ , columns=['XOM', 'CVX'])\\ncenters\\nClusters 1 and 3 represent “down” markets, while clusters 2 and 4 represent “up\\nmarkets. ”\\nAs the K-means algorithm uses randomized starting points, the results may differ\\nbetween subsequent runs and different implementations of the method. In general,\\nyou should check that the fluctuations aren’t too large.\\nIn this example, with just two variables, it is straightforward to visualize the clusters\\nand their means:\\nggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +\\n  geom_point (alpha=.3) +\\n  geom_point (data=centers,  aes(x=XOM, y=CVX), size=3, stroke=2)\\nThe seaborn  scatterplot  function makes it easy to color ( hue) and style ( style ) the\\npoints by a property:\\nfig, ax = plt.subplots (figsize=(4, 4))\\nax = sns.scatterplot (x='XOM', y='CVX', hue='cluster' , style='cluster' ,\\n                     ax=ax, data=df)\\nax.set_xlim (-3, 3)\\nax.set_ylim (-3, 3)\\ncenters.plot.scatter(x='XOM', y='CVX', ax=ax, s=50, color='black')\\nThe resulting plot, shown in Figure 7-5 , shows the cluster assignments and the cluster\\nmeans. Note that K-means will assign records to clusters, even if those clusters are\\nnot well separated (which can be useful if you need to optimally divide records into\\ngroups).\\nK-Means Clustering | 297\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 314}),\n",
       " Document(page_content='Figure 7-5. The clusters of K-means applied to daily stock returns for ExxonMobil and\\nChevron (the cluster centers are highlighted with black symbols)\\nK-Means Algorithm\\nIn general, K-means can be applied to a data set with p variables X1, ...,Xp. While the\\nexact solution to K-means is computationally very difficult, heuristic algorithms pro‐\\nvide an efficient way to compute a locally optimal solution.\\nThe algorithm starts with a user-specified K and an initial set of cluster means and\\nthen iterates the following steps:\\n1.Assign each record to the nearest cluster mean as measured by squared distance.\\n2.Compute the new cluster means based on the assignment of records.\\nThe algorithm converges when the assignment of records to clusters does not change.\\nFor the first iteration, you need to specify an initial set of cluster means. Usually you\\ndo this by randomly assigning each record to one of the K clusters and then finding\\nthe means of those clusters.\\nSince this algorithm isn’t guaranteed to find the best possible solution, it is recom‐\\nmended to run the algorithm several times using different random samples to initial‐\\nize the algorithm. When more than one set of iterations is used, the K-means result is\\ngiven by the iteration that has the lowest within-cluster sum of squares.\\n298 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 315}),\n",
       " Document(page_content=\"The nstart  parameter to the R function kmeans  allows you to specify the number of\\nrandom starts to try. For example, the following code runs K-means to find 5 clusters\\nusing 10 different starting cluster means:\\nsyms <- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',\\n           'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\\ndf <- sp500_px [row.names (sp500_px ) >= '2011-01-01' , syms]\\nkm <- kmeans(df, centers=5, nstart=10)\\nThe function automatically returns the best solution out of the 10 different starting\\npoints. Y ou can use the argument iter.max  to set the maximum number of iterations\\nthe algorithm is allowed for each random start.\\nThe scikit-learn  algorithm is repeated 10 times by default ( n_init ). The argument\\nmax_iter  (default 300) can be used to control the number of iterations:\\nsyms = sorted(['AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',\\n               'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST'])\\ntop_sp = sp500_px .loc[sp500_px .index >= '2011-01-01' , syms]\\nkmeans = KMeans(n_clusters =5).fit(top_sp)\\nInterpreting the Clusters\\nAn important part of cluster analysis can involve the interpretation of the clusters.\\nThe two most important outputs from kmeans  are the sizes of the clusters and the\\ncluster means. For the example in the previous subsection, the sizes of resulting clus‐\\nters are given by this R command:\\nkm$size\\n[1] 106 186 285 288 266\\nIn Python , we can use the collections.Counter  class from the standard library to\\nget this information. Due to differences in the implementation and the inherent ran‐\\ndomness of the algorithm, results will vary:\\nfrom collections  import Counter\\nCounter(kmeans.labels_)\\nCounter({4: 302, 2: 272, 0: 288, 3: 158, 1: 111})\\nThe cluster sizes are relatively balanced. Imbalanced clusters can result from distant\\noutliers, or from groups of records very distinct from the rest of the data—both may\\nwarrant further inspection.\\nK-Means Clustering | 299\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 316}),\n",
       " Document(page_content='Y ou can plot the centers of the clusters using the gather  function in conjunction with\\nggplot :\\ncenters <- as.data.frame (t(centers))\\nnames(centers) <- paste(\"Cluster\" , 1:5)\\ncenters$Symbol <- row.names (centers)\\ncenters <- gather(centers, \\'Cluster\\' , \\'Mean\\', -Symbol)\\ncenters$Color = centers$Mean > 0\\nggplot(centers, aes(x=Symbol, y=Mean, fill=Color)) +\\n  geom_bar (stat=\\'identity\\' , position =\\'identity\\' , width=.75) +\\n  facet_grid (Cluster ~ ., scales=\\'free_y\\' )\\nThe code to create this visualization in Python  is similar to what we used for PCA:\\ncenters = pd.DataFrame (kmeans.cluster_centers_ , columns=syms)\\nf, axes = plt.subplots (5, 1, figsize=(5, 5), sharex=True)\\nfor i, ax in enumerate (axes):\\n    center = centers.loc[i, :]\\n    maxPC = 1.01 * np.max(np.max(np.abs(center)))\\n    colors = [\\'C0\\' if l > 0 else \\'C1\\' for l in center]\\n    ax.axhline(color=\\'#888888\\' )\\n    center.plot.bar(ax=ax, color=colors)\\n    ax.set_ylabel (f\\'Cluster {i + 1}\\' )\\n    ax.set_ylim (-maxPC, maxPC)\\nThe resulting plot is shown in Figure 7-6  and reveals the nature of each cluster. For\\nexample, clusters 4 and 5 correspond to days on which the market is down and up,\\nrespectively. Clusters 2 and 3 are characterized by up-market days for consumer\\nstocks and down-market days for energy stocks, respectively. Finally, cluster 1 cap‐\\ntures the days in which energy stocks were up and consumer stocks were down.\\n300 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 317}),\n",
       " Document(page_content='Figure 7-6. The means of the variables in each cluster (“cluster means”)\\nK-Means Clustering | 301', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 318}),\n",
       " Document(page_content='Cluster Analysis Versus PCA\\nThe plot of cluster means is similar in spirit to looking at the load‐\\nings for principal components analysis (PCA); see “Interpreting\\nPrincipal Components” on page 289. A major distinction is that\\nunlike with PCA, the sign of the cluster means is meaningful. PCA\\nidentifies principal directions of variation, whereas cluster analysis\\nfinds groups of records located near one another.\\nSelecting the Number of Clusters\\nThe K-means algorithm requires that you specify the number of clusters K. Some‐\\ntimes the number of clusters is driven by the application. For example, a company\\nmanaging a sales force might want to cluster customers into “personas” to focus and\\nguide sales calls. In such a case, managerial considerations would dictate the number\\nof desired customer segments—for example, two might not yield useful differentia‐\\ntion of customers, while eight might be too many to manage.\\nIn the absence of a cluster number dictated by practical or managerial considerations,\\na statistical approach could be used. There is no single standard method to find the\\n“best” number of clusters.\\nA common approach, called the elbow method , is to identify when the set of clusters\\nexplains “most” of the variance in the data. Adding new clusters beyond this set con‐\\ntributes relatively little in the variance explained. The elbow is the point where the\\ncumulative variance explained flattens out after rising steeply, hence the name of the\\nmethod.\\nFigure 7-7  shows the cumulative percent of variance explained for the default data for\\nthe number of clusters ranging from 2 to 15. Where is the elbow in this example?\\nThere is no obvious candidate, since the incremental increase in variance explained\\ndrops gradually. This is fairly typical in data that does not have well-defined clusters.\\nThis is perhaps a drawback of the elbow method, but it does reveal the nature of the\\ndata.\\n302 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 319}),\n",
       " Document(page_content=\"Figure 7-7. The elbow method applied to the stock data\\nIn R, the kmeans  function doesn’t provide a single command for applying the elbow\\nmethod, but it can be readily applied from the output of kmeans  as shown here:\\npct_var <- data.frame (pct_var = 0,\\n                      num_clusters  = 2:14)\\ntotalss <- kmeans(df, centers=14, nstart=50, iter.max =100)$totss\\nfor (i in 2:14) {\\n  kmCluster  <- kmeans(df, centers=i, nstart=50, iter.max =100)\\n  pct_var[i-1, 'pct_var' ] <- kmCluster $betweenss  / totalss\\n}\\nFor the KMeans  result, we get this information from the property inertia_ . After con‐\\nversion into a pandas  data frame, we can use its plot  method to create the graph:\\ninertia = []\\nfor n_clusters  in range(2, 14):\\n    kmeans = KMeans(n_clusters =n_clusters , random_state =0).fit(top_sp)\\n    inertia.append(kmeans.inertia_  / n_clusters )\\ninertias  = pd.DataFrame ({'n_clusters' : range(2, 14), 'inertia' : inertia})\\nax = inertias .plot(x='n_clusters' , y='inertia' )\\nplt.xlabel('Number of clusters(k)' )\\nplt.ylabel('Average Within-Cluster Squared Distances' )\\nK-Means Clustering | 303\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 320}),\n",
       " Document(page_content='plt.ylim((0, 1.1 * inertias .inertia.max()))\\nax.legend().set_visible (False)\\nIn evaluating how many clusters to retain, perhaps the most important test is this:\\nhow likely are the clusters to be replicated on new data? Are the clusters interpretable,\\nand do they relate to a general characteristic of the data, or do they just reflect a spe‐\\ncific instance? Y ou can assess this, in part, using cross-validation; see “Cross-\\nValidation” on page 155 .\\nIn general, there is no single rule that will reliably guide how many clusters to\\nproduce.\\nThere are several more formal ways to determine the number of\\nclusters based on statistical or information theory. For example,\\nRobert Tibshirani, Guenther Walther, and Trevor Hastie propose a\\n“gap” statistic  based on statistical theory to identify the elbow. For\\nmost applications, a theoretical approach is probably not necessary,\\nor even appropriate.\\nKey Ideas\\n•The number of desired clusters, K, is chosen by the user.\\n•The algorithm develops clusters by iteratively assigning records to the nearest\\ncluster mean until cluster assignments do not change.\\n•Practical considerations usually dominate the choice of K; there is no statistically\\ndetermined optimal number of clusters.\\nHierarchical Clustering\\nHierarchical clustering  is an alternative to K-means that can yield very different clus‐\\nters. Hierarchical clustering allows the user to visualize the effect of specifying differ‐\\nent numbers of clusters. It is more sensitive in discovering outlying or aberrant\\ngroups or records. Hierarchical clustering also lends itself to an intuitive graphical\\ndisplay, leading to easier interpretation of the clusters.\\n304 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 321}),\n",
       " Document(page_content=\"Key Terms for Hierarchical Clustering\\nDendrogram\\nA visual representation of the records and the hierarchy of clusters to which they\\nbelong.\\nDistance\\nA measure of how close one record  is to another.\\nDissimilarity\\nA measure of how close one cluster  is to another.\\nHierarchical clustering’s flexibility comes with a cost, and hierarchical clustering does\\nnot scale well to large data sets with millions of records. For even modest-sized data\\nwith just tens of thousands of records, hierarchical clustering can require intensive\\ncomputing resources. Indeed, most of the applications of hierarchical clustering are\\nfocused on relatively small data sets.\\nA Simple Example\\nHierarchical clustering works on a data set with n records and p variables and is\\nbased on two basic building blocks:\\n•A distance metric di,j to measure the distance between two records i and j.\\n•A dissimilarity metric DA,B to measure the difference between two clusters A and\\nB based on the distances di,j between the members of each cluster.\\nFor applications involving numeric data, the most importance choice is the dissimi‐\\nlarity metric. Hierarchical clustering starts by setting each record as its own cluster\\nand iterates to combine the least dissimilar clusters.\\nIn R, the hclust  function can be used to perform hierarchical clustering. One big dif‐\\nference with hclust  versus kmeans  is that it operates on the pairwise distances di,j\\nrather than the data itself. Y ou can compute these using the dist  function. For exam‐\\nple, the following applies hierarchical clustering to the stock returns for a set of\\ncompanies:\\nsyms1 <- c('GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB',\\n           'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\\n# take transpose: to cluster companies, we need the stocks along the rows\\ndf <- t(sp500_px [row.names (sp500_px ) >= '2011-01-01' , syms1])\\nd <- dist(df)\\nhcl <- hclust(d)\\nHierarchical Clustering | 305\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 322}),\n",
       " Document(page_content=\"Clustering algorithms will cluster the records (rows) of a data frame. Since we want to\\ncluster the companies, we need to transpose  (t) the data frame and put the stocks\\nalong the rows and the dates along the columns.\\nThe scipy  package offers a number of different methods for hierarchical clustering in\\nthe scipy.cluster.hierarchy  module. Here we use the linkage  function with the\\n“complete” method:\\nsyms1 = ['AAPL', 'AMZN', 'AXP', 'COP', 'COST', 'CSCO', 'CVX', 'GOOGL', 'HD',\\n         'INTC', 'JPM', 'MSFT', 'SLB', 'TGT', 'USB', 'WFC', 'WMT', 'XOM']\\ndf = sp500_px .loc[sp500_px .index >= '2011-01-01' , syms1].transpose ()\\nZ = linkage(df, method='complete' )\\nThe Dendrogram\\nHierarchical clustering lends itself to a natural graphical display as a tree, referred to\\nas a dendrogram . The name comes from the Greek words dendro  (tree) and gramma\\n(drawing). In R, you can easily produce this using the plot  command:\\nplot(hcl)\\nWe can use the dendrogram  method to plot the result of the linkage  function in\\nPython :\\nfig, ax = plt.subplots (figsize=(5, 5))\\ndendrogram (Z, labels=df.index, ax=ax, color_threshold =0)\\nplt.xticks(rotation =90)\\nax.set_ylabel ('distance' )\\nThe result is shown in Figure 7-8  (note that we are now plotting companies that are\\nsimilar to one another, not days). The leaves of the tree correspond to the records.\\nThe length of the branch in the tree indicates the degree of dissimilarity between cor‐\\nresponding clusters. The returns for Google and Amazon are quite dissimilar to one\\nanother and to the returns for the other stocks. The oil stocks (SLB, CVX, XOM,\\nCOP) are in their own cluster, Apple (AAPL) is by itself, and the rest are similar to\\none another.\\n306 | Chapter 7: Unsupervised Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 323}),\n",
       " Document(page_content='Figure 7-8. A dendrogram of stocks\\nIn contrast to K-means, it is not necessary to prespecify the number of clusters.\\nGraphically, you can identify different numbers of clusters with a horizontal line that\\nslides up or down; a cluster is defined wherever the horizontal line intersects the ver‐\\ntical lines. To extract a specific number of clusters, you can use the cutree  function:\\ncutree(hcl, k=4)\\nGOOGL  AMZN  AAPL  MSFT  CSCO  INTC   CVX   XOM   SLB   COP   JPM   WFC\\n    1     2     3     3     3     3     4     4     4     4     3     3\\n  USB   AXP   WMT   TGT    HD  COST\\n    3     3     3     3     3     3\\nHierarchical Clustering | 307', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 324}),\n",
       " Document(page_content='In Python , you achieve the same with the fcluster  method:\\nmemb = fcluster (Z, 4, criterion =\\'maxclust\\' )\\nmemb = pd.Series(memb, index=df.index)\\nfor key, item in memb.groupby(memb):\\n    print(f\"{key} : {\\', \\'.join(item.index)}\" )\\nThe number of clusters to extract is set to 4, and you can see that Google and Ama‐\\nzon each belong to their own cluster. The oil stocks all belong to another cluster. The\\nremaining stocks are in the fourth cluster.\\nThe Agglomerative Algorithm\\nThe main algorithm for hierarchical clustering is the agglomerative  algorithm, which \\niteratively merges similar clusters. The agglomerative algorithm begins with each\\nrecord constituting its own single-record cluster and then builds up larger and larger\\nclusters. The first step is to calculate distances between all pairs of records.\\nFor each pair of records x1,x2, ...,xp and y1,y2, ...,yp, we measure the distance\\nbetween the two records, dx,y, using a distance metric (see “Distance Metrics” on\\npage 241 ). For example, we can use Euclidian distance:\\ndx,y= x1−y12+x2−y22+⋯+xp−yp2\\nWe now turn to inter-cluster distance. Consider two clusters A and B, each with a dis‐\\ntinctive set of records, A=a1,a2, ...,am and B=b1,b2, ...,bq. We can measure the\\ndissimilarity between the clusters DA,B by using the distances between the mem‐\\nbers of A and the members of B.\\nOne measure of dissimilarity is the complete-linkage  method, which is the maximum\\ndistance across all pairs of records between A and B:\\nDA,B= max dai,bjfor all pairs i,j\\nThis defines the dissimilarity as the biggest difference between all pairs.\\n308 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 325}),\n",
       " Document(page_content='The main steps of the agglomerative algorithm are:\\n1.Create an initial set of clusters with each cluster consisting of a single record for\\nall records in the data.\\n2.Compute the dissimilarity DCk,Cℓ between all pairs of clusters k, ℓ.\\n3.Merge the two clusters Ck and Cℓ that are least dissimilar as measured by\\nDCk,Cℓ.\\n4.If we have more than one cluster remaining, return to step 2. Otherwise, we are\\ndone.\\nMeasures of Dissimilarity\\nThere are four common measures of dissimilarity: complete linkage , single linkage ,\\naverage linkage , and minimum variance . These (plus other measures) are all sup‐\\nported by most hierarchical clustering software, including hclust  and linkage . The\\ncomplete linkage method defined earlier tends to produce clusters with members that\\nare similar. The single linkage method is the minimum distance between the records\\nin two clusters:\\nDA,B= min dai,bjfor all pairs i,j\\nThis is a “greedy” method and produces clusters that can contain quite disparate ele‐\\nments. The average linkage method is the average of all distance pairs and represents\\na compromise between the single and complete linkage methods. Finally, the mini‐\\nmum variance method, also referred to as Ward’s  method, is similar to K-means since\\nit minimizes the within-cluster sum of squares (see “K-Means Clustering” on page\\n294).\\nFigure 7-9  applies hierarchical clustering using the four measures to the ExxonMobil\\nand Chevron stock returns. For each measure, four clusters are retained.\\nHierarchical Clustering | 309', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 326}),\n",
       " Document(page_content='Figure 7-9. A comparison of measures of dissimilarity applied to stock data\\nThe results are strikingly different: the single linkage measure assigns almost all of the\\npoints to a single cluster. Except for the minimum variance method ( R: Ward.D ;\\nPython : ward ), all measures end up with at least one cluster with just a few outlying\\npoints. The minimum variance method is most similar to the K-means cluster; com‐\\npare with Figure 7-5 .\\nKey Ideas\\n•Hierarchical clustering starts with every record in its own cluster.\\n•Progressively, clusters are joined to nearby clusters until all records belong to a\\nsingle cluster (the agglomerative algorithm).\\n•The agglomeration history is retained and plotted, and the user (without specify‐\\ning the number of clusters beforehand) can visualize the number and structure of\\nclusters at different stages.\\n•Inter-cluster distances are computed in different ways, all relying on the set of all\\ninter-record distances.\\n310 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 327}),\n",
       " Document(page_content='Model-Based Clustering\\nClustering methods such as hierarchical clustering and K-means are based on heuris‐\\ntics and rely primarily on finding clusters whose members are close to one another, as\\nmeasured directly with the data (no probability model involved). In the past 20 years,\\nsignificant effort has been devoted to developing model-based clustering  methods.\\nAdrian Raftery and other researchers at the University of Washington made critical\\ncontributions to model-based clustering, including both theory and software. The\\ntechniques are grounded in statistical theory and provide more rigorous ways to\\ndetermine the nature and number of clusters. They could be used, for example, in\\ncases where there might be one group of records that are similar to one another but\\nnot necessarily close to one another (e.g., tech stocks with high variance of returns),\\nand another group of records that are similar and also close (e.g., utility stocks with\\nlow variance).\\nMultivariate Normal Distribution\\nThe most widely used model-based clustering methods rest on the multivariate nor‐\\nmal distribution.  The multivariate normal distribution is a generalization of the nor‐\\nmal distribution to a set of p variables X1,X2, ...,Xp. The distribution is defined by a\\nset of means μ=μ1,μ2, ...,μ\\ud835 and a covariance matrix Σ. The covariance matrix is a\\nmeasure of how the variables correlate with each other (see “Covariance Matrix”  on\\npage 202  for details on the covariance). The covariance matrix Σ consists of p varian‐\\nces σ12,σ22, ...,σp2 and covariances σi,j for all pairs of variables i≠j. With the variables\\nput along the rows and duplicated along the columns, the matrix looks like this:\\nΣ=σ12σ1, 2⋯σ1,p\\nσ2, 1σ22⋯σ2,p\\n⋮ ⋮ ⋱ ⋮\\nσp, 1σp, 22⋯ σp2\\nNote that the covariance matrix is symmetric around the diagonal from upper left to\\nlower right. Since σi,j=σj,i, there are only p×p− 1 /2 covariance terms. In total,\\nthe covariance matrix has p×p− 1 /2 + p parameters. The distribution is denoted\\nby:\\nX1,X2, ...,Xp∼Npμ,Σ\\nModel-Based Clustering | 311', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 328}),\n",
       " Document(page_content='This is a symbolic way of saying that the variables are all normally distributed, and\\nthe overall distribution is fully described by the vector of variable means and the\\ncovariance matrix.\\nFigure 7-10  shows the probability contours for a multivariate normal distribution for\\ntwo variables X and Y (the 0.5 probability contour, for example, contains 50% of the\\ndistribution).\\nThe means are μx= 0.5  and μy= − 0.5 , and the covariance matrix is:\\nΣ=1 1\\n1 2\\nSince the covariance σxy is positive, X and Y are positively correlated.\\nFigure 7-10. Probability contours for a two-dimensional normal distribution\\nMixtures of Normals\\nThe key idea behind model-based clustering is that each record is assumed to be dis‐\\ntributed as one of K multivariate normal distributions, where K is the number of\\n312 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 329}),\n",
       " Document(page_content=\"clusters.  Each distribution has a different mean μ and covariance matrix Σ. For\\nexample , if you have two variables, X and Y, then each row Xi,Yi is modeled as hav‐\\ning been sampled from one of K multivariate normal distributions\\nNμ1,Σ1,Nμ2,Σ2, ...,NμK,ΣK.\\nR has a very rich package for model-based clustering called mclust , originally devel‐\\noped by Chris Fraley and Adrian Raftery. With this package, we can apply model-\\nbased clustering to the stock return data we previously analyzed using K-means and\\nhierarchical clustering:\\n> library(mclust)\\n> df <- sp500_px [row.names (sp500_px ) >= '2011-01-01' , c('XOM', 'CVX')]\\n> mcl <- Mclust(df)\\n> summary(mcl)\\nMclust VEE (ellipsoidal , equal shape and orientation ) model with 2 components :\\n log.likelihood     n df       BIC       ICL\\n      -2255.134  1131  9 -4573.546  -5076.856\\nClustering  table:\\n  1   2\\n963 168\\nscikit-learn  has the sklearn.mixture.GaussianMixture  class for model-based\\nclustering:\\ndf = sp500_px .loc[sp500_px .index >= '2011-01-01' , ['XOM', 'CVX']]\\nmclust = GaussianMixture (n_components =2).fit(df)\\nmclust.bic(df)\\nIf you execute this code, you will notice that the computation takes significantly\\nlonger than other procedures. Extracting the cluster assignments using the predict\\nfunction, we can visualize the clusters:\\ncluster <- factor(predict(mcl)$classification )\\nggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +\\n  geom_point (alpha=.8)\\nHere is the Python  code to create a similar figure:\\nfig, ax = plt.subplots (figsize=(4, 4))\\ncolors = [f'C{c}' for c in mclust.predict(df)]\\ndf.plot.scatter(x='XOM', y='CVX', c=colors, alpha=0.5, ax=ax)\\nax.set_xlim (-3, 3)\\nax.set_ylim (-3, 3)\\nThe resulting plot is shown in Figure 7-11 . There are two clusters: one cluster in the\\nmiddle of the data, and a second cluster in the outer edge of the data. This is very\\ndifferent from the clusters obtained using K-means ( Figure 7-5 ) and hierarchical\\nclustering ( Figure 7-9 ), which find clusters that are compact.\\nModel-Based Clustering | 313\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 330}),\n",
       " Document(page_content='Figure 7-11. Two clusters are obtained for stock return data using mclust\\nY ou can extract the parameters to the normal distributions using the summary\\nfunction:\\n> summary(mcl, parameters =TRUE)$mean\\n          [,1]        [,2]\\nXOM 0.05783847  -0.04374944\\nCVX 0.07363239  -0.21175715\\n> summary(mcl, parameters =TRUE)$variance\\n, , 1\\n          XOM       CVX\\nXOM 0.3002049  0.3060989\\nCVX 0.3060989  0.5496727\\n, , 2\\n         XOM      CVX\\nXOM 1.046318  1.066860\\nCVX 1.066860  1.915799\\n314 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 331}),\n",
       " Document(page_content=\"In Python , you get this information from the means_  and covariances_  properties of\\nthe result:\\nprint('Mean')\\nprint(mclust.means_)\\nprint('Covariances' )\\nprint(mclust.covariances_ )\\nThe distributions have similar means and correlations, but the second distribution\\nhas much larger variances and covariances. Due to the randomness of the algorithm,\\nresults can vary slightly between different runs.\\nThe clusters from mclust  may seem surprising, but in fact, they illustrate the statisti‐\\ncal nature of the method. The goal of model-based clustering is to find the best-fitting\\nset of multivariate normal distributions. The stock data appears to have a normal-\\nlooking shape: see the contours of Figure 7-10 . In fact, though, stock returns have a\\nlonger-tailed distribution than a normal distribution. To handle this, mclust  fits a\\ndistribution to the bulk of the data but then fits a second distribution with a bigger\\nvariance.\\nSelecting the Number of Clusters\\nUnlike K-means and hierarchical clustering, mclust  automatically selects the number\\nof clusters in R (in this case, two).  It does this by choosing the number of clusters for \\nwhich the Bayesian Information Criteria  (BIC) has the largest value (BIC is similar to\\nAIC; see “Model Selection and Stepwise Regression” on page 156). BIC works by\\nselecting the best-fitting model with a penalty for the number of parameters in the\\nmodel.  In the case of model-based clustering, adding more clusters will always\\nimprove the fit at the expense of introducing additional parameters in the model.\\nNote that in most cases BIC is usually minimized. The authors of\\nthe mclust  package decided to define BIC to have the opposite sign\\nto make interpretation of plots easier.\\nmclust  fits 14 different models with increasing number of components and chooses\\nan optimal model automatically. Y ou can plot the BIC values of these models using a\\nfunction in mclust :\\nplot(mcl, what='BIC', ask=FALSE)\\nThe number of clusters—or number of different multivariate normal models (compo‐\\nnents)—is shown on the x-axis (see Figure 7-12 ).\\nModel-Based Clustering | 315\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 332}),\n",
       " Document(page_content=\"Figure 7-12. BIC values for 14 models of the stock return data with increasing numbers\\nof components\\nThe GaussianMixture  implementation on the other hand will not try out various\\ncombinations. As shown here, it is straightforward to run multiple combinations\\nusing Python . This implementation defines BIC as usual. Therefore, the calculated\\nBIC value will be positive, and we need to minimize it.\\nresults = []\\ncovariance_types  = ['full', 'tied', 'diag', 'spherical' ]\\nfor n_components  in range(1, 9):\\n    for covariance_type  in covariance_types :\\n        mclust = GaussianMixture (n_components =n_components , warm_start =True,\\n                                 covariance_type =covariance_type ) \\n        mclust.fit(df)\\n        results.append({\\n            'bic': mclust.bic(df),\\n            'n_components' : n_components ,\\n            'covariance_type' : covariance_type ,\\n        })\\n316 | Chapter 7: Unsupervised Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 333}),\n",
       " Document(page_content=\"results = pd.DataFrame (results)\\ncolors = ['C0', 'C1', 'C2', 'C3']\\nstyles = ['C0-','C1:','C0-.', 'C1--']\\nfig, ax = plt.subplots (figsize=(4, 4))\\nfor i, covariance_type  in enumerate (covariance_types ):\\n    subset = results.loc[results.covariance_type  == covariance_type , :]\\n    subset.plot(x='n_components' , y='bic', ax=ax, label=covariance_type ,\\n                kind='line', style=styles[i])\\nWith the warm_start  argument, the calculation will reuse information from the\\nprevious fit. This will speed up the convergence of subsequent calculations.\\nThis plot is similar to the elbow plot used to identify the number of clusters to choose\\nfor K-means, except the value being plotted is BIC instead of percent of variance\\nexplained (see Figure 7-7 ). One big difference is that instead of one line, mclust\\nshows 14 different lines! This is because mclust  is actually fitting 14 different models\\nfor each cluster size, and ultimately it chooses the best-fitting model. GaussianMix\\nture  implements fewer approaches, so the number of lines will be only four.\\nWhy does mclust  fit so many models to determine the best set of multivariate nor‐\\nmals? It’s because there are different ways to parameterize the covariance matrix Σ for\\nfitting a model. For the most part, you do not need to worry about the details of the\\nmodels and can simply use the model chosen by mclust . In this example, according\\nto BIC, three different models (called VEE, VEV , and VVE) give the best fit using two\\ncomponents.\\nModel-based clustering is a rich and rapidly developing area of\\nstudy, and the coverage in this text spans only a small part of the\\nfield. Indeed, the mclust  help file is currently 154 pages long. Navi‐\\ngating the nuances of model-based clustering is probably more\\neffort than is needed for most problems encountered by data\\nscientists.\\nModel-based clustering techniques do have some limitations. The methods require an\\nunderlying assumption of a model for the data, and the cluster results are very depen‐\\ndent on that assumption. The computations requirements are higher than even hier‐\\narchical clustering, making it difficult to scale to large data. Finally, the algorithm is\\nmore sophisticated and less accessible than that of other methods.\\nModel-Based Clustering | 317\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 334}),\n",
       " Document(page_content='Key Ideas\\n•Clusters are assumed to derive from different data-generating processes with dif‐\\nferent probability distributions.\\n•Different models are fit, assuming different numbers of (typically normal)\\ndistributions.\\n•The method chooses the model (and the associated number of clusters) that fits\\nthe data well without using too many parameters (i.e., overfitting).\\nFurther Reading\\nFor more detail on model-based clustering, see the mclust  and GaussianMixture\\ndocumentation.\\nScaling and Categorical Variables\\nUnsupervised learning techniques generally require that the data be appropriately\\nscaled.  This is different from many of the techniques for regression and classification\\nin which scaling is not important (an exception is K-Nearest Neighbors; see “K-\\nNearest Neighbors” on page 238 ).\\nKey Terms for Scaling Data\\nScaling\\nSquashing or expanding data, usually to bring multiple variables to the same\\nscale.\\nNormalization\\nOne method of scaling—subtracting the mean and dividing by the standard\\ndeviation.\\nSynonym\\nStandardization\\nGower’s distance\\nA scaling algorithm applied to mixed numeric and categorical data to bring all\\nvariables to a 0–1 range.\\nFor example, with the personal loan data, the variables have widely different units\\nand magnitude. Some variables have relatively small values (e.g., number of years\\nemployed), while others have very large values (e.g., loan amount in dollars). If the\\n318 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 335}),\n",
       " Document(page_content=\"data is not scaled, then the PCA, K-means, and other clustering methods will be\\ndominated by the variables with large values and ignore the variables with small\\nvalues.\\nCategorical data can pose a special problem for some clustering procedures. As with\\nK-Nearest Neighbors, unordered factor variables are generally converted to a set of\\nbinary (0/1) variables using one hot encoding (see “One Hot Encoder” on page 242).\\nNot only are the binary variables likely on a different scale from other data, but the\\nfact that binary variables have only two values can prove problematic with techniques\\nsuch as PCA and K-means.\\nScaling the Variables\\nVariables with very different scale and units need to be normalized appropriately\\nbefore you apply a clustering procedure. For example, let’s apply kmeans  to a set of\\ndata of loan defaults without normalizing:\\ndefaults  <- loan_data [loan_data $outcome=='default' ,]\\ndf <- defaults [, c('loan_amnt' , 'annual_inc' , 'revol_bal' , 'open_acc' ,\\n                   'dti', 'revol_util' )]\\nkm <- kmeans(df, centers=4, nstart=10)\\ncenters <- data.frame (size=km$size, km$centers)\\nround(centers, digits=2)\\n   size loan_amnt  annual_inc  revol_bal  open_acc    dti revol_util\\n1    52  22570.19   489783.40   85161.35     13.33  6.91      59.65\\n2  1192  21856.38   165473.54   38935.88     12.61 13.48      63.67\\n3 13902  10606.48    42500.30   10280.52      9.59 17.71      58.11\\n4  7525  18282.25    83458.11   19653.82     11.66 16.77      62.27\\nHere is the corresponding Python  code:\\ndefaults  = loan_data .loc[loan_data ['outcome' ] == 'default' ,]\\ncolumns = ['loan_amnt' , 'annual_inc' , 'revol_bal' , 'open_acc' ,\\n           'dti', 'revol_util' ]\\ndf = defaults [columns]\\nkmeans = KMeans(n_clusters =4, random_state =1).fit(df)\\ncounts = Counter(kmeans.labels_)\\ncenters = pd.DataFrame (kmeans.cluster_centers_ , columns=columns)\\ncenters['size'] = [counts[i] for i in range(4)]\\ncenters\\nThe variables annual_inc  and revol_bal  dominate the clusters, and the clusters have\\nvery different sizes. Cluster 1 has only 52 members with comparatively high income\\nand revolving credit balance.\\nA common approach to scaling the variables is to convert them to z-scores by\\nsubtracting  the mean and dividing by the standard deviation. This is termed\\nScaling and Categorical Variables | 319\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 336}),\n",
       " Document(page_content=\"standardization  or normalization  (see “Standardization (Normalization, z-Scores)”  on\\npage 243  for more discussion about using z-scores):\\nz=x−x\\ns\\nSee what happens to the clusters when kmeans  is applied to the normalized data:\\ndf0 <- scale(df)\\nkm0 <- kmeans(df0, centers=4, nstart=10)\\ncenters0  <- scale(km0$centers, center=FALSE,\\n                 scale=1 / attr(df0, 'scaled:scale' ))\\ncenters0  <- scale(centers0 , center=-attr(df0, 'scaled:center' ), scale=FALSE)\\ncenters0  <- data.frame (size=km0$size, centers0 )\\nround(centers0 , digits=2)\\n  size loan_amnt  annual_inc  revol_bal  open_acc    dti revol_util\\n1 7355  10467.65    51134.87   11523.31      7.48 15.78      77.73\\n2 5309  10363.43    53523.09    6038.26     8.68 11.32      30.70\\n3 3713  25894.07   116185.91   32797.67     12.41 16.22      66.14\\n4 6294  13361.61    55596.65   16375.27     14.25 24.23      59.61\\nIn Python , we can use scikit-learn ’s StandardScaler . The inverse_transform\\nmethod allows converting the cluster centers back to the original scale:\\nscaler = preprocessing .StandardScaler ()\\ndf0 = scaler.fit_transform (df * 1.0)\\nkmeans = KMeans(n_clusters =4, random_state =1).fit(df0)\\ncounts = Counter(kmeans.labels_)\\ncenters = pd.DataFrame (scaler.inverse_transform (kmeans.cluster_centers_ ),\\n                       columns=columns)\\ncenters['size'] = [counts[i] for i in range(4)]\\ncenters\\nThe cluster sizes are more balanced, and the clusters are not dominated by\\nannual_inc  and revol_bal , revealing more interesting structure in the data. Note\\nthat the centers are rescaled to the original units in the preceding code. If we had left\\nthem unscaled, the resulting values would be in terms of z-scores and would there‐\\nfore be less interpretable.\\nScaling is also important for PCA. Using the z-scores is equivalent\\nto using the correlation matrix (see “Correlation”  on page 30)\\ninstead of the covariance matrix in computing the principal com‐\\nponents. Software to compute PCA usually has an option to use the\\ncorrelation matrix (in R, the princomp  function has the argument\\ncor).\\n320 | Chapter 7: Unsupervised Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 337}),\n",
       " Document(page_content=\"Dominant Variables\\nEven in cases where the variables are measured on the same scale and accurately\\nreflect relative importance (e.g., movement to stock prices), it can sometimes be use‐\\nful to rescale the variables.\\nSuppose we add Google (GOOGL) and Amazon (AMZN) to the analysis in “Inter‐\\npreting Principal Components” on page 289 . We see how this is done in R below:\\nsyms <- c('GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\\n          'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\\ntop_sp1 <- sp500_px [row.names (sp500_px ) >= '2005-01-01' , syms]\\nsp_pca1 <- princomp (top_sp1)\\nscreeplot (sp_pca1)\\nIn Python , we get the screeplot as follows:\\nsyms = ['GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\\n        'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST']\\ntop_sp1 = sp500_px .loc[sp500_px .index >= '2005-01-01' , syms]\\nsp_pca1 = PCA()\\nsp_pca1.fit(top_sp1)\\nexplained_variance  = pd.DataFrame (sp_pca1.explained_variance_ )\\nax = explained_variance .head(10).plot.bar(legend=False, figsize=(4, 4))\\nax.set_xlabel ('Component' )\\nThe screeplot displays the variances for the top principal components. In this case,\\nthe screeplot in Figure 7-13  reveals that the variances of the first and second compo‐\\nnents are much larger than the others. This often indicates that one or two variables\\ndominate the loadings. This is, indeed, the case in this example:\\nround(sp_pca1$loadings [,1:2], 3)\\n      Comp.1 Comp.2\\nGOOGL  0.781  0.609\\nAMZN   0.593 -0.792\\nAAPL   0.078  0.004\\nMSFT   0.029  0.002\\nCSCO   0.017 -0.001\\nINTC   0.020 -0.001\\nCVX    0.068 -0.021\\nXOM    0.053 -0.005\\n...\\nIn Python , we use the following:\\nloadings  = pd.DataFrame (sp_pca1.components_ [0:2, :], columns=top_sp1.columns)\\nloadings .transpose ()\\nThe first two principal components are almost completely dominated by GOOGL\\nand AMZN. This is because the stock price movements of GOOGL and AMZN dom‐\\ninate the variability.\\nScaling and Categorical Variables | 321\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 338}),\n",
       " Document(page_content='To handle this situation, you can either include them as is, rescale the variables (see\\n“Scaling the Variables” on page 319), or exclude the dominant variables from the\\nanalysis and handle them separately. There is no “correct” approach, and the treat‐\\nment depends on the application.\\nFigure 7-13. A screeplot for a PCA of top stocks from the S&P 500, including GOOGL\\nand AMZN\\nCategorical Data and Gower’s Distance\\nIn the case of categorical data, you must convert it to numeric data, either by ranking\\n(for an ordered factor) or by encoding as a set of binary (dummy) variables. If the\\ndata consists of mixed continuous and binary variables, you will usually want to scale\\nthe variables so that the ranges are similar; see “Scaling the Variables” on page 319.\\nOne popular method is to use Gower’s distance .\\nThe basic idea behind Gower’s distance is to apply a different distance metric to each\\nvariable depending on the type of data:\\n•For numeric variables and ordered factors, distance is calculated as the absolute\\nvalue of the difference between two records ( Manhattan distance ).\\n322 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 339}),\n",
       " Document(page_content=\"•For categorical variables, the distance is 1 if the categories between two records\\nare different, and the distance is 0 if the categories are the same.\\nGower’s distance is computed as follows:\\n1.Compute the distance di,j for all pairs of variables i and j for each record.\\n2.Scale each pair di,j so the minimum is 0 and the maximum is 1.\\n3.Add the pairwise scaled distances between variables together, using either a sim‐\\nple or a weighted mean, to create the distance matrix.\\nTo illustrate Gower’s distance, take a few rows from the loan data in R:\\n> x <- loan_data [1:5, c('dti', 'payment_inc_ratio' , 'home_', 'purpose_' )]\\n> x\\n# A tibble: 5 × 4\\n    dti payment_inc_ratio    home            purpose\\n  <dbl>             <dbl> <fctr>             <fctr>\\n1  1.00           2.39320   RENT                car\\n2  5.55           4.57170    OWN     small_business\\n3 18.08           9.71600   RENT              other\\n4 10.08          12.21520    RENT debt_consolidation\\n5  7.06           3.90888   RENT              other\\nThe function daisy  in the cluster  package in R can be used to compute Gower’s\\ndistance:\\nlibrary(cluster)\\ndaisy(x, metric='gower')\\nDissimilarities  :\\n          1         2         3         4\\n2 0.6220479\\n3 0.6863877  0.8143398\\n4 0.6329040  0.7608561  0.4307083\\n5 0.3772789  0.5389727  0.3091088  0.5056250\\nMetric :  mixed ;  Types = I, I, N, N\\nNumber of objects : 5\\nAt the moment of this writing, Gower’s distance is not available in any of the popular\\nPython packages. However, activities are ongoing to include it in scikit-learn . We\\nwill update the accompanying source code once the implementation is released.\\nAll distances are between 0 and 1. The pair of records with the biggest distance is 2\\nand 3: neither has the same values for home  and purpose , and they have very different\\nlevels of dti (debt-to-income) and payment_inc_ratio . Records 3 and 5 have the\\nsmallest distance because they share the same values for home  and purpose .\\nY ou can pass the Gower’s distance matrix calculated from daisy  to hclust  for hier‐\\narchical clustering (see “Hierarchical Clustering” on page 304 ):\\nScaling and Categorical Variables | 323\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 340}),\n",
       " Document(page_content=\"df <- defaults [sample(nrow(defaults ), 250),\\n               c('dti', 'payment_inc_ratio' , 'home', 'purpose' )]\\nd = daisy(df, metric='gower')\\nhcl <- hclust(d)\\ndnd <- as.dendrogram (hcl)\\nplot(dnd, leaflab='none')\\nThe resulting dendrogram is shown in Figure 7-14 . The individual records are not\\ndistinguishable on the x-axis, but we can cut the dendrogram horizontally at 0.5 and\\nexamine the records in one of the subtrees with this code:\\ndnd_cut <- cut(dnd, h=0.5)\\ndf[labels(dnd_cut$lower[[1]]),]\\n        dti payment_inc_ratio  home_           purpose_\\n44532 21.22           8.37694   OWN debt_consolidation\\n39826 22.59           6.22827   OWN debt_consolidation\\n13282 31.00           9.64200   OWN debt_consolidation\\n31510 26.21          11.94380    OWN debt_consolidation\\n6693  26.96           9.45600   OWN debt_consolidation\\n7356  25.81           9.39257   OWN debt_consolidation\\n9278  21.00          14.71850    OWN debt_consolidation\\n13520 29.00          18.86670    OWN debt_consolidation\\n14668 25.75          17.53440    OWN debt_consolidation\\n19975 22.70          17.12170    OWN debt_consolidation\\n23492 22.68          18.50250    OWN debt_consolidation\\nThis subtree consists entirely of owners with a loan purpose labeled as “debt_consoli‐\\ndation. ” While strict separation is not true of all subtrees, this illustrates that the cate‐\\ngorical variables tend to be grouped together in the clusters.\\nFigure 7-14. A dendrogram of hclust  applied to a sample of loan default data with\\nmixed variable types\\n324 | Chapter 7: Unsupervised Learning\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 341}),\n",
       " Document(page_content=\"Problems with Clustering Mixed Data\\nK-means and PCA are most appropriate for continuous variables. For smaller data\\nsets, it is better to use hierarchical clustering with Gower’s distance.  In principle,\\nthere is no reason why K-means can’t be applied to binary or categorical data. Y ou\\nwould usually use the “one hot encoder” representation (see “One Hot Encoder” on\\npage 242) to convert the categorical data to numeric values. In practice, however,\\nusing K-means and PCA with binary data can be difficult.\\nIf the standard z-scores are used, the binary variables will dominate the definition of\\nthe clusters. This is because 0/1 variables take on only two values, and K-means can\\nobtain a small within-cluster sum-of-squares by assigning all the records with a 0 or 1\\nto a single cluster. For example, apply kmeans  to loan default data including factor\\nvariables home  and pub_rec_zero , shown here in R:\\ndf <- model.matrix (~ -1 + dti + payment_inc_ratio  + home_ + pub_rec_zero ,\\n                   data=defaults )\\ndf0 <- scale(df)\\nkm0 <- kmeans(df0, centers=4, nstart=10)\\ncenters0  <- scale(km0$centers, center=FALSE,\\n                 scale=1/attr(df0, 'scaled:scale' ))\\nround(scale(centers0 , center=-attr(df0, 'scaled:center' ), scale=FALSE), 2)\\n    dti payment_inc_ratio  home_MORTGAGE  home_OWN  home_RENT  pub_rec_zero\\n1 17.20              9.27          0.00        1      0.00         0.92\\n2 16.99              9.11          0.00        0      1.00         1.00\\n3 16.50              8.06          0.52        0      0.48         0.00\\n4 17.46              8.42          1.00        0      0.00         1.00\\nIn Python :\\ncolumns = ['dti', 'payment_inc_ratio' , 'home_', 'pub_rec_zero' ]\\ndf = pd.get_dummies (defaults [columns])\\nscaler = preprocessing .StandardScaler ()\\ndf0 = scaler.fit_transform (df * 1.0)\\nkmeans = KMeans(n_clusters =4, random_state =1).fit(df0)\\ncenters = pd.DataFrame (scaler.inverse_transform (kmeans.cluster_centers_ ),\\n                       columns=df.columns)\\ncenters\\nThe top four clusters are essentially proxies for the different levels of the factor vari‐\\nables. To avoid this behavior, you could scale the binary variables to have a smaller\\nvariance than other variables. Alternatively, for very large data sets, you could apply\\nclustering to different subsets of data taking on specific categorical values. For exam‐\\nple, you could apply clustering separately to those loans made to someone who has a\\nmortgage, owns a home outright, or rents.\\nScaling and Categorical Variables | 325\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 342}),\n",
       " Document(page_content='Key Ideas\\n•Variables measured on different scales need to be transformed to similar scales so\\nthat their impact on algorithms is not determined mainly by their scale.\\n•A common scaling method is normalization (standardization)—subtracting the\\nmean and dividing by the standard deviation.\\n•Another method is Gower’s distance, which scales all variables to the 0–1 range\\n(it is often used with mixed numeric and categorical data).\\nSummary\\nFor dimension reduction of numeric data, the main tools are either principal compo‐\\nnents analysis or K-means clustering. Both require attention to proper scaling of the\\ndata to ensure meaningful data reduction.\\nFor clustering with highly structured data in which the clusters are well separated, all\\nmethods will likely produce a similar result. Each method offers its own advantage.\\nK-means scales to very large data and is easily understood. Hierarchical clustering\\ncan be applied to mixed data types—numeric and categorical—and lends itself to an\\nintuitive display (the dendrogram). Model-based clustering is founded on statistical\\ntheory and provides a more rigorous approach, as opposed to the heuristic methods.\\nFor very large data, however, K-means is the main method used.\\nWith noisy data, such as the loan and stock data (and much of the data that a data\\nscientist will face), the choice is more stark. K-means, hierarchical clustering, and\\nespecially model-based clustering all produce very different solutions. How should a\\ndata scientist proceed? Unfortunately, there is no simple rule of thumb to guide the\\nchoice. Ultimately, the method used will depend on the data size and the goal of the\\napplication.\\n326 | Chapter 7: Unsupervised Learning', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 343}),\n",
       " Document(page_content='Bibliography\\n[Baumer-2017]  Baumer, Benjamin, Daniel Kaplan, and Nicholas Horton. Modern\\nData Science with R . Boca Raton, Fla.: Chapman & Hall/CRC Press, 2017.\\n[bokeh]  Bokeh Development Team. “Bokeh: Python library for interactive visualiza‐\\ntion” (2014). https://bokeh.pydata.org .\\n[Deng-Wickham-2011]  Deng, Henry, and Hadley Wickham. “Density Estimation in\\nR. ” September 2011. https://oreil.ly/-Ny_6 .\\n[Donoho-2015]  Donoho, David. “50 Y ears of Data Science. ” September 18, 2015.\\nhttps://oreil.ly/kqFb0 .\\n[Duong-2001]  Duong, Tarn. “ An Introduction to Kernel Density Estimation. ” 2001.\\nhttps://oreil.ly/Z5A7W .\\n[Few-2007]  Few, Stephen. “Save the Pies for Dessert. ” Visual Business Intelligence\\nNewsletter . Perceptual Edge. August 2007. https://oreil.ly/_iGAL .\\n[Freedman-2007]  Freedman, David, Robert Pisani, and Roger Purves. Statistics . 4th\\ned. New Y ork: W . W . Norton, 2007.\\n[Hintze-Nelson-1998]  Hintze, Jerry L., and Ray D. Nelson. “Violin Plots: A Box Plot–\\nDensity Trace Synergism. ” The American Statistician  52, no. 2 (May 1998): 181–84.\\n[Galton-1886]  Galton, Francis. “Regression Towards Mediocrity in Hereditary Stat‐\\nure. ” The Journal of the Anthropological Institute of Great Britain and Ireland  15\\n(1886): 246–63. https://oreil.ly/DqoAk .\\n[ggplot2]  Wickham, Hadley. ggplot2: Elegant Graphics for Data Analysis . New Y ork:\\nSpringer-Verlag New Y ork, 2009. https://oreil.ly/O92vC .\\n[Hyndman-Fan-1996]  Hyndman, Rob J., and Y anan Fan. “Sample Quantiles in Statis‐\\ntical Packages. ” American Statistician  50, no. 4 (1996): 361–65.\\n[lattice]  Sarkar, Deepayan. Lattice: Multivariate Data Visualization with R . New Y ork:\\nSpringer, 2008. http://lmdvr.r-forge.r-project.org .\\n[Legendre]  Legendre, Adrien-Marie. Nouvelle méthodes pour la détermination des\\norbites des comètes . Paris: F. Didot, 1805. https://oreil.ly/8FITJ .\\n[NIST-Handbook-2012]  “Measures of Skewness and Kurtosis. ” In NIST/SEMATECH\\ne-Handbook of Statistical Methods . 2012. https://oreil.ly/IAdHA .\\n327', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 344}),\n",
       " Document(page_content='[R-base-2015]  R Core Team. “R: A Language and Environment for Statistical Com‐\\nputing. ” R Foundation for Statistical Computing. 2015. https://www.r-project.org .\\n[Salsburg-2001]  Salsburg, David. The Lady Tasting Tea: How Statistics Revolutionized\\nScience in the Twentieth Century . New Y ork: W . H. Freeman, 2001.\\n[seaborn]  Waskom, Michael. “Seaborn: Statistical Data Visualization. ” 2015. https://\\nseaborn.pydata.org .\\n[Trellis-Graphics]  Becker, Richard A., William S.Cleveland, Ming-Jen Shyu, and Ste‐\\nphen P . Kaluzny. “ A Tour of Trellis Graphics. ” April 15, 1996. https://oreil.ly/\\nLVnOV .\\n[Tukey-1962]  Tukey, John W . “The Future of Data Analysis. ” The Annals of Mathemat‐\\nical Statistics  33, no. 1 (1962): 1–67. https://oreil.ly/qrYNW .\\n[Tukey-1977]  Tukey, John W . Exploratory Data Analysis . Reading, Mass.: Addison-\\nWesley, 1977.\\n[Tukey-1987]  Tukey, John W . The Collected Works of John W . Tukey . Vol. 4, Philosophy\\nand Principles of Data Analysis: 1965–1986 , edited by Lyle V . Jones. Boca Raton,\\nFla.: Chapman & Hall/CRC Press, 1987.\\n[Zhang-Wang-2007]  Zhang, Qi, and Wei Wang. “ A Fast Algorithm for Approximate\\nQuantiles in High Speed Data Streams. ” 19th International Conference on Scientific\\nand Statistical Database Management (SSDBM 2007) . Piscataway, NJ: IEEE, 2007.\\nAlso available at https://oreil.ly/qShjk .\\n328 | Bibliography', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 345}),\n",
       " Document(page_content=\"Index\\nA\\nA/B testing, 88-93\\nbenefits of using a control group, 90\\nepsilon-greedy algorithm for, 133\\nexamples of, 88\\nhypotheses in, 93\\nimportance of obtaining permissions, 92\\nmulti-arm bandits versus, 134\\ntraditional, shortcoming of, 91\\naccuracy, 220\\nAdaboost, 270\\nboosting algorithm, 271\\nadjusted R-squared, 154\\nadjustment of p-values, 113\\nagglomerative algorithm, 308\\nAIC (Akaike's Information Criteria), 157, 315\\nAICc, 157\\nall subset regression, 157\\nalpha, 103, 107\\nalpha inflation, 113\\nalternative hypothesis, 93, 95\\nAmerican Statistical Association (ASA), state‐\\nment on use of p-values, 108\\nanalysis of variance (ANOV A), 82, 118-124\\ndecomposition of variance, 123\\nF-statistic, 121\\ntwo-way ANOV A, 123\\nanomaly detection, 141\\noutliers and, 11\\narms (multi-arm bandits), 132\\nAUC (area under the ROC curve), 226-228\\naverage linkage metric, 309\\naverage value (see mean)B\\nb-spline (basis spline), 190\\nbackward elimination, 157, 159\\nbackward selection, 159\\nbagging, 64, 96, 238, 260\\nboosting versus, 270\\nbandit algorithms, 132\\n(see also multi-arm bandits)\\nbar charts, 27\\nhistograms and, 29\\nBayesian approach in Thompson's sampling,\\n134\\nBayesian classification, 197\\n(see also naive Bayes algorithm)\\nBayesian information criteria (BIC), 157,\\n315-317\\nbeta distribution, 134\\nbias, 50\\nbiased estimates from naive Bayes classifier,\\n200\\nselection bias, 54-57\\nbias-variance trade-off, 247\\nbiased estimates, 15\\nbidirectional alternative hypothesis, 95\\nbig data\\npredictive models in, 47\\nvalue of, 52\\nbinary data, 2\\nexploring, 27-30\\nbinary dummy variables, 242\\nbinary outcomes, 78\\nbinary variables, 163, 319\\nbinomial distribution, 79-80\\nbinomial trials, 79\\n329\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 346}),\n",
       " Document(page_content=\"bins, in frequency tables and histograms, 22\\nbiplot, 292\\nbivariate analysis, 36\\nblack swan theory, 73\\nblind studies, 91\\nboosted trees, 249, 260\\nboosting, 238, 270-282\\nversus bagging, 270\\nboosting algorithm, 271\\nhyperparameters and cross-validation,\\n279-281\\nregularization, avoiding overfitting with,\\n274-279\\nXGBoost software, 272-274\\nbootstrap, 60, 61-65\\nalgorithm for bootstrap resampling of the\\nmean, 62\\nbootstrap and permutation tests, 102\\nconfidence interval generation, 66, 162-163\\nin resampling, 96\\nresampling versus bootstrapping, 65\\nsampling of variables in random forest par‐\\ntitioning, 261\\nstandard error and, 61\\nbootstrap aggregating (see bagging)\\nbootstrap sample, 62\\nboxplots, 19\\ncomparing numeric and categorical data, 41\\nextending with conditional variables, 43\\npercentiles and, 20-21\\nviolin plots versus, 42\\nBreiman, Leo, 237, 249\\nbubble plots, 180\\nC\\ncategorical data\\nexploring, 27-30\\nexpected value, 29\\nmode, 29\\nprobability, 30\\nexploring numeric variable grouped by cate‐\\ngorical variable, 41\\nexploring two categorical variables, 39\\nimportance of the concept, 3\\ncategorical variables, 163\\n(see also factor variables)\\nconverting to dummy variables, 165\\nrequired for naive Bayes algorithm, 197scaling (see scaling and categorical vari‐\\nables)\\ncausation, regression and, 149\\ncentral limit theorem, 57, 60\\nStudent's t-distribution and, 77\\ncentral tendency (see estimates of location)\\nchance variation, 104\\nchi-square distribution, 80, 127\\nchi-square statistic, 124, 125\\nchi-square test, 124-131\\nFisher's exact test, 128-130\\nrelevance for data science, 130\\nresampling approach, 124\\nstatistical theory, 127\\nclass purity, 254, 254\\nclassification, 195-236\\ndiscriminant analysis, 201-207\\ncovariance matrix, 202\\nFisher's linear discriminant, 203\\nsimple example, 204-207\\nevaluating models, 219-230\\nAUC metric, 226-228\\nconfusion matrix, 221-222\\nlift, 228\\nprecision, recall, and specificity, 223\\nrare class problem, 223\\nROC curve, 224-226\\nlogistic regression, 208-219\\ncomparison to linear regression, 214-216\\ngeneralized linear models, 210-212\\ninterpreting coefficients and odds ratio,\\n213\\nlogistic response function and logit, 208\\npredicted values from, 212\\nnaive Bayes algorithm, 196-201\\napplying to numeric predictor variables,\\n200\\npredicting more than two classes, 196\\nstrategies for imbalanced data, 230-236\\ncost-based classification, 234\\ndata generation, 233\\nexploring the predictions, 234-235\\noversampling and up/down weighting,\\n232\\nundersampling, 231\\nunsupervised learning as building block,\\n284\\nClassification and Regression Trees (CART),\\n249\\n330 | Index\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 347}),\n",
       " Document(page_content=\"(see also tree models)\\ncluster mean, 294, 295, 300\\nPCA loadings versus, 302\\nclustering, 283\\ncategorical variables posing problems in,\\n319\\nhierarchical, 304-310\\nagglomerative algorithm, 308\\ndissimilarity metrics, 309-310\\nrepresentation in a dendrogram, 306-308\\nsimple example, 305\\nK-means, 294-304\\ninterpreting the clusters, 299-302\\nK-means algorithm, 298\\nselecting the number of clusters, 302\\nsimple example, 295\\nmodel-based, 311-318\\nmixtures of normals, 313-315\\nmultivariate normal distribution, 311\\nselecting the number of clusters, 315-317\\nproblems with clustering mixed data, 325\\nuses of, 284\\nclusters, 294\\ncoefficient of determination, 154\\ncoefficients\\nconfidence intervals and, 162\\nin logistic regression, 213\\nin multiple linear regression, 152\\nin simple linear regression, 143\\ncold-start problems, using clustering for, 284\\ncomplete-linkage method, 308, 309\\nconditional probability, 197\\nconditioning variables, 43\\nconfidence intervals, 161\\nalgorithm for bootstrap confidence interval,\\n66\\napplication to data science, 68\\nlevel of confidence, 68\\nprediction intervals versus, 163\\nconfounding variables, 170, 172, 172-173\\nconfusion matrix, 220, 221-222\\ncontingency tables, 36\\nsummarizing two categorical variables, 39\\ncontinuous data, 2\\ncontour plots, 36\\nusing with hexagonal binning, 36-39\\ncontrast coding, 167\\ncontrol group, 88\\nbenefits of using, 90Cook's distance, 180\\ncorrelated predictor variables, 170\\ncorrelation, 30-36\\nexample, correlation between ETF returns,\\n32\\nkey concepts, 35\\nkey terms for, 30\\nscatterplots, 34\\ncorrelation coefficient, 31\\ncalculating Pearson's correlation coefficient,\\n31\\nother types of, 34\\ncorrelation matrix, 32, 320\\ncorrespondence analysis, 292-294\\ncost-based classification, 234\\ncovariance, 202, 288\\ncovariance matrix, 203, 242, 311\\ncross validation, 155, 247\\nusing for hyperparameters, 279-281\\nusing to select principal components, 292\\nusing to test values of hyperparameters, 270\\ncumulative gains chart, 228\\nD\\nd.f. (degrees of freedom), 117\\n(see also degrees of freedom)\\ndata analysis, 1\\n(see also exploratory data analysis)\\ndata distribution, 19-27, 57\\ndensity plots and estimates, 24-27\\nfrequency table and histogram, 22-24\\npercentiles and boxplots, 20-21\\nsampling distribution versus, 58\\ndata frames, 4\\nhistograms for, 23\\nand indexes, 6\\ntypical data frame, 5\\ndata generation, 230, 233\\ndata quality, 48\\nsample size versus, 52\\ndata science\\nA/B testing in, 91\\nmultiplicity and, 115\\np-values and, 109\\npermutation tests, value of, 102\\nrelevance of chi-square tests, 130\\nt-statistic and, 155\\nvalue of heteroskedasticity for, 183\\ndata snooping, 54\\nIndex | 331\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 348}),\n",
       " Document(page_content=\"data types\\nkey terms for, 2\\nresources for further reading, 4\\ndata-centric approach, excessive, 75\\ndatabase normalization vs. normalization in\\nstatistics, 243\\ndatabases, data types in, 4\\ndecile gains charts, 228\\ndecision trees, 249\\nensemble learning applied to, 237\\nolder meaning in human decision analysis,\\n250\\nrunning multiple on bootstrap samples, 63\\ndecomposition of variance, 118, 123\\ndegrees of freedom, 15, 116-118, 122\\nfor chi-square distribution, 127\\nt-distribution and, 77\\ndendrograms, 305, 306-308, 324\\ndensity plots, 19\\nand estimates, 24\\ndeparture from expectation, 80\\ndependent variables, 6, 143\\n(see also response)\\ndeviance, 215\\nattempt to minimize in logistic regression,\\n233\\ndeviation coding, 164, 167\\ndeviations\\ndefined, 13\\nstandard deviation and related estimates, 14\\ndiscrete data, 2\\ndiscriminant analysis, 201-207\\ncovariance matrix, 202\\nFisher's linear discriminant, 203\\nlinear discriminant analysis (LDA), 202\\nsimple example, 204-207\\nvariants of, 207\\ndiscriminant function, 202\\ndiscriminant weights, 202\\ndispersion, 13\\n(see also variability)\\ndissimilarity metrics, 305, 309-310\\ncomplete-linkage method, 308\\ndistance metrics, 239, 241\\nGower's distance, 322\\nin hierarchical clustering, 305, 308\\nManhattan distance, 322\\ndominant variables, 321-322\\nDonoho, David, 1double blind studies, 91\\ndownsampling, 231\\n(see also undersampling)\\ndummy variables, 163, 242\\nrepresentation of factor variables in regres‐\\nsion, 164\\nDurbin-Watson statistic, 184\\nE\\neffect size, 135, 137\\nelbow method, 302, 317\\nElder, John, 55\\nensemble learning, 237\\nstaged use of K-Nearest Neighbors, 247\\nensemble of models, 259\\nbagging and boosting, 260\\ncreating using boosting, 270\\nentropy of information, 254\\nepsilon-greedy algorithm for A/B test, 133\\nerrors, 69\\nprediction errors, 146\\n(see also residuals)\\nestimates\\nhat notation and, 147\\nmetrics and, 9\\nestimates of location, 7-13\\nEuclidean distance, 242, 308\\nexact tests, 102\\nFisher's exact test, 128-130\\nexhaustive permutation tests, 102\\nexpectation or expected, 124\\ndeparture from, 80\\nexpected value, 27, 29\\nexplanation (profiling), prediction versus, 149\\nexploratory data analysis, 1-46\\ncategorical and binary data, 2-4, 27-30\\ncorrelation, 30-36\\ndata distribution, 19-27\\nestimates of location, 7-13\\nestimates of variability, 13-19\\nexploring two or more variables, 36-46\\nfor predictions from classification models,\\n234\\nunsupervised learning as extension of, 283\\nExploratory Data Analysis (Tukey), 1\\nexponential distribution, 84\\nextrapolation\\ndangers of, 161\\ndefined, 161\\n332 | Index\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 349}),\n",
       " Document(page_content=\"F\\nF-distribution, 82\\nF-statistic, 118, 121, 155\\nfacets, 44\\nfactor variables, 3, 117, 163-169\\nbinary, odds ratio for, 213\\ncoding in logistic regression, 216\\ndifferent codings, 167\\ndummy variable representations, 164\\nin naive Bayes algorithm, 197\\nordered, 169\\nwith many levels, 167-168\\nfailure rate, estimating, 84\\nfalse discovery rate, 113, 115\\nfalse positive rate, 222\\nfeature selection\\nchi-square tests in, 131\\nin stepwise regression, 156\\nusing discriminant analysis for, 204\\nfeatures, 5, 143\\n(see also predictor variables)\\nK-Nearest Neighbors as feature engine,\\n247-248\\nterminology differences, 6\\nfield view (spatial data), 6\\nFisher's exact test, 128-130\\nrelevance for data science, 130\\nFisher's linear discriminant, 203\\nFisher's scoring, 215\\nFisher, R.A., 201\\nfitted values, 142\\nin multiple linear regression, 151\\nin simple linear regression, 146\\nfitting the model\\nbias-variance trade-off, 247\\nK-Nearest Neighbors, advantages of, 247\\nlinear versus logistic regression, 215\\nrandom forest fit to loan default data, 265\\nrules for simple tree model fit to loan data,\\n251\\nfolds, 156, 279\\nforward selection, 157, 159\\nfrequency tables, 19\\nexample, murder rates by state, 22\\nFriedman, Jerome H. (Jerry), 237\\nG\\ngains, 228\\n(see also lift)Gallup Poll, 49\\nGalton, Francis, 55, 259\\nGauss, Carl Friedrich, 148\\nGaussian distribution, 70\\n(see also normal distribution)\\ngeneralized additive models (GAM), 187,\\n192-193\\nlogistic regression fit with, 234\\ngeneralized linear models (GLMs), 210-212\\ncharacteristics and applications of, 212\\nGini coefficient, 255\\nGini impurity, 254, 266\\nGoogle Analytics, 98\\nGosset, W . S., 76\\nGower's distance, 318\\nusing to scale categorical variables, 322\\ngradient boosting, 270, 272\\ngraphs, 7\\nin computer science versus statistics, 7\\ngreedy algorithms, 134\\nH\\nhat notation, estimates versus known values,\\n147\\nhat-value, 176, 179\\nheat maps, 39\\nheteroskedastic errors, 183\\nheteroskedasticity, 176, 182\\nvalue to data science, 183\\nhexagonal binning, 36\\nand contours, plotting relationship between\\ntwo numeric values, 36-39\\nextending with conditional variables, 43\\nhierarchical clustering, 304-310\\nagglomerative algorithm, 308\\ndissimilarity metrics, 309-310\\nrepresentation in dendrogram, 306-308\\nsimple example, 305\\nusing with Gower's distance, 323\\nhistograms, 19\\nbar charts and, 29\\nplotting of, 23\\nvisualizing frequency tables with, 23\\nhomogeneity, measuring, 254\\nhyperparameters, 269, 271\\nhypothesis tests, 93-96\\nalternative hypothesis, 95\\nfalse discovery rate, 115\\nmisinterpreting randomness, 93\\nIndex | 333\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 350}),\n",
       " Document(page_content=\"null hypothesis, 94\\none-way and two-way tests, 95\\nstructured to minimize type 1 errors, 109\\nI\\nif-then-else rules (tree models), 250\\nimbalanced data strategies for classification\\nmodels, 230-236, 241\\ncost-based classification, 234\\ndata generation, 233\\nexploring the predictions, 234-235\\noversampling and up/down weighting, 232\\nimpurity, 250\\nmeasuring, 254, 257\\nin-sample validation methods, 159\\nindependent variables, 142, 143\\n(see also predictor variables)\\nindexes, data frames and, 6\\nindicator variables, 163\\ninference, 1, 87\\ninfluence plots, 180\\ninfluential values, 176, 179\\ninteractions, 170, 265\\nand main effects, 174-176\\nintercept, 142, 143\\nInternet of Things (IoT), 2\\ninterquartile range, 14, 17\\ncalculating, 18\\ninterval endpoints, 66\\ninverse logit function, 208\\ninverse odds function, 209\\nIQR (see interquartile range)\\nK\\nK (in K-means clustering), 294, 302\\nK (in K-Nearest Neighbors), 230, 239, 249\\nk-fold cross-validation, 155\\nK-means clustering, 294-304\\napplying to data without normalization, 319\\napplying to normalized data, 320\\ninterpreting the clusters, 299-302\\nK-means algorithm, 298\\nselecting the number of clusters, 302\\nsimple example, 295\\nusing with binary data, 325\\nK-Nearest Neighbors, 237, 238-249\\ncategorical data and, 319\\nchoosing K, 246\\ndistance metrics, 241example, predicting loan default, 239-241\\nas a feature engine, 247-248\\none hot encoder and, 242\\nstandardization in, 243-246\\nKendall's tau, 34\\nkernel density estimates, 19, 24\\n(see also density plots)\\nKNN (see K-Nearest Neighbors)\\nknots, 187, 190\\nkurtosis, 24\\nL\\nlasso regression, 159, 278\\nLatent Dirichlet Allocation, 202\\nleaf, 250\\nleast squares regression, 143, 148, 278\\nLegendre, Adrien-Marie, 149\\nlevel of confidence, 68\\nleverage\\ndefined, 176\\ninfluential values in regression, 179\\nlift, 220, 228\\nlift curve, 228\\nuplift and, 229\\nlinear discriminant analysis (LDA), 202, 234\\n(see also discriminant analysis)\\nprincipal components analysis as unsuper‐\\nvised version, 285\\nlinear discriminant function, 204, 207\\nlinear model (lm), 144, 150\\nlinear regression\\ncomparison to logistic regression, 214-216\\nexamination of residuals to see if fit can be\\nimproved, 270\\ngeneralized linear model (GLM), 210\\nmulticollinearity problems caused by one\\nhot encoding, 243\\nmultiple, 150-161\\nassessing the model, 153\\ncross validation, 155\\nexample, estimating value of houses,\\n151-152\\nmodel selection and stepwise regression,\\n156-159\\nweighted regression, 159-161\\nprediction vs. explanation, 149\\nsimple, 141-150\\nfitted values and residuals, 146\\nleast squares, 148\\n334 | Index\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 351}),\n",
       " Document(page_content='regression equation, 143-146\\nLiterary Digest poll of 1936, 49\\nloadings, 285, 285\\ncluster mean versus, 302\\nplotting for top principal components, 290\\nwith negative signs, 288\\nlocation, estimates of, 7-13\\nlog-odds function (see logit function)\\nlog-odds ratio and odds ratio, 214\\nlogistic linear regression, 234\\nlogistic regression, 208-219\\nassessing the model, 216-219\\ncomparison to linear regression, 214-216\\nfit using generalized additive model, 234\\nand the generalized linear model, 210-212\\ninterpreting coefficients and odds ratio, 213\\nlogistic response function and logit, 208\\nmulticollinearity problems caused by one\\nhot encoding, 243\\npredicted values from, 212\\nlogistic response function, 208, 209\\nlogit function, 208, 209\\nlong-tail distributions, 73-75\\nloss, 250\\nin simple tree model example, 252\\nloss function, 233\\nM\\nmachine learning, 237\\n(see also statistical machine learning; super‐\\nvised learning; unsupervised learning)\\noverfitting risk, mitigating, 113\\nstatistics versus, 238\\nuse of resampling to improve models, 96\\nMAD (see median absolute deviation from the\\nmedian)\\nMahalanobis distance, 203, 242\\nmain effects, 170\\ninteractions and, 174-176\\nMallows Cp, 157\\nManhattan distance, 242, 278, 322\\nmaximum likelihood estimation (MLE), 215,\\n219\\nmean, 9, 9\\nregression to, 55\\nsample mean versus population mean, 53\\ntrimmed mean, 9\\nweighted mean, 10\\nmean absolute deviation, 14formula for calculating, 15\\nmedian, 8, 10\\nmedian absolute deviation from the median\\n(MAD), 14, 16\\nmedical screening tests, false positives and, 222\\nmetrics, 9\\nminimum variance metric, 309\\nmode, 27\\nexamples in categorical data, 29\\nmodel-based clustering, 311-318\\nlimitations of, 317\\nmixtures of normals, 313-315\\nmultivariate normal distribution, 311\\nselecting the number of clusters, 315-317\\nmoments (of a distribution), 24\\nmulti-arm bandits, 91, 131-134\\nmulticollinearity, 170, 172\\nand predictors used twice in KNN, 247\\nproblems caused by one hot encoding, 243\\nmulticollinearity errors, 117, 165\\nmultiple testing, 112-116\\nmultivariate analysis, 36-46\\nmultivariate bootstrap sampling, 63\\nmultivariate normal distribution, 311\\nmixtures of normals, 313-315\\nN\\nN (or n) referring to total records, 9\\nn or n – 1, dividing by in variance formula, 15\\nn or n – 1, dividing by in variance or standard\\ndeviation formula, 116\\nn or sample size, 117\\nnaive Bayes algorithm, 196-201\\nnumeric predictor variables with, 200\\nsolution, 198-200\\nwhy exact Bayesian classification is imprac‐\\ntical, 197\\nneighbors (in K-Nearest Neighbors), 239\\nnetwork data structures, 7\\nnodes, 249\\nnon-normal residuals, 176, 182\\nnonlinear regression, 188\\nnonrandom samples, 49\\nnonrectangular data structures, 6\\nnormal distribution, 69-72\\nkey concepts, 72\\nmisconceptions about, 69\\nmultivariate, 311\\nstandard normal and QQ-Plots, 71\\nIndex | 335', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 352}),\n",
       " Document(page_content=\"normalization, 71, 243, 318\\n(see also standardization)\\nin statistics, vs. database normalization, 243\\nscaling the variables, 319-320\\nnull hypothesis, 93, 94, 104\\nin click rate testing for web headlines, 125\\nusing alternative hypothesis with, 95\\nnumeric data, 2\\ndimension reduction of, 326\\nexploring relationship between two numeric\\nvariables, 36-39\\ngrouped by categorical variable, exploring,\\n41\\nnumeric variables\\nconversion of factor variables to, in regres‐\\nsion, 163\\nconverting ordered factor variables to, 169\\nMahalanobis distance, 242\\nO\\nobject representation (spatial data), 6\\nOccam's razor, 156\\nodds, 209\\nobtaining probability from, 209\\nodds ratio, 213\\nrelationship with log-odds ratio, 214\\nomnibus tests, 118\\none hot encoding, 164, 165, 243, 319\\none-way ANOV A, 123\\none-way tests, 93, 95\\norder statistics, 14, 16\\nordered factor variables, 169\\nordinal data, 2\\nimportance of the concept, 3\\nordinary least squares (OLS) regression, 148,\\n182\\nout-of-bag estimate of error, 262\\nout-of-sample validation, 155\\noutcome, 5\\noutliers, 8, 11\\ncorrelation coefficient and, 33\\nin boxplots, 21\\nin regression diagnostics, 177-178\\noverfitting, 113\\navoiding using regularization, 274-279\\noversampling, 230\\nand up/down weighting, 232P\\np-values, 103-110, 121\\nadjustment of, 113\\nalpha, 107\\nchi-square distribution and, 127\\ncontroversy over use of, 107\\ndata science and, 109\\npractical significance and, 109\\nt-statistic and, 154\\ntype 1 and type 2 errors, 109\\npairwise comparisons, 118, 119\\npartial residual plots, 176, 185\\nfor spline regression, 191\\nin logistic regression, 217\\nnonlinearity and, 187\\npartitions in trees, 249, 258\\nrandom forests, 261\\nrecursive partitioning algorithm, 252-254\\nPCA (see principal components analysis)\\nPearson residuals, 125\\nPearson's chi-square test, 124\\nPearson's correlation coefficient, 31\\nPearson, Karl, 285\\npenalized regression, 159, 175\\npenalty on model complexity, 276\\npercentiles, 8, 14\\nand boxplots, 20-21\\nestimates based on, 17\\nprecise definition of, 17\\npermissions for scientific and medical testing,\\n92\\npermutation tests, 96, 97-103, 105\\nfor ANOV A, 120\\nfor chi-square test, 126\\nestimating p-values from, 106\\nexhaustive and bootstrap, 102\\nvalue for data science, 102\\nweb stickiness example, 98-102\\npertinent records (in searches), 53\\npie charts, 27, 28\\npivot tables, 40\\n(see also contingency tables)\\npoint estimates, 66\\nPoisson distributions, 83, 212\\npolynomial coding, 167\\npolynomial regression, 188\\npopulation, 48\\npopulation mean vs. sample mean, 53\\nposterior probability, 197, 200\\n336 | Index\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 353}),\n",
       " Document(page_content='power and sample size, 135-139\\ncalculating, components in, 137\\npower, 135\\nsample size, 136\\npractical significance versus statistical signifi‐\\ncance, 109\\nprecision, 220, 223\\nprecision-recall (PR) curve, 226\\npredicted values, 146\\n(see also fitted values)\\nprediction, 195\\n(see also classification)\\nexploring predictions from classification\\nmodels, 234\\nfitted values and residuals in simple linear\\nregression, 146\\nfrom random forests, 264\\nfrom XGBoost applied to loan default data,\\n274\\nharnessing results from multiple trees, 258\\npredicted values from logistic regression,\\n212\\npredicting a continuous value with tree\\nmodel, 257\\npredicting loan default with K-Nearest\\nNeighbors, 239-241\\nprediction vs. explanation in simple linear\\nregression, 149\\nunsupervised learning and, 284\\nusing regression, 141, 161-163\\nconfidence and prediction intervals,\\n161-163\\ndangers of extrapolation, 161\\nprediction intervals, 68, 161\\nconfidence intervals versus, 163\\npredictive modeling\\nKNN as first stage for, 247, 249\\nmachine learning vs. statistics, 238\\npredictor variables, 6, 143\\ncorrelated, 170\\nisolating relationship between response and,\\n185\\nmain effects and interactions, 174-176\\nnonlinear relationships among, captured by\\ntrees, 258\\nnumeric, applying naive Bayes algorithm to,\\n200\\nredundancy in, 172standardization in K-Nearest Neighbors,\\n243\\nt-statistic and, 155\\nused twice in KNN, 247\\nusing more than two in linear discriminant\\nanalysis, 207\\nprincipal components, 285\\nprincipal components analysis, 284-294, 325\\ncluster analysis versus, 302\\ncomputing principal components, 288\\ncorrespondence analysis, 292-294\\ndeciding how many components to choose,\\n292\\ninterpreting principal components, 289-292\\nscaling of variables, 320\\nsimple example, 285-288\\nusing with binary data, 325\\nprobability, 30, 195\\nassociated with a confidence interval, 67\\noutput by K-Nearest Neighbors, 241\\nproduced by tree models, 254\\npropensity, 195\\n(see also probability)\\nproxy variables, 98\\npruning, 250, 256\\npseudo-residuals, 272\\nQ\\nQQ-Plots, 69\\nfor returns of NFLX stock, 74\\nstandard normal distribution and, 71\\nquadratic discriminant analysis (QDA), 207\\nquantiles, 17\\n(see also percentiles)\\nfunctions for, 17\\nR\\nR-squared, 151, 154\\nR-Tutorial website, 4\\nrandom forests, 238, 249, 259, 261-264\\nhyperparameters, 269\\nvariable importance in, 265-269\\nrandom numbers, generation from Poisson dis‐\\ntribution, 83\\nrandom sampling, 48-54\\nbias, 50\\nrandom selection, 51\\nsample mean versus population mean, 53\\nsize versus quality, 52\\nIndex | 337', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 354}),\n",
       " Document(page_content='random subset of variables (in random forest),\\n261\\nrandomization, 88\\nrandomness, underestimating and misinter‐\\npreting, 93\\nrange, 14, 16\\nranking records, naive Bayes algorithm, 200\\nrare class problem, 223\\nrecall, 223\\n(see also sensitivity)\\ntrade-off with specificity, 224\\nReceiver Operating Characteristics curve (see\\nROC curve)\\nrecords, 5, 142\\nrectangular data, 4-7\\nkey terms for, 4\\nterminology differences, 6\\nrecursive partitioning, 249, 252-254\\nreducing the dimension of the data, 283\\nreference coding, 164, 166, 175, 211\\nregression, 141-194\\nANOV A as first step toward statistical\\nmodel, 123\\ncausation caution, 149\\ndiagnostics, 176-187\\nheteroskedasticity, non-normality, and\\ncorrelated errors, 182-185\\ninfluential values, 179\\noutliers, 177\\npartial residual plots and nonlinearity,\\n185\\nfactor variables in, 117, 163-169\\ndummy variables representation, 164\\nordered factor variables, 169\\nwith many levels, 167-168\\ninterpreting the regression equation,\\n169-176\\nconfounding variables, 172-173\\ncorrelated predictor variables, 170\\ninteractions and main effects, 174-176\\nmulticollinearity, 172\\nlogistic regression, 208-219\\ncomparison to linear regression, 214-216\\nmeanings of term, 149\\nmultiple linear regression, 150-161\\nassessing the model, 153\\ncross validation, 155\\nexample, estimating value of houses,\\n151-152model selection and stepwise regression,\\n156-159\\nweighted regression, 159-161\\npolynomial and spline regression, 187-193\\ngeneralized additive models, 192-193\\npolynomial, 188\\nsplines, 189\\nprediction with, 161-163\\nconfidence and prediction intervals,\\n161-163\\ndangers of extrapolation, 161\\nsimple linear regression, 141-150\\nfitted values and residuals, 146\\nleast squares, 148\\nprediction vs. explanation, 149\\nregression equation, 143-146\\nwith a tree, 257\\nunsupervised learning as building block,\\n284\\nregression coefficients, 142, 145\\ncomparison with full data and with influen‐\\ntial data removed, 181\\nconfidence intervals and, 161\\ncorrelated predictors and, 171\\nregression to the mean, 55\\nregularization, 271\\navoiding overfitting with, 274-279\\nL1 regularization, 276, 278\\nL2 regularization, 276, 278\\nreplacement (in sampling), 48, 97\\nin bootstrap permutation tests, 102\\nsample with replacement, 62\\nrepresentativeness, 48\\nthrough random sampling, 51\\nresampling, 62, 96-103\\nbootstrap and permutation tests, 96\\nbootstrapping versus, 65\\npermutation tests, 97-103, 120\\nexhaustive and bootstrap tests, 102\\nvalue for data science, 102\\nweb stickiness example, 98-102\\nusing in chi-square test, 124, 126\\nrescaling variables, methods other than z-\\nscores, 246\\nresidual standard error (RSE), 150, 153\\nresidual sum of squares (RSS), 148, 278\\nresiduals, 142\\nanalysis of, in logistic regression, 217\\nin multiple linear regression, 151\\n338 | Index', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 355}),\n",
       " Document(page_content=\"in simple linear regression, 146\\nresponse, 6, 142, 143\\nisolating relationship between predictor\\nvariable and, 185\\nridge regression, 159, 278\\nRMSE (see root mean squared error)\\nrobust, 8\\nrobust estimates of correlation, 33\\nrobust estimates of location, 10-13\\nexample, location estimates of population\\nand murder rates, 12\\nmedian, 10\\noutliers and, 11\\nother robust metrics for, 11\\nweighted median, 11\\nrobust estimates of variability, median absolute\\ndeviation from the median, 16\\nrobust estimates of variance, calculating robust\\nMAD, 18\\nROC curve, 220, 224-226\\nAUC metric, 226-228\\nroot mean squared error (RMSE), 150, 153, 257\\nRSE (see residual standard error)\\nRSS (see residual sum of squares)\\nS\\nsample bias, 49\\nsample statistic, 57\\nsamples\\nsample size, power and, 135-139\\nterminology differences, 6\\nsampling, 47-86\\nbinomial distribution, 79-80\\nbootstrap, 61-65\\nchi-square distribution, 80\\nconfidence intervals, 65-68\\nF-distribution, 82\\nlong-tailed distributions, 73-75\\nnormal distribution, 69-72\\nPoisson and related distributions, 82-86\\nestimating the failure rate, 84\\nexponential distribution, 84\\nPoisson distributions, 83\\nWeibull distribution, 85\\nrandom sampling and sample bias, 48-54\\nwith and without replacement, 48, 62, 97,\\n102\\nselection bias, 54-57\\nStudent's t-distribution, 75-78sampling distribution, 57-61\\ncentral limit theorem, 60\\ndata distribution versus, 58\\nstandard error, 60\\nsampling variability, 57\\nscaling, 318\\nscaling and categorical variables, 318-326\\ncategorical variables and Gower's distance,\\n322\\ndominant variables, 321-322\\nproblems with clustering mixed data, 325\\nscaling the variables, 319-320\\nscatterplot smoothers, 185\\nscatterplots, 31, 34\\nbiplot, 292\\nextending with conditional variables, 43\\nscientific fraud, detecting, 129\\nscreeplots, 285, 321\\nfor PCA of top stocks, 289\\nsearch\\nneed for enormous quantities of data, 52\\nvast search effect, 55\\nselection bias, 54-57\\nregression to the mean, 55-56\\ntypical forms of, 55\\nself-selection sampling bias, 50\\nsensitivity, 220, 223\\nsignal to noise ratio (SNR), 246\\nsignificance level, 135, 137\\nsignificance tests, 93\\n(see also hypothesis tests)\\nunderestimating and misinterpreting ran‐\\ndom events in, 93\\nsimple random sample, 48\\nsingle linkage metric, 309\\nskew, 73\\nskewness, 24\\nslope, 143\\n(see also regression coefficients)\\nslot machines used in gambling, 132\\nsmoothing parameter, use with naive Bayes\\nalgorithm, 201\\nSMOTE algorithm, 233\\nspatial data structures, 6\\nSpearman's rho, 34\\nspecificity, 220, 224\\ntrade-off with recall, 224\\nspline regression, 187, 189\\nsplit value, 249, 258\\nIndex | 339\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 356}),\n",
       " Document(page_content=\"SQL (Structured Query Language), 4\\nsquare-root of n rule, 60\\nSS (see sum of squares)\\nstandard deviation, 14\\ncovariance matrix and, 203\\nin A/B testing, 90\\nand related estimates, 15\\nstandard error, 57, 60\\nstandard normal distribution, 69\\nand QQ-Plots, 71\\nstandardization, 69, 71, 239, 242\\nin K-Nearest Neighbors, 243-246\\nof continuous variables, 295\\nstandardized residuals\\ndefined, 176\\nexamining to detect outliers, 177\\nhistogram of, for housing data regression,\\n183\\nstatistical experiments and significance testing,\\n87-139\\nA/B testing, 88-93\\nanalysis of variance (ANOV A), 118-124\\nchi-square test, 124-131\\ndegrees of freedom, 116-118\\nhypothesis tests, 93-96\\nmulti-arm bandits, 131-134\\nmultiple testing, 112-116\\npower and sample size, 135-139\\nresampling, 96-103\\nstatistical significance and p-values, 103-110\\nalpha, 107\\ncontroversy over p-values, 107\\ndata science and p-values, 109\\np-values, 106\\ntype 1 and type 2 errors, 109\\nt-tests, 110-112\\nstatistical inference, classical inference pipeline,\\n87\\nstatistical machine learning, 237-282\\nbagging and the random forest, 259-270\\nbagging, 260\\nhyperparameters, 269\\nrandom forests, 261-264\\nvariable importance, 265-269\\nboosting, 270-282\\nboosting algorithm, 271\\nhyperparameters and cross-validation,\\n279-281overfitting, avoiding with regularization,\\n274-279\\nXGBoost software, 272-274\\nK-Nearest Neighbors, 238-249\\ndistance metrics, 241\\nexample, predicting loan default,\\n239-241\\nKNN as feature engine, 247-248\\none hot encoder, 242\\nstandardization, 243-246\\ntree models, 249-259\\nhow trees are used, 258\\nmeasuring homogeneity or impurity,\\n254\\npredicting a continuous value, 257\\nrecursive partitioning, 252-254\\nsimple example, 250\\nstopping tree growth, 256\\nunsupervised learning, 283\\n(see also unsupervised learning)\\nstatistical significance, 98\\npractical significance versus, 109\\nstatistics versus machine learning, 238\\nstepwise regression, 157\\nstochastic gradient boosting, 270\\nXGBoost implementation, 272-274\\nstratified sampling, 52\\nstructured data, 2-4\\nStudent's t-distribution, 75-78, 110\\nsubjects, 88\\nsuccess, 78\\nsum contrasts, 167\\nsum of squares (SS), 118, 122\\nwithin-cluster SS, 295\\nsupervised learning, 141, 195, 237\\nSynthetic Minority Oversampling Technique\\n(see SMOTE algorithm)\\nT\\nt-distribution, 75-78, 110, 114\\nt-statistic, 110, 151, 154\\nt-tests, 110-112\\ntails, 73\\nsummarizing with percentiles, 20\\ntarget, 6\\ntarget shuffling, 55\\ntest statistic, 90, 91, 111, 111\\nThompson's sampling, 134\\ntreatment, 88\\n340 | Index\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 357}),\n",
       " Document(page_content=\"treatment group, 88\\ntree models, 175, 234, 249-259\\nadvantages of, 250\\nensemble, random forest and boosted trees,\\n260\\nhow trees are used, 258\\nmeasuring homogeneity or impurity, 254\\npredicting a continuous value, 257\\nrecursive partitioning algorithm, 252-254\\nsimple example, 250\\nstopping tree growth, 256\\nTrellis graphics, 45\\ntrials, 79\\ntrimmed mean, 8\\nformula for, 9\\nTukey's HSD (honest significance difference),\\n114\\nTukey, John Wilder, 1\\ntwo-way ANOV A, 123\\ntwo-way tests, 93, 95\\ntype 1 errors, 103, 109, 113\\ntype 2 errors, 103, 109\\nU\\nunbiased estimates, 15\\nundersampling, 231\\nuniform random distribution, 129\\nunivariate analysis, 36\\nunsupervised learning, 283-326\\ngoals achieved by, 283\\nhierarchical clustering, 304-310\\nagglomerative algorithm, 308\\ndissimilarity metrics, 309-310\\nrepresentation in a dendrogram, 306-308\\nsimple example, 305\\nK-means clustering, 294-304\\ninterpreting the clusters, 299-302\\nK-means algorithm, 298\\nselecting the number of clusters, 302\\nsimple example, 295\\nmodel-based clustering, 311-318\\nmixtures of normals, 313-315\\nmultivariate normal distribution, 311\\nselecting the number of clusters, 315-317\\nand prediction, 284\\nprincipal components analysis, 284-294\\ncomputing principal components, 288\\ncorrespondence analysis, 292-294interpreting principal components,\\n289-292\\nsimple example, 285-288\\nscaling and categorical variables, 318-326\\ncategorical variables and Gower's dis‐\\ntance, 322\\ndominant variables, 321-322\\nproblems with clustering mixed data,\\n325\\nscaling the variables, 319-320\\nup/down weighting, 230, 232\\nuplift, 229\\n(see also lift)\\nV\\nvariability, estimates of, 13-19\\nexample, murder rates by state population,\\n18\\nkey terminology, 13\\npercentiles, 16\\nstandard deviation and related estimates,\\n14-16\\nvariable importance, 259, 265-269\\nvariables\\ncovariance between, 202\\nexploring two or more, 36-46\\ncategorical and numeric data, 41\\ncategorical variables, 39\\nkey terms, 36\\nusing hexagonal binning and contour\\nplot, 36-39\\nvisualizing multiple variables, 43\\nrescaling, methods other than z-scores, 246\\nvariance, 13, 15\\nanalysis of (ANOV A), 82, 118-124\\nbias-variance trade-off in fitting the model,\\n247\\nvast search effect, 55\\nviolin plots, 36\\nboxplots versus, 42\\nvisualizations, 7\\n(see also graphs)\\nW\\nW3Schools guide for SQL, 4\\nweb testing\\nA/B testing in data science, 91\\nclick rate for three different headlines, 124\\npopularity of bandit algorithms in, 132\\nIndex | 341\", metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 358}),\n",
       " Document(page_content='web stickiness example, 98-102, 118\\nWeibull distribution, 85\\nweighted mean, 8\\nformula for, 10\\nweighted median, 8, 11\\nweighted regression, 151, 159-161\\nweighting\\nup weight and down weight, 232\\nusing to change loss function in classifica‐\\ntion, 233\\nweights for principal components (see load‐\\nings)\\nwhiskers (in boxplots), 21\\nwins, 132\\nwithin-cluster sum of squares (SS), 295X\\nXGBoost, 272-274\\nhyperparameters, 281\\nusing regularization to avoid overfitting,\\n274-279\\nZ\\nz-scores, 69, 71, 203, 230, 239\\nconversion of data to, normal distribution\\nand, 72\\nin data standardization for KNN, 243\\nusing to scale variables, 319\\n342 | Index', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 359}),\n",
       " Document(page_content='About the Authors\\nPeter Bruce  founded and grew the Institute for Statistics Education at Statistics.com,\\nwhich now offers about one hundred courses in statistics, roughly a third of which\\nare aimed at the data scientist. In recruiting top authors as instructors and forging a\\nmarketing strategy to reach professional data scientists, Peter has developed both a\\nbroad view of the target market and his own expertise to reach it.\\nAndrew Bruce  has over 30 years of experience in statistics and data science in aca‐\\ndemia, government, and business. He has a PhD in statistics from the University of\\nWashington and has published numerous papers in refereed journals. He has devel‐\\noped statistical-based solutions to a wide range of problems faced by a variety of\\nindustries, from established financial firms to internet startups, and offers a deep\\nunderstanding of the practice of data science.\\nPeter Gedeck  has over 30 years of experience in scientific computing and data sci‐\\nence. After 20 years as a computational chemist at Novartis, he now works as a senior\\ndata scientist at Collaborative Drug Discovery. He specializes in the development of\\nmachine learning algorithms to predict biological and physicochemical properties of\\ndrug candidates. Coauthor of Data Mining for Business Analytics , he earned a PhD in\\nchemistry from the University of Erlangen-Nürnberg in Germany and studied math‐\\nematics at the Fernuniversität Hagen, Germany.', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 360}),\n",
       " Document(page_content='Colophon\\nThe animal on the cover of Practical Statistics for Data Scientists  is a lined shore crab\\n(Pachygrapsus crassipes ), also known as a striped shore crab. It is found along the\\ncoasts and beaches of the Pacific Ocean in North America, Central America, Korea,\\nand Japan. These crustaceans live under rocks, in tidepools, and within crevices. They\\nspend about half their time on land, and periodically return to the water to wet their\\ngills.\\nThe lined shore crab is named for the green stripes on its brown-black carapace. It\\nhas red claws and purple legs, which also have a striped or mottled pattern. The crab\\ngenerally grows to be 3–5 centimeters in size; females are slightly smaller. Their eyes\\nare on flexible stalks that can rotate to give them a full field of vision as they walk.\\nCrabs are omnivores, feeding primarily on algae but also on mollusks, worms, fungi,\\ndead animals, and other crustaceans (depending on what is available). They moult\\nmany times as they grow to adulthood, taking in water to expand and crack open\\ntheir old shell. Once this is achieved, they spend several difficult hours getting free,\\nand then must hide until the new shell hardens.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world.\\nThe cover illustration is by Karen Montgomery, based on a black-and-white engrav‐\\ning from Pictorial Museum of Animated Nature . The cover fonts are Gilroy Semibold\\nand Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe\\nMyriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 361}),\n",
       " Document(page_content='There’s much more  \\nwhere this came from.\\nExperience books, videos, live online  \\ntraining courses, and more from O’Reilly  \\nand our 200+ partners—all in one place.\\nLearn more at oreilly.com/online-learning\\n©2019 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175', metadata={'source': 'C:\\\\Users\\\\Rushikesh\\\\Downloads\\\\Practical Statistics for DataScientists.pdf', 'page': 362})]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "mykey = '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'\n",
    "client=OpenAI(openai_api_key=mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name=PromptTemplate(\n",
    "    input_variables=['product'],\n",
    "    template = \"what is a good name for a company that makes {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=client,prompt=prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cakes\n",
      "\n",
      "Rainbow Cupcake Co.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('colorful cup').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name=PromptTemplate(\n",
    "    input_variables=['product'],\n",
    "    template = \"what would be a good name for a company that makes {product}\")\n",
    "chain = LLMChain(llm=client,prompt=prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DroneTech Solutions.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('Drons').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBuffer Memory\n",
    "We can attcah memory to remember all previous conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name=PromptTemplate(\n",
    "    input_variables=['product'],\n",
    "    template = \"what would be a good name for a company that makes {product}\")\n",
    "chain = LLMChain(llm=client,prompt=prompt_template_name,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nVineyard Reserve Cellars'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Wines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nPhotoVision Solutions'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Camera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Wines'), AIMessage(content='\\n\\nVineyard Reserve Cellars'), HumanMessage(content='Camera'), AIMessage(content=' Drones\\n\\nSkyCam Drones'), HumanMessage(content='Camera'), AIMessage(content='\\n\\nPhotoVision Solutions')]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Wines\n",
      "AI: \n",
      "\n",
      "Vineyard Reserve Cellars\n",
      "Human: Camera\n",
      "AI:  Drones\n",
      "\n",
      "SkyCam Drones\n",
      "Human: Camera\n",
      "AI: \n",
      "\n",
      "PhotoVision Solutions\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Chain\n",
    "Conversation buffer memory goes growing endlessly<br>\n",
    "Just remember last 5 Conversation Chain<br>\n",
    "Just remember lat 10-20 Coversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = ConversationChain(llm=OpenAI(openai_api_key=mykey,temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first ever Cricket World Cup was held in 1975 in England and was won by the West Indies.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket worldcup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sure, 5 + 5 is equal to 10.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"can you tell how much 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sure, 5 * (5 + 1) is equal to 30.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"can you tell how much will 5*(5+1)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the West Indies team that won the first Cricket World Cup in 1975 was Clive Lloyd.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  Sure, what is the equation you'd like me to solve?\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"can you divide the  numbers and can you give me final answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBuffer Window Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = ConversationChain(llm=OpenAI(openai_api_key=mykey,temperature=0.7),memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first Cricket World Cup was won by the West Indies in 1975.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket worldcup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sure! 5+5 equals 10.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"can you tell how much 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I do not know.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "convo = ConversationChain(llm=OpenAI(openai_api_key=mykey,temperature=0.7),memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first cricket worldcup was won by the West Indies in 1975. They beat Australia in the final by 17 runs.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket worldcup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5+5 is equal to 10.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"can you tell how much 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the winning team in the first cricket worldcup was Clive Lloyd.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
